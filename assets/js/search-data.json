{
  
    
        "post0": {
            "title": "Financial Planner Review - Discounting Cash Flows",
            "content": "Among the list of formulas I learned in the Financial Analyst coures, this is valuation is the one I instintively distrusted. Discounted Cash Flows is a way to evaluate the future returns on an investement based on: . The initial Investment. | The Expected Cashflow per each year. | The problem I have here is that the Expected Cashflows are purely guesses about what we belive the numbers to be in the future. While the example in the class picked - as far as I can tell - random numbers to fill them in, I would hope that the numbers provided here would be based on research of other similar projects. At least, I would hope in the real world this is was happens. . Discounting Cash Flows . The formula for this is: . $DCF= (CF_1)/(1 + r)^1 + (CF_2)/(1 + r)^2 + .. + (CF_n)/(1 + r)^n$ . ... where:- CF stands for Cash Flow . r is the interest rate | n is the period of discount. | . This all follows from the simple idea that money now is more valuble than money in the future. If given the choice between $100 now and $100 in a year then you&#39;d obviously take the $100 now. This also applies to money we earn in the future: $60 now is better than the $60 we&#39;d earn in the future. So, example time. . We&#39;re going to use a similar example to the class: Imagine that we&#39;re considering inveseting money into a venture. We&#39;re going to do something a bit more modern and say we&#39;re investing in a growing Online Streamer with the expectation that we&#39;ll get some of the money in turn. A deal is worked out and you&#39;ll be getting some kind of slice of the money they make in exchange. They&#39;re going to ask for $500 in investment. For the interest rate, we&#39;ll use the Inflation Rate since this is not a loan. Right now, it&#39;s 8.2% so we&#39;ll use that as our Interest Rate; Sometimes this also called the Discount Rate and Inflation certainly applies. . We&#39;ll use random numbers over the span of 6 years and then calculate if this was a good idea. . from random import randint year = range(0,6) interestRate = .082 predictions = [ randint(0, 300) for _ in range(5)] cashFlow = [-500] + predictions cashFlow . [-500, 195, 127, 44, 42, 196] . We&#39;ll need a function to calculate the Present Value for each term in the formula. For this, we&#39;ll just write up a quick lambda function in python. . PV = (lambda f,i,n: f/(1+i)**n) PV(30, interestRate, 1) . 27.726432532347502 . We&#39;ll usually see this in a table so we&#39;ll add all this to a Data frame. . data = pd.DataFrame( {&quot;Year&quot;:year, &quot;Cash&quot;:cashFlow, &#39;Pv&#39;:repeat(0,len(year)) }) data . Year Cash Pv . 0 0 | -500 | 0 | . 1 1 | 195 | 0 | . 2 2 | 127 | 0 | . 3 3 | 44 | 0 | . 4 4 | 42 | 0 | . 5 5 | 196 | 0 | . ... and now we can iterate through the rows and fill in the Present Value per year. . for _,r in data.iterrows(): data.loc[r.Year, &#39;PV&#39;] = round(PV(r.Cash, interestRate, r.Year), 2 ) . Lastly, we&#39;ll take the sum to see if it was worth it. . data.PV.sum() . -13.749999999999972 . Looks like we lost about $14 which is not that surprising since making money in Streaming can be quite challenging. .",
            "url": "https://orgulo.us/python/code/finanical/analyst/2022/11/09/fp-discounted-cash-flows.html",
            "relUrl": "/python/code/finanical/analyst/2022/11/09/fp-discounted-cash-flows.html",
            "date": " • Nov 9, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Four.",
            "content": "Introduction . Welcome back to the fourth post in the series. Last time, we were having issues with the data since there were missing values - including the Job that prompted the initial investigation. We&#39;ll jump right into it picking up with the mismatch between naming in tables. . While working through the list of problems I found reviewing the data, there is clearly an error in the Regex built before. The symptom of this was the Autoloader which has a hanging number when the group is pulled out. . tasksSubset = siteJobs[1][[&quot;Name&quot;, &quot;Description&quot;, &quot;Tasks&quot;]].copy() tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()] tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(&quot;Kill&quot;)] regex = r&quot;( d+ s[ w]+ s[ w]+)&quot; tmp = tasksSubset.Tasks.str.extractall(regex) tmp[tmp.reset_index()[0].str.contains(&#39;Autoloader&#39;).values] . 0 . match . 8 0 1 Autoloader 5 | . 32 0 2 Autoloader 8 | . After some work, this was because the hanging [ w]+ was too greedy and had to be toned down. . newRegex = r&quot;( d+ s[ w]+ s?[a-zA-Z]+)&quot; tmp = tasksSubset.Tasks.str.extractall(newRegex) tmp[tmp.reset_index()[0].str.contains(&#39;Autoloader&#39;).values] . 0 . match . 8 0 1 Autoloader | . 32 0 2 Autoloader | . This didn&#39;t end up being the only correction to the Regex; in fact, the Co-Tool Multitool was being broken apart since it has a - in the name. So, we had to update this to check and include the - on the split. . newRegex = r&quot;(( d+ s[ w -]+ s?[a-zA-Z]+))&quot; . Now we&#39;ll have our updated version of the breakLoot function we write before. . def breakLoot(taskString, index=0): parts = taskString.split(&#39; &#39;, maxsplit=1) if index == 0: return int(parts[index]) elif index == 1: return parts[index] else: # This shouldn&#39;t be called. return None tasks = [] for index in range(0,3): tasksSubset = siteJobs[index][[&quot;Name&quot;, &quot;Description&quot;, &quot;Tasks&quot;]].copy() tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()] tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(&quot;Kill&quot;)] newRegex = r&quot;(( d+ s[ w -]+ s?[a-zA-Z]+))&quot; tmp = tasksSubset.Tasks.str.extractall(newRegex) count = tmp.reset_index()[0].apply(breakLoot).values aLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values tmp = tmp.assign( count = count, loot = aLoot ) nameDescriptSlice = tasksSubset.loc[tmp.reset_index()[&quot;level_0&quot;], [&#39;Name&#39;, &#39;Description&#39;]] tmp = tmp.assign( name = nameDescriptSlice.Name.values, description = nameDescriptSlice.Description.values ) taskSlice = tmp.reset_index().drop([ &#39;level_0&#39;, &#39;match&#39;, 0 ], axis =1 ) taskSlice = taskSlice[[&#39;name&#39;, &#39;count&#39;, &#39;loot&#39;, &#39;description&#39;]] tasks.append(taskSlice) tasks = pd.concat([*tasks]) . Fix Item Names . Now we can continue and move to fixing the name mismatches like intended. There was quite a few of these so I&#39;ll only show the process for a few of them - and then the fix for all of them. We&#39;ll pull the code from the previous post so we can start by using the 0 values and work backwards. . allJobs = pd.concat([korolevRewards, icaRewards, osirisRewards]) lootKMarks = loot.query(&#39;Name == &quot;K-Marks&quot;&#39;) tasks = tasks.copy() taskLoot = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;outer&#39;) taskLoot[&#39;Cost&#39;] = taskLoot[&#39;count&#39;] * taskLoot[&#39;Unit&#39;] results = allJobs.query(&#39;Rewards == &quot;K-Marks&quot;&#39;).merge( taskLoot[[&#39;name&#39;, &#39;Cost&#39;]].groupby(&#39;name&#39;).sum(), left_on=&quot;Job&quot;, right_on=&quot;name&quot;, how=&#39;left&#39;) results[&#39;Balance&#39;] = results[&#39;Units&#39;] - results[&#39;Cost&#39;] # How many are there? len(results.query(&quot;Cost == 0&quot;)) . 30 . There are 31 rows with a Cost of 0 which we&#39;ll need to investigate. The first one which I had noticed and corrected in the previous post was the CPU&#39;s so we&#39;ll address this one first. We&#39;ll need to find the values in the tasks table and the lootKMarks table so we can figure out where the disconnect is. . tasks.loc[tasks.loot.str.contains(&quot;Master|Zero&quot;)] . name count loot description . 3 Mining Bot | 2 | Zero Systems | Our engineers have designed an autonomous mini... | . 6 Insufficient Processing Power | 1 | Master Unit | Prospector! The Zero Systems CPU you brought u... | . 12 Automated Security | 5 | Zero Systems | We will have to build new turrets to help prot... | . 16 Classified I | 2 | Master Unit | Prospector! We need Derelict Explosives, Maste... | . 47 Upgrades | 1 | Master Unit | We want to take over an ICA Data Center. Stash... | . 9 Sabotage | 4 | Zero Systems | Prospector! Those Korolev simpletons are drill... | . 34 Spare Parts | 4 | Zero Systems | One of the Servers at Starport has been failin... | . 40 NEW-Hard-ICA-Gather-6 | 1 | Zero Systems | DESCRIPTION MISSING | . 11 Data Center Upgrades | 2 | Master Unit | Turns out our data center was not powerful eno... | . lootKMarks.loc[lootKMarks.Loot.str.contains(&quot;CPU&quot;)] . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 131 3845.0 | K-Marks | Epic | Yes x3 | Yes x3 | Yes | No | Master Unit CPU | . 321 506.0 | K-Marks | Rare | Yes x37 | Yes x6 | Yes | No | Zero Systems CPU | . The tasks table has a shortened version of its name so the easiest way to fix this would be to append CPU to the loot name in the tasks table. . tasks.loc[ tasks.loot == &quot;Master Unit&quot;, &#39;loot&#39;] = &#39;Master Unit CPU&#39; tasks.loc[ tasks.loot == &quot;Master Unit CPU&quot;, &#39;loot&#39;] . 6 Master Unit CPU 16 Master Unit CPU 47 Master Unit CPU 11 Master Unit CPU Name: loot, dtype: object . Another was the Pure Focus Crystals which has the same problem of having the name truncated; this is a trend among all the affected materials with higher tiered materials. Those will all be included in the final correction and will skip showing the process; it&#39;s literally just the same and tedious. . # Missing the Crystal part tasks.loc[tasks.loot.str.contains(&quot;Pure Focus&quot;)] . name count loot description . 25 Geologist | 1 | Pure Focus | You got time for a job, Prospector? The sample... | . 26 Industry Secret | 3 | Pure Focus | Hm. Interesting. The Pure Focus Crystals you b... | . 28 Laser Rifles | 3 | Pure Focus | We are prototyping a new laser rifle that can ... | . 16 Arms Race | 4 | Pure Focus | Prospector. It seems like Korolev is working o... | . lootKMarks.loc[lootKMarks.Loot.str.contains(&quot;Crystal&quot;)] . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 56 961.0 | K-Marks | Rare | Yes x19 | Yes x10 | Yes | No | Focus Crystal | . 61 2883.0 | K-Marks | Epic | Yes x6 | Yes x9 | Yes | No | Pure Focus Crystal | . 66 38924.0 | K-Marks | Legendary | Yes x6 | Yes x9 | Yes | No | Polirium Crystal | . 81 1709.0 | K-Marks | Epic | Yes x36 | Yes x9 | Yes | No | Teratomorphic Crystal Core | . This one was a surprise since truncating it doesn&#39;t make any sense and gains nothing but the Magnetic Field Stabilizer is being truncated as well. . # Missing the Stabilizer: tasks.loc[tasks.loot.str.contains(&quot;Magnetic&quot;)] . name count loot description . 42 Stability is Key | 1 | Magnetic Field | Our Comms are jammed. We need you to stash a M... | . 43 Stability is Key | 1 | Magnetic Field | Our Comms are jammed. We need you to stash a M... | . 44 Stability is Key | 1 | Magnetic Field | Our Comms are jammed. We need you to stash a M... | . 28 Storm Interference | 2 | Magnetic Field | Prospector, the Storm has been distorting all ... | . 2 Surveillance Center | 2 | Magnetic Field | We want to expand our surveillance operations ... | . lootKMarks.loc[lootKMarks.Loot.str.contains(&quot;Magnetic&quot;)] . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 171 338.0 | K-Marks | Uncommon | Yes x71 | Yes x25 | Yes | No | Magnetic Field Stabilizer | . Lastly, we&#39;ll show the NiC Oil which was a problem from the previous post which is missing the Cannister at the end again. . tasks.loc[tasks.loot.str.contains(&quot;Oil&quot;)] . name count loot description . 41 Striking Big | 10 | NiC Oil | Damn it! We ran out of Fuel for our Radiation ... | . lootKMarks.loc[lootKMarks.Loot.str.contains(&quot;Oil&quot;)] . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 121 20183.0 | K-Marks | Epic | Yes x103 | Yes x10 | Yes | No | NiC Oil Cannister | . There we go! Now, we&#39;ll update the data again and run the merge once more. . # Most corrections: tasks.loc[ tasks.loot == &quot;Master Unit&quot;, &#39;loot&#39;] = &#39;Master Unit CPU&#39; tasks.loc[ tasks.loot == &quot;Zero Systems&quot;, &#39;loot&#39;] = &#39;Zero Systems CPU&#39; tasks.loc[ tasks.loot == &quot;Pure Focus&quot;, &#39;loot&#39;] = &#39;Pure Focus Crystal&#39; tasks.loc[ tasks.loot == &quot;Heavy Mining&quot;, &#39;loot&#39;] = &#39;Heavy Mining Tool&#39; tasks.loc[ tasks.loot == &quot;Magnetic Field&quot;, &#39;loot&#39;] = &#39;Magnetic Field Stabilizer&#39; tasks.loc[ tasks.loot == &quot;Brittle Titan&quot;, &#39;loot&#39;] = &#39;Brittle Titan Ore&#39; tasks.loc[ tasks.loot == &quot;NiC Oil&quot;, &#39;loot&#39;] = &#39;NiC Oil Cannister&#39; tasks.loc[ tasks.loot == &quot;Charged Spinal&quot;, &#39;loot&#39;] = &#39;Charged Spinal Base&#39; tasks.loc[ tasks.loot == &quot;Hardened Bone&quot;, &#39;loot&#39;] = &#39;Hardened Bone Plates&#39; tasks.loc[ tasks.loot == &quot;Pale Ivy&quot;, &#39;loot&#39;] = &#39;Pale Ivy Blossom&#39; tasks.loc[ tasks.loot == &quot;Glowy Brightcap&quot;, &#39;loot&#39;] = &#39;Glowy Brightcap Mushroom&#39; tasks.loc[ tasks.loot == &quot;Blue Runner&quot;, &#39;loot&#39;] = &#39;Blue Runner Egg&#39; tasks.loc[ tasks.loot == &quot;Magic&quot;, &#39;loot&#39;] = &#39;Magic-GROW Fertilizer&#39; tasks.loc[ tasks.loot == &quot;Letium&quot;, &#39;loot&#39;] = &#39;Letium Clot&#39; tasks.loc[ tasks.loot == &quot;Azure Tree&quot;, &#39;loot&#39;] = &#39;Azure Tree Bark&#39; . tasks = tasks.copy() taskLoot = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;outer&#39;) taskLoot[&#39;Cost&#39;] = taskLoot[&#39;count&#39;] * taskLoot[&#39;Unit&#39;] results = allJobs.query(&#39;Rewards == &quot;K-Marks&quot;&#39;).merge( taskLoot[[&#39;name&#39;, &#39;Cost&#39;]].groupby(&#39;name&#39;).sum(), left_on=&quot;Job&quot;, right_on=&quot;name&quot;, how=&#39;left&#39;) results[&#39;Balance&#39;] = results[&#39;Units&#39;] - results[&#39;Cost&#39;] len(results.query(&quot;Cost == 0&quot;)) . 13 . We&#39;ve made some good progress so far but we&#39;ve still got quite a few jobs to update. Let&#39;s check the list of missing valuse once more and see if there is a new pattern here. . tmp = tasks.merge(results[[&#39;Job&#39;, &#39;Cost&#39;]], left_on=&quot;name&quot;, right_on=&quot;Job&quot;, how=&#39;left&#39;).query(&quot;Cost == 0&quot;) tmp . name count loot description Job Cost . 40 Excavation Gear | 1 | Heavy Mining Tool | For excavations, we need you to stash a Heavy ... | Excavation Gear | 0.0 | . 46 And two smoking Barrels | 1 | PKR Maelstrom | Prospector. Get down there and stash a PKR Mae... | And two smoking Barrels | 0.0 | . 47 And two smoking Barrels | 200 | Shotgun Ammo | Prospector. Get down there and stash a PKR Mae... | And two smoking Barrels | 0.0 | . 74 Data Drive II | 1 | Rare Data | The Data you brought us was helpful, but we ne... | Data Drive II | 0.0 | . 75 Data Drive III | 2 | Rare Data | Good work last time, Prospector. The Data was ... | Data Drive III | 0.0 | . 76 Data Drive IV | 1 | Epic Data | In order to be able to predict Storm Behaviour... | Data Drive IV | 0.0 | . 77 Data Drive V | 1 | Legendary Data | Yes! The more precise Data you brought us was ... | Data Drive V | 0.0 | . 78 Data Drive VI | 3 | Legendary Data | We&#39;re finding more than just Storm data now...... | Data Drive VI | 0.0 | . 82 Grenadier | 1 | Frag Grenade | Prospector. You have heard of Badum&#39;s Dead Dro... | Grenadier | 0.0 | . 86 Ammo Supplies | 1000 | Medium Ammo | Our Field Agents need more Ammo if they are to... | Ammo Supplies | 0.0 | . 87 Data Drop | 2 | Rare Data | Prospector. One of our Scientists is convinced... | Data Drop | 0.0 | . 90 Provide an Advocate | 1 | Advocate at | Our field agent requested better gear to take ... | Provide an Advocate | 0.0 | . 129 Loadout Drop | 1 | Rare Shield | One of our more... lethal assets on Fortuna re... | Loadout Drop | 0.0 | . 130 Loadout Drop | 1 | Rare Helmet | One of our more... lethal assets on Fortuna re... | Loadout Drop | 0.0 | . 131 Loadout Drop | 1 | Rare Backpack | One of our more... lethal assets on Fortuna re... | Loadout Drop | 0.0 | . 138 NEW-Hard-Osiris-EliteCrusher-1 | 1 | Alpha Crusher | DESCRIPTION MISSING | NEW-Hard-Osiris-EliteCrusher-1 | 0.0 | . The biggest standout problem here now is all those data drive quests so lets resolve those next. . Data Drives: . We have another problem though since none of the tables we have actually contains the data we&#39;re after. So, we&#39;re back to the wiki to do some more scraping work. If we also look at the names of the data drives we&#39;re going to have another problem soon - which you&#39;ll see when we download the data. . # Missing the Data Drive tasks.loc[tasks.loot.str.contains(&quot;Data&quot;)] . name count loot description . 19 Data Drive II | 1 | Rare Data | The Data you brought us was helpful, but we ne... | . 20 Data Drive III | 2 | Rare Data | Good work last time, Prospector. The Data was ... | . 21 Data Drive IV | 1 | Epic Data | In order to be able to predict Storm Behaviour... | . 22 Data Drive V | 1 | Legendary Data | Yes! The more precise Data you brought us was ... | . 23 Data Drive VI | 3 | Legendary Data | We&#39;re finding more than just Storm data now...... | . 32 Data Drop | 2 | Rare Data | Prospector. One of our Scientists is convinced... | . Like the previous posts, scraping is a tedious process of matching keywords and pulling the right tables so that&#39;s getting skipped; it&#39;s just mostly trial and error. . urlDataDrives = &#39;https://thecyclefrontier.wiki/wiki/Utilities#Data_Drives-0&#39; siteDrive = pd.read_html(urlDataDrives, attrs={&quot;class&quot;:&quot;zebra&quot;})[2] . We&#39;ll use a modified version of the code we wrote before for parsing the loot table for this. . driveSubset = siteDrive[ [&#39;Image&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Weight&#39;] ].copy() driveSubset = driveSubset.assign( Loot = np.NaN ) index = range( 0, len(driveSubset) - 2, 3) offset = np.array([1, 2]) for i in index: aLoot = driveSubset.iloc[i, 1] indexes = i + offset driveSubset.iloc[indexes, 4] = aLoot tmp = driveSubset.iloc[:, 1:4] tmp = tmp.fillna(method=&quot;ffill&quot;) driveSubset.iloc[:, 1:4] = tmp cutNA = driveSubset.Loot.isna() driveData = driveSubset[ ~cutNA ] driveData = driveData.rename(columns={&#39;Image&#39;:&#39;Unit&#39;, &#39;Name&#39;:&#39;Reward&#39;}) driveData[&#39;Rarity&#39;] = pd.Categorical( driveData.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;] ) driveData[&#39;Label&#39;] = driveData[&#39;Rarity&#39;].astype(&#39;str&#39;) + &#39; Data&#39; drives = driveData drives . Unit Reward Rarity Weight Loot Label . 1 30.0 | K-Marks | Common | 15.0 | Data Drive Tier 1 | Common Data | . 2 0.0 | Reputation | Common | 15.0 | Data Drive Tier 1 | Common Data | . 4 1013.0 | K-Marks | Uncommon | 15.0 | Data Drive Tier 2 | Uncommon Data | . 5 1.0 | Reputation | Uncommon | 15.0 | Data Drive Tier 2 | Uncommon Data | . 7 2531.0 | K-Marks | Rare | 15.0 | Data Drive Tier 3 | Rare Data | . 8 3.0 | Reputation | Rare | 15.0 | Data Drive Tier 3 | Rare Data | . 10 6075.0 | K-Marks | Epic | 15.0 | Data Drive Tier 4 | Epic Data | . 11 6.0 | Reputation | Epic | 15.0 | Data Drive Tier 4 | Epic Data | . 13 10252.0 | K-Marks | Legendary | 15.0 | Data Drive Tier 5 | Legendary Data | . 14 10.0 | Reputation | Legendary | 15.0 | Data Drive Tier 5 | Legendary Data | . Since this is the second time that we&#39;ve needed this - and we&#39;re going to need this again - we should write a function to wrap this whole process. . # this is the function, where: ## siteData: the table from the scraped site ## columns: the columns you want to keep from the scraped data ## adjust: the count from the bottom containing an index ## step: how many rows between values we care about ## offset: how many rewards are there? def extractSite(siteData, columns, adjust, step, offset): if not isinstance(columns, list): print(&quot;Columns argument must be a list.&quot;) return None siteSubset = siteData[columns].copy() siteSubset = siteSubset.assign( Loot = np.NaN ) # Some extra error handling if not isinstance(adjust, int): print(&quot;adjust argument must be an int.&quot;) return None if not isinstance(step, int): print(&quot;step argument must be an int.&quot;) return None if not isinstance(offset, list): print(&quot;offset argument must be a list.&quot;) return None index = range( 0, len(siteSubset) - adjust, step) offset = np.array(offset) for i in index: aLoot = siteSubset.iloc[i, 1] indexes = i + offset siteSubset.iloc[indexes, len(siteSubset.columns)-1] = aLoot tmp = siteSubset.iloc[:, 1:len(siteSubset.columns)] tmp = tmp.fillna(method=&quot;ffill&quot;) siteSubset.iloc[:, 1:len(siteSubset.columns)-1] = tmp cutNA = siteSubset.Loot.isna() returnData = siteSubset[ ~cutNA ] returnData = returnData.rename(columns={&#39;Image&#39;:&#39;Unit&#39;, &#39;Name&#39;:&#39;Reward&#39;}) return returnData . Now we&#39;ll sanity check this to make sure it works. . drives = extractSite(siteDrive, [&#39;Image&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Weight&#39;], 2, 3, [1, 2]) drives[&#39;Rarity&#39;] = pd.Categorical( drives.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;] ) drives . Unit Reward Rarity Weight Loot . 1 30.0 | K-Marks | Common | 15.0 | Data Drive Tier 1 | . 2 0.0 | Reputation | Common | 15.0 | Data Drive Tier 1 | . 4 1013.0 | K-Marks | Uncommon | 15.0 | Data Drive Tier 2 | . 5 1.0 | Reputation | Uncommon | 15.0 | Data Drive Tier 2 | . 7 2531.0 | K-Marks | Rare | 15.0 | Data Drive Tier 3 | . 8 3.0 | Reputation | Rare | 15.0 | Data Drive Tier 3 | . 10 6075.0 | K-Marks | Epic | 15.0 | Data Drive Tier 4 | . 11 6.0 | Reputation | Epic | 15.0 | Data Drive Tier 4 | . 13 10252.0 | K-Marks | Legendary | 15.0 | Data Drive Tier 5 | . 14 10.0 | Reputation | Legendary | 15.0 | Data Drive Tier 5 | . Perfect! Now we just add this to our list of adjustments right? Sadly no. If you look at the names of the drives you&#39;ll find that we&#39;re not quite there. The names of the drives were renamed in Season 2 but the values in our tasks were not updated from their previous values. This is not too hard to update since the old drive names were just the Rarity + Data and we have that so we&#39;ll just need a new column. . drives[&#39;Loot&#39;] = drives[&#39;Rarity&#39;].astype(&#39;str&#39;) + &#39; Data&#39; drives . Unit Reward Rarity Weight Loot . 1 30.0 | K-Marks | Common | 15.0 | Common Data | . 2 0.0 | Reputation | Common | 15.0 | Common Data | . 4 1013.0 | K-Marks | Uncommon | 15.0 | Uncommon Data | . 5 1.0 | Reputation | Uncommon | 15.0 | Uncommon Data | . 7 2531.0 | K-Marks | Rare | 15.0 | Rare Data | . 8 3.0 | Reputation | Rare | 15.0 | Rare Data | . 10 6075.0 | K-Marks | Epic | 15.0 | Epic Data | . 11 6.0 | Reputation | Epic | 15.0 | Epic Data | . 13 10252.0 | K-Marks | Legendary | 15.0 | Legendary Data | . 14 10.0 | Reputation | Legendary | 15.0 | Legendary Data | . # drives pd.concat( [lootKMarks.rename({&quot;Name&quot;:&quot;Reward&quot;}, axis=1)[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], drives.query(&#39;Reward == &quot;K-Marks&quot;&#39;).drop(&quot;Weight&quot;, axis=1)]) . Unit Reward Rarity Loot . 1 150.0 | K-Marks | Common | Flawed Veltecite | . 6 570.0 | K-Marks | Uncommon | Cloudy Veltecite | . 11 854.0 | K-Marks | Rare | Clear Veltecite | . 16 1922.0 | K-Marks | Epic | Pure Veltecite | . 21 6487.0 | K-Marks | Legendary | Veltecite Heart | . ... ... | ... | ... | ... | . 1 30.0 | K-Marks | Common | Common Data | . 4 1013.0 | K-Marks | Uncommon | Uncommon Data | . 7 2531.0 | K-Marks | Rare | Rare Data | . 10 6075.0 | K-Marks | Epic | Epic Data | . 13 10252.0 | K-Marks | Legendary | Legendary Data | . 99 rows × 4 columns . Add that to our pipeline and let&#39;s see where we are now . allJobs = pd.concat([korolevRewards, icaRewards, osirisRewards]) lootKMarks = loot.query(&#39;Name == &quot;K-Marks&quot;&#39;) # Most corrections: tasks.loc[ tasks.loot == &quot;Master Unit&quot;, &#39;loot&#39;] = &#39;Master Unit CPU&#39; tasks.loc[ tasks.loot == &quot;Zero Systems&quot;, &#39;loot&#39;] = &#39;Zero Systems CPU&#39; tasks.loc[ tasks.loot == &quot;Pure Focus&quot;, &#39;loot&#39;] = &#39;Pure Focus Crystal&#39; tasks.loc[ tasks.loot == &quot;Heavy Mining&quot;, &#39;loot&#39;] = &#39;Heavy Mining Tool&#39; tasks.loc[ tasks.loot == &quot;Magnetic Field&quot;, &#39;loot&#39;] = &#39;Magnetic Field Stabilizer&#39; tasks.loc[ tasks.loot == &quot;Brittle Titan&quot;, &#39;loot&#39;] = &#39;Brittle Titan Ore&#39; tasks.loc[ tasks.loot == &quot;NiC Oil&quot;, &#39;loot&#39;] = &#39;NiC Oil Cannister&#39; tasks.loc[ tasks.loot == &quot;Charged Spinal&quot;, &#39;loot&#39;] = &#39;Charged Spinal Base&#39; tasks.loc[ tasks.loot == &quot;Hardened Bone&quot;, &#39;loot&#39;] = &#39;Hardened Bone Plates&#39; tasks.loc[ tasks.loot == &quot;Pale Ivy&quot;, &#39;loot&#39;] = &#39;Pale Ivy Blossom&#39; tasks.loc[ tasks.loot == &quot;Glowy Brightcap&quot;, &#39;loot&#39;] = &#39;Glowy Brightcap Mushroom&#39; tasks.loc[ tasks.loot == &quot;Blue Runner&quot;, &#39;loot&#39;] = &#39;Blue Runner Egg&#39; tasks.loc[ tasks.loot == &quot;Magic&quot;, &#39;loot&#39;] = &#39;Magic-GROW Fertilizer&#39; tasks.loc[ tasks.loot == &quot;Letium&quot;, &#39;loot&#39;] = &#39;Letium Clot&#39; tasks.loc[ tasks.loot == &quot;Azure Tree&quot;, &#39;loot&#39;] = &#39;Azure Tree Bark&#39; tasks = tasks.copy() # Extract Stuff here for now: drives = extractSite(siteDrive, [&#39;Image&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Weight&#39;], 2, 3, [1, 2]) drives[&#39;Rarity&#39;] = pd.Categorical( drives.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;] ) drives[&#39;Loot&#39;] = drives[&#39;Rarity&#39;].astype(&#39;str&#39;) + &#39; Data&#39; lootKMarks = pd.concat( [lootKMarks.rename({&quot;Name&quot;:&quot;Reward&quot;}, axis=1)[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], drives.query(&#39;Reward == &quot;K-Marks&quot;&#39;).drop(&quot;Weight&quot;, axis=1)]) taskLoot = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;outer&#39;) taskLoot[&#39;Cost&#39;] = taskLoot[&#39;count&#39;] * taskLoot[&#39;Unit&#39;] results = allJobs.query(&#39;Rewards == &quot;K-Marks&quot;&#39;).merge( taskLoot[[&#39;name&#39;, &#39;Cost&#39;]].groupby(&#39;name&#39;).sum(), left_on=&quot;Job&quot;, right_on=&quot;name&quot;, how=&#39;left&#39;) results[&#39;Balance&#39;] = results[&#39;Units&#39;] - results[&#39;Cost&#39;] len(results.query(&quot;Cost == 0&quot;)) . 7 . That&#39;s much better! So, what&#39;s left to do? . tmp = tasks.merge(results[[&#39;Job&#39;, &#39;Cost&#39;]], left_on=&quot;name&quot;, right_on=&quot;Job&quot;, how=&#39;left&#39;).query(&quot;Cost == 0&quot;) tmp . name count loot description Job Cost . 40 Excavation Gear | 1 | Heavy Mining Tool | For excavations, we need you to stash a Heavy ... | Excavation Gear | 0.0 | . 46 And two smoking Barrels | 1 | PKR Maelstrom | Prospector. Get down there and stash a PKR Mae... | And two smoking Barrels | 0.0 | . 47 And two smoking Barrels | 200 | Shotgun Ammo | Prospector. Get down there and stash a PKR Mae... | And two smoking Barrels | 0.0 | . 82 Grenadier | 1 | Frag Grenade | Prospector. You have heard of Badum&#39;s Dead Dro... | Grenadier | 0.0 | . 86 Ammo Supplies | 1000 | Medium Ammo | Our Field Agents need more Ammo if they are to... | Ammo Supplies | 0.0 | . 90 Provide an Advocate | 1 | Advocate at | Our field agent requested better gear to take ... | Provide an Advocate | 0.0 | . 129 Loadout Drop | 1 | Rare Shield | One of our more... lethal assets on Fortuna re... | Loadout Drop | 0.0 | . 130 Loadout Drop | 1 | Rare Helmet | One of our more... lethal assets on Fortuna re... | Loadout Drop | 0.0 | . 131 Loadout Drop | 1 | Rare Backpack | One of our more... lethal assets on Fortuna re... | Loadout Drop | 0.0 | . 138 NEW-Hard-Osiris-EliteCrusher-1 | 1 | Alpha Crusher | DESCRIPTION MISSING | NEW-Hard-Osiris-EliteCrusher-1 | 0.0 | . Add the Guns . We&#39;re going to move to getting the gun data included since we already actualy have it. This was part of another post which was done - not included in the series. . url = &quot;https://thecyclefrontier.wiki/wiki/Weapons&quot; siteGun = pd.read_html(url, attrs={&quot;class&quot;:&quot;zebra&quot;})[0] gunData = siteGun[~siteGun.Type.isna()] indx = gunData[&#39;Proj. Speed&#39;] == &#39;Hitscan&#39; gunData.loc[indx, &#39;Proj. Speed&#39;] = np.NaN gunData = gunData.assign( Unit = gunData[&#39;Sell Value&#39;].str.replace(&#39; K-Marks&#39;, &#39;&#39;).astype(&#39;float&#39;), Reward = &quot;K-Marks&quot;, Loot = gunData[&#39;Name&#39;] ) # # This removes the legendary weapons # data = data.query(&#39;Faction != &quot;Printing&quot;&#39;) guns = gunData guns[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]].head(15) . Unit Reward Rarity Loot . 0 17429.0 | K-Marks | Epic | Advocate | . 3 524.0 | K-Marks | Common | AR-55 Autorifle | . 6 12341.0 | K-Marks | Epic | Asp Flechette Gun | . 9 371.0 | K-Marks | Common | B9 Trenchgun | . 12 63080.0 | K-Marks | Exotic | Basilisk | . 15 1918.0 | K-Marks | Uncommon | Bulldog | . 18 2052.0 | K-Marks | Common | C-32 Bolt Action | . 21 17429.0 | K-Marks | Epic | Gorgon | . 24 16805.0 | K-Marks | Exotic | Hammer | . 27 7143.0 | K-Marks | Rare | ICA Guarantee | . 30 228.0 | K-Marks | Common | K-28 | . 33 325513.0 | K-Marks | Legendary | KARMA-1 | . 36 22781.0 | K-Marks | Epic | KBR Longshot | . 39 94459.0 | K-Marks | Exotic | Kinetic Arbiter | . 42 1918.0 | K-Marks | Uncommon | KM-9 &#39;Scrapper&#39; | . Now we have the gun data to be added to the loot table but if you were paying attention you may have noticed something is wrong with one of our values. . # Not sure what this is from. tasks.loc[tasks.loot.str.contains(&#39; at&#39;)] . name count loot description . 35 Provide an Advocate | 1 | Advocate at | Our field agent requested better gear to take ... | . The Advocate in this row is labeled as Advocate at which is another problem. In this instance, instead of trying to deal with the regex we&#39;re just going to update that single value. . tasks.loc[tasks.loot.str.contains(&#39;Advocate at&#39;), &#39;loot&#39;] = &#39;Advocate&#39; tasks.loc[tasks.loot.str.contains(&#39;Advocate&#39;)] . name count loot description . 35 Provide an Advocate | 1 | Advocate | Our field agent requested better gear to take ... | . And, make sure that the append works as intended: . pd.concat( [lootKMarks.rename({&quot;Name&quot;:&quot;Reward&quot;}, axis=1)[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], drives.query(&#39;Reward == &quot;K-Marks&quot;&#39;).drop(&quot;Weight&quot;, axis=1), guns[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]]] ) . Unit Reward Rarity Loot . 1 150.0 | K-Marks | Common | Flawed Veltecite | . 6 570.0 | K-Marks | Uncommon | Cloudy Veltecite | . 11 854.0 | K-Marks | Rare | Clear Veltecite | . 16 1922.0 | K-Marks | Epic | Pure Veltecite | . 21 6487.0 | K-Marks | Legendary | Veltecite Heart | . ... ... | ... | ... | ... | . 63 371.0 | K-Marks | Common | S-576 PDW | . 66 1179.0 | K-Marks | Uncommon | Scarab | . 69 12341.0 | K-Marks | Epic | Shattergun | . 72 34172.0 | K-Marks | Exotic | Voltaic Brute | . 75 270540.0 | K-Marks | Legendary | Zeus Beam | . 125 rows × 4 columns . Deal With Backpacks . We&#39;re going to try to fix the Backpacks, the Shields and the Helmets toegther since they&#39;re all on the same page of the wiki. Again, this data was not part of the previous tables that we had so we&#39;ll need to go get it. . gearUrl = &#39;https://thecyclefrontier.wiki/wiki/Gear&#39; site = pd.read_html(gearUrl) . This was tricky to collect - which is only why I&#39;m pointing out how it was done. While working out how to collect the data from the website, there was no good strategy to collect just the data that I wanted. What you will see is that I use hard coded values to pull out where each table is. . siteBackPacks = site[0] siteHelmet = site[10] siteShield = site[22] . What I did was enumerate throught all the tables collected to find which results had outlier sizes. I then pulled those to make sure they were what I was after like this: . # 10 # 22 list(enumerate(map(len, site))) . [(0, 22), (1, 1), (2, 1), (3, 3), (4, 1), (5, 4), (6, 1), (7, 4), (8, 1), (9, 1), (10, 46), (11, 1), (12, 1), (13, 3), (14, 4), (15, 4), (16, 3), (17, 4), (18, 4), (19, 3), (20, 4), (21, 4), (22, 43), (23, 1), (24, 1), (25, 3), (26, 4), (27, 4), (28, 4), (29, 4), (30, 4), (31, 4), (32, 4)] . It is not pretty but web scraping rarely is. And, it works. Next we&#39;ll start with the backup packs; with some tweaking of the values to the extractSite function which was defined earlier, this becomes really easy. . backpacks = extractSite( siteData = siteBackPacks.loc[siteBackPacks.Name.str.contains(&quot;Backpack|K-Marks&quot;)], columns = [&#39;Image&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Space&#39;, &#39;Sale Price&#39;], adjust = 2, step = 3, offset = [1, 2]) backpacks . Unit Reward Rarity Space Sale Price Loot . 1 600.0 | K-Marks | Common | 200.0 | 180 K-Marks | Small Backpack | . 2 180.0 | K-Marks | Common | 200.0 | 180 K-Marks | Small Backpack | . 4 2700.0 | K-Marks | Uncommon | 250.0 | 810 K-Marks | Medium Backpack | . 7 810.0 | K-Marks | Uncommon | 250.0 | 810 K-Marks | Medium Backpack | . 9 6100.0 | K-Marks | Rare | 300.0 | 1,830 K-Marks | Large Backpack | . 13 1830.0 | K-Marks | Rare | 300.0 | 1,830 K-Marks | Large Backpack | . 15 12000.0 | K-Marks | Epic | 350.0 | 4,000 K-Marks | Heavy Duty Backpack | . 19 4000.0 | K-Marks | Epic | 350.0 | 4,000 K-Marks | Heavy Duty Backpack | . Looking at these results though, what we&#39;re really after is the Sale Price and so we&#39;ll need to move some stuff around. . backpacks = backpacks.assign( Unit = backpacks[&#39;Sale Price&#39;].str.replace(&#39; K-Marks&#39;, &#39;&#39;).str.replace(&#39;,&#39;, &#39;&#39;).astype(&#39;float&#39;) ) backpacks = backpacks.drop([&quot;Sale Price&quot;, &quot;Space&quot;], axis=1) backpacks . Unit Reward Rarity Loot . 1 180.0 | K-Marks | Common | Small Backpack | . 2 180.0 | K-Marks | Common | Small Backpack | . 4 810.0 | K-Marks | Uncommon | Medium Backpack | . 7 810.0 | K-Marks | Uncommon | Medium Backpack | . 9 1830.0 | K-Marks | Rare | Large Backpack | . 13 1830.0 | K-Marks | Rare | Large Backpack | . 15 4000.0 | K-Marks | Epic | Heavy Duty Backpack | . 19 4000.0 | K-Marks | Epic | Heavy Duty Backpack | . We have the same problem which we had for the Data Drives: the the old name Rare Backpack is in the tasks table but is not how they are named here. Again, we&#39;re just going to steal and modify the solution we had before and apply it here. . backpacks[&#39;Loot&#39;] = backpacks[&#39;Rarity&#39;].astype(&#39;str&#39;) + &#39; Backpack&#39; . Time to add them to the pipeline. . pd.concat([ lootKMarks.rename({&quot;Name&quot;:&quot;Reward&quot;}, axis=1)[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], drives.query(&#39;Reward == &quot;K-Marks&quot;&#39;).drop(&quot;Weight&quot;, axis=1), guns[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], backpacks ]) . Unit Reward Rarity Loot . 1 150.0 | K-Marks | Common | Flawed Veltecite | . 6 570.0 | K-Marks | Uncommon | Cloudy Veltecite | . 11 854.0 | K-Marks | Rare | Clear Veltecite | . 16 1922.0 | K-Marks | Epic | Pure Veltecite | . 21 6487.0 | K-Marks | Legendary | Veltecite Heart | . ... ... | ... | ... | ... | . 7 810.0 | K-Marks | Uncommon | Uncommon Backpack | . 9 1830.0 | K-Marks | Rare | Rare Backpack | . 13 1830.0 | K-Marks | Rare | Rare Backpack | . 15 4000.0 | K-Marks | Epic | Epic Backpack | . 19 4000.0 | K-Marks | Epic | Epic Backpack | . 133 rows × 4 columns . This is much better. So, how many are missing now? . len(results.query(&quot;Cost == 0&quot;)) . 4 . Much better. But, there is actually a new problem here: masking missing values. If we look back at our data from before we&#39;ll see that there was Helment and Shield information missing and now it&#39;s gone! Since we&#39;ve fixed some of the values for those jobs the Balance is no longer 0 and thefore we&#39;ve lost track of it! Let&#39;s step back and look at the merged result and look for mistakes. And, upon doing this we run into our first masked problem. . tmp = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;outer&#39;) tmp . name count loot description Loot Unit . 0 New Mining Tools | 2.0 | Hydraulic Piston | We are producing new Mining Tools for new Pros... | Hydraulic Piston | 338.0 | . 1 Excavator Improvements | 3.0 | Hydraulic Piston | The suspension on our mining excavators need i... | Hydraulic Piston | 338.0 | . 2 New Mining Tools | 10.0 | Hardened Metals | We are producing new Mining Tools for new Pros... | Hardened Metals | 150.0 | . 3 Automated Security | 16.0 | Hardened Metals | We will have to build new turrets to help prot... | Hardened Metals | 150.0 | . 4 Air Lock Upgrades | 12.0 | Hardened Metals | Our engineers designed a safer airlock system ... | Hardened Metals | 150.0 | . ... ... | ... | ... | ... | ... | ... | . 211 NaN | NaN | NaN | NaN | Common Backpack | 180.0 | . 212 NaN | NaN | NaN | NaN | Uncommon Backpack | 810.0 | . 213 NaN | NaN | NaN | NaN | Uncommon Backpack | 810.0 | . 214 NaN | NaN | NaN | NaN | Epic Backpack | 4000.0 | . 215 NaN | NaN | NaN | NaN | Epic Backpack | 4000.0 | . 216 rows × 6 columns . There are all these NaN values at the bottom which were attached and included due to the outer including all the rows. Can we simply change this over to a left? . tmp = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;left&#39;) tmp.loc[tmp.Loot.isna()] . name count loot description Loot Unit . 39 Excavation Gear | 1 | Heavy Mining Tool | For excavations, we need you to stash a Heavy ... | NaN | NaN | . 46 And two smoking Barrels | 200 | Shotgun Ammo | Prospector. Get down there and stash a PKR Mae... | NaN | NaN | . 79 Grenadier | 1 | Frag Grenade | Prospector. You have heard of Badum&#39;s Dead Dro... | NaN | NaN | . 83 Ammo Supplies | 1000 | Medium Ammo | Our Field Agents need more Ammo if they are to... | NaN | NaN | . 126 Loadout Drop | 1 | Rare Shield | One of our more... lethal assets on Fortuna re... | NaN | NaN | . 127 Loadout Drop | 1 | Rare Helmet | One of our more... lethal assets on Fortuna re... | NaN | NaN | . 136 NEW-Hard-Osiris-EliteCrusher-1 | 1 | Alpha Crusher | DESCRIPTION MISSING | NaN | NaN | . This is more in line what I&#39;d have expected. We&#39;ll temporarily keep this - but mostly for discussion purposes. The problem here is that when you look through the wiki there are no sell values for the Shields and Helmets. We cannot get these from the wiki which means we cannot automate it. We could do this manually but then I&#39;d have to constantly update this value when it changes - which I&#39;m trying to avoid. This looks to be a case where we&#39;re going to need to remove this job for now. . # Adding this to the pipeline: idx = tmp.name.str.contains(&quot;Loadout Drop&quot;) tmp = tmp.loc[-idx] tmp . name count loot description Loot Unit . 0 New Mining Tools | 2 | Hydraulic Piston | We are producing new Mining Tools for new Pros... | Hydraulic Piston | 338.0 | . 1 New Mining Tools | 10 | Hardened Metals | We are producing new Mining Tools for new Pros... | Hardened Metals | 150.0 | . 2 Explosive Excavation | 4 | Derelict Explosives | One of our mines collapsed with valuable equip... | Derelict Explosives | 1709.0 | . 3 Mining Bot | 2 | Zero Systems CPU | Our engineers have designed an autonomous mini... | Zero Systems CPU | 506.0 | . 4 Mining Bot | 3 | Ball Bearings | Our engineers have designed an autonomous mini... | Ball Bearings | 338.0 | . ... ... | ... | ... | ... | ... | ... | . 136 NEW-Hard-Osiris-EliteCrusher-1 | 1 | Alpha Crusher | DESCRIPTION MISSING | NaN | NaN | . 137 Flexible Sealant | 8 | Resin Gun | A number of our weather balloons took damage f... | Resin Gun | 759.0 | . 138 Indigenous Fruit | 4 | Indigenous Fruit | Ah, Prospector, have you come across any Indig... | Indigenous Fruit | 759.0 | . 139 Indigenous Fruit | 4 | Biological Sampler | Ah, Prospector, have you come across any Indig... | Biological Sampler | 1139.0 | . 140 Don&#39;t get crushed | 3 | Crusher Hide | Gear up, Prospector! We need Crusher Skins for... | Crusher Hide | 11533.0 | . 137 rows × 6 columns . Ok, so it looks like next will be solving the ammo listings. . tmp.loc[tmp.Loot.isna()] . name count loot description Loot Unit . 39 Excavation Gear | 1 | Heavy Mining Tool | For excavations, we need you to stash a Heavy ... | NaN | NaN | . 46 And two smoking Barrels | 200 | Shotgun Ammo | Prospector. Get down there and stash a PKR Mae... | NaN | NaN | . 79 Grenadier | 1 | Frag Grenade | Prospector. You have heard of Badum&#39;s Dead Dro... | NaN | NaN | . 83 Ammo Supplies | 1000 | Medium Ammo | Our Field Agents need more Ammo if they are to... | NaN | NaN | . 136 NEW-Hard-Osiris-EliteCrusher-1 | 1 | Alpha Crusher | DESCRIPTION MISSING | NaN | NaN | . Deal with Ammo . ammoUrl = &quot;https://thecyclefrontier.wiki/wiki/Ammo&quot; ammo = pd.read_html(ammoUrl)[0] . We&#39;ve already done this before so this is simply here to show it was done. And, you add it to the pipeline just the same. . ammo = ammo.rename( {&quot;Item Name&quot;:&quot;Loot&quot;, &quot;Sell Value&quot;:&quot;Unit&quot;}, axis=1 ).assign( Reward = &quot;K-Marks&quot;, Rarity = pd.Categorical( ammo.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;]) )[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]] . Deal with The Heavy Mining Tool . Sadly, this tool is on its own page so we&#39;ll need something custom again. There is a table here we can pull but it&#39;s oriented incorrectly for our use. . site = pd.read_html(&quot;https://thecyclefrontier.wiki/wiki/Heavy_Mining_Tool&quot;) minerData = site[0] minerData . 0 1 . 0 Description | Allows faster mining of materials. | . 1 Rarity | Common | . 2 Weight | 30 | . 3 Buy Value | 600 | . 4 Sell Value | 180 | . 5 Faction Points | 2 | . Thankfully, a data frame has the Transpose function that a matrix does. A simple explanation of this function is that it swaps the rows to columns and columns to rows. . minerData[0].T . 0 Description 1 Rarity 2 Weight 3 Buy Value 4 Sell Value 5 Faction Points Name: 0, dtype: object . We&#39;re going to extract the rows values to create a new dataframe object. Since the Heaving Mining Tool designation is missing from the data then we&#39;ll need to add it ourself. . row = minerData[1].T.to_list() + [&#39;Heavy Mining Tool&#39;] row . [&#39;Allows faster mining of materials.&#39;, &#39;Common&#39;, &#39;30&#39;, &#39;600&#39;, &#39;180&#39;, &#39;2&#39;, &#39;Heavy Mining Tool&#39;] . columns = minerData[0].to_list() + [&#39;Loot&#39;] columns . [&#39;Description&#39;, &#39;Rarity&#39;, &#39;Weight&#39;, &#39;Buy Value&#39;, &#39;Sell Value&#39;, &#39;Faction Points&#39;, &#39;Loot&#39;] . And, join them together. . mineTool = pd.DataFrame(columns = columns) mineTool.loc[0] = row mineTool = mineTool.assign( Reward = &quot;K-Marks&quot;, Rarity = pd.Categorical( mineTool.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;] ), Unit = mineTool[&#39;Sell Value&#39;].astype(int), )[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]] mineTool . Unit Reward Rarity Loot . 0 180 | K-Marks | Common | Heavy Mining Tool | . And, add it to the pipeline. . lootKMarks = pd.concat([ lootKMarks.rename({&quot;Name&quot;:&quot;Reward&quot;}, axis=1)[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], drives.query(&#39;Reward == &quot;K-Marks&quot;&#39;).drop(&quot;Weight&quot;, axis=1), guns[[&#39;Unit&#39;, &#39;Reward&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]], backpacks, ammo, mineTool ]) # Drop this quest idx = tasks.name.str.contains(&quot;Loadout Drop&quot;) tasks = tasks.loc[-idx] taskLoot = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;outer&#39;) taskLoot[&#39;Cost&#39;] = taskLoot[&#39;count&#39;] * taskLoot[&#39;Unit&#39;] results = allJobs.query(&#39;Rewards == &quot;K-Marks&quot;&#39;).merge( taskLoot[[&#39;name&#39;, &#39;Cost&#39;]].groupby(&#39;name&#39;).sum(), left_on=&quot;Job&quot;, right_on=&quot;name&quot;, how=&#39;left&#39;) results[&#39;Balance&#39;] = results[&#39;Units&#39;] - results[&#39;Cost&#39;] . results.query(&quot;Cost == 0&quot;) . Units Rewards Job Cost Balance . 77 2400 | K-Marks | Grenadier | 0.0 | 2400.0 | . 138 155000 | K-Marks | NEW-Hard-Osiris-EliteCrusher-1 | 0.0 | 155000.0 | . tmp = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;left&#39;) tmp.loc[tmp.Loot.isna()] . name count loot description Loot Unit . 93 Grenadier | 1 | Frag Grenade | Prospector. You have heard of Badum&#39;s Dead Dro... | NaN | NaN | . 151 NEW-Hard-Osiris-EliteCrusher-1 | 1 | Alpha Crusher | DESCRIPTION MISSING | NaN | NaN | . Conclusions . This is the best we&#39;ll get I suppose. Now, back to our check: How bad is our good old job And two smoking Barrels? . results.loc[results.Job.str.contains(&quot;Barrel&quot;)] . Units Rewards Job Cost Balance . 39 19000 | K-Marks | And two smoking Barrels | 40716.0 | -21716.0 | . And, there we go! So, how many jobs have a negative balance? . results.query(&quot;Balance &lt; 0 &quot;) . Units Rewards Job Cost Balance . 39 19000 | K-Marks | And two smoking Barrels | 40716.0 | -21716.0 | . 70 9500 | K-Marks | Data Drive III | 10124.0 | -624.0 | . 73 56000 | K-Marks | Data Drive VI | 61512.0 | -5512.0 | . 81 17000 | K-Marks | Ammo Supplies | 69000.0 | -52000.0 | . That&#39;s not as bad as I expected. What is the average job balance? . results.Balance.mean(), results.Balance.median() . (13300.702127659575, 11256.0) . So, what jobs are above the mean? . results.query(f&quot;Balance &gt;= {results.Balance.mean()}&quot;) . Units Rewards Job Cost Balance . 5 25000 | K-Marks | Excavator Improvements | 4306.0 | 20694.0 | . 6 37000 | K-Marks | A new type of Alloy | 15468.0 | 21532.0 | . 7 27000 | K-Marks | Automated Security | 4930.0 | 22070.0 | . 9 52000 | K-Marks | Classified I | 27030.0 | 24970.0 | . 15 27000 | K-Marks | Geologist | 6727.0 | 20273.0 | . 16 29000 | K-Marks | Industry Secret | 8649.0 | 20351.0 | . 17 34000 | K-Marks | Veltecite Hearts | 12974.0 | 21026.0 | . 18 40000 | K-Marks | Laser Rifles | 18259.0 | 21741.0 | . 19 47000 | K-Marks | Classified II | 23922.0 | 23078.0 | . 20 41000 | K-Marks | Unlimited Power | 17377.0 | 23623.0 | . 32 27000 | K-Marks | Meteor Core | 7594.0 | 19406.0 | . 33 30000 | K-Marks | Shards of pure Power | 8104.0 | 21896.0 | . 34 48000 | K-Marks | Battery Day | 22782.0 | 25218.0 | . 41 28000 | K-Marks | Upgrades | 3845.0 | 24155.0 | . 43 140000 | K-Marks | Exclusive Drilling Rights | 115330.0 | 24670.0 | . 44 32000 | K-Marks | Stocking Up | 9120.0 | 22880.0 | . 45 26000 | K-Marks | Shock and Awe | 11385.0 | 14615.0 | . 50 37000 | K-Marks | Sabotage | 15696.0 | 21304.0 | . 51 28000 | K-Marks | Old Currency | 6000.0 | 22000.0 | . 52 26000 | K-Marks | Recoil Compensation | 9390.0 | 16610.0 | . 54 45000 | K-Marks | New 3D Printer | 21362.0 | 23638.0 | . 55 33000 | K-Marks | Arms Race | 11532.0 | 21468.0 | . 56 35000 | K-Marks | Energy Crisis | 18984.0 | 16016.0 | . 71 26000 | K-Marks | Data Drive IV | 12150.0 | 13850.0 | . 76 33000 | K-Marks | A Solution | 10125.0 | 22875.0 | . 82 25000 | K-Marks | Data Drop | 10124.0 | 14876.0 | . 83 26000 | K-Marks | Field Supplies | 3000.0 | 23000.0 | . 84 25000 | K-Marks | Spare Parts | 2024.0 | 22976.0 | . 90 29000 | K-Marks | NEW-Hard-ICA-Gather-6 | 9142.0 | 19858.0 | . 91 227000 | K-Marks | Striking Big | 201830.0 | 25170.0 | . 93 25000 | K-Marks | A Craving | 4048.0 | 20952.0 | . 97 26000 | K-Marks | Cretins | 6300.0 | 19700.0 | . 98 27000 | K-Marks | Sensor Array Repairs | 6908.0 | 20092.0 | . 99 32000 | K-Marks | A new Energy Source | 11289.0 | 20711.0 | . 100 30000 | K-Marks | Data Center Upgrades | 7690.0 | 22310.0 | . 101 30000 | K-Marks | Keep the Experiment running | 7200.0 | 22800.0 | . 102 61000 | K-Marks | Safety Measures | 36309.0 | 24691.0 | . 122 31000 | K-Marks | Harvest Day | 6583.0 | 24417.0 | . 125 37000 | K-Marks | Core Values | 15188.0 | 21812.0 | . 126 45000 | K-Marks | Promising Results | 21266.0 | 23734.0 | . 130 24000 | K-Marks | Stormy Samples | 2700.0 | 21300.0 | . 131 25000 | K-Marks | Stormy Samples II | 2024.0 | 22976.0 | . 132 33000 | K-Marks | Loadout Drop | 7320.0 | 25680.0 | . 136 33000 | K-Marks | NEW-Hard-Osiris-Gather-7 | 12816.0 | 20184.0 | . 138 155000 | K-Marks | NEW-Hard-Osiris-EliteCrusher-1 | 0.0 | 155000.0 | . Perfect! Next we&#39;ll loop back around to automating the pipeline and uploading the data to Kaggle. .",
            "url": "https://orgulo.us/game/python/data/science/exploration/cycle/frontier/2022/11/04/cycle-jobs-part-four.html",
            "relUrl": "/game/python/data/science/exploration/cycle/frontier/2022/11/04/cycle-jobs-part-four.html",
            "date": " • Nov 4, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Simple Function for Highlight Box Effect",
            "content": "While browsing around for inspiration, I found this wonderful post by someone named SonyMonthonaRK on Medium; the post is here since it contains more than what I&#39;ll be talking about. The graph is simple aside from the red highlighted box around relevant information: seen below: . I&#39;ve seen this effect before but never actually seen the code for it. Thankfully, he provided the a link to it on their Github page since I wanted to learn this trick. The Good news is that the effect is not complicated; the bad news is that it&#39;s implementation here is conditional and ugly. We&#39;re going to copy it here just for reference: . plt.figure(figsize=(15,10)) # mengatur ukuran figure sns.lineplot(x=&#39;month&#39;, y=&#39;average_num_booking&#39;, hue=&#39;hotel_type&#39;, color= &#39;gray&#39;, size=&quot;hotel_type&quot;, sizes=(2.5, 2.5), data=df1_gr) # plot awal menggunakan lineplot dari library seaborn plt.tick_params(axis=&#39;both&#39;, which=&#39;major&#39;, labelsize=14) # memperbesar ukuran x-y axis label plt.grid() # menambahkan gridline plt.legend(title=&#39;Hotel Type&#39;, title_fontsize=15, prop={&#39;size&#39;:13}) # mengatur judul dan ukuran font pada legenda plt.xlabel(&#39;Arrival Month&#39;, fontsize=15) # mengatur title pada x-axis plt.ylabel(&#39;Average Number of Booking&#39;, fontsize=15) # mengatur title pada y-axis plt.ylim(0, 4800) # membatasi y axis plt.axvline(4, ls=&#39;--&#39;, color=&#39;red&#39;) # membuat garis vertikal untuk menghighlight insight plt.axvline(6, ls=&#39;--&#39;, color=&#39;red&#39;) # membuat garis vertikal untuk menghighlight insight plt.text(x=4.5, y=4400, s=&#39;Holiday nSeason&#39;, fontsize=16, color=&#39;red&#39;) # menambahkan teks keterangan plt.stackplot(np.arange(4,7,1), [[4800]], color=&#39;grey&#39;, alpha=0.3) # memberikan blok warna pada area yang dihighlight 2 garis vertikal plt.axvline(9, ls=&#39;--&#39;, color=&#39;red&#39;) # membuat garis vertikal untuk menghighlight insight plt.axvline(11, ls=&#39;--&#39;, color=&#39;magenta&#39;) # membuat garis vertikal untuk menghighlight insight plt.text(x=9.5, y=4400, s=&#39;Holiday nSeason&#39;, fontsize=16, color=&#39;red&#39;) # menambahkan teks keterangan plt.stackplot(np.arange(9,12,1), [[4800]], color=&#39;grey&#39;, alpha=0.3) # memberikan blok warna pada plt.text(x=-0.5, y=5200, s=&quot;Both Hotels Have More Guests During Holiday Season&quot;, fontsize=20, fontweight=&#39;bold&#39;) # memberikan judul yang informatif plt.text(x=-0.5, y=4850, s=&quot;City Hotel has the decreased guests in August and September, while both hotels have less ncustomer during not holiday season (Jan-Mar)&quot;, fontsize=18) # memberikan keterangan tambahan atas judul plt.tight_layout() # mengatur layout dari visualisasi agar tidak terpotong . This is a lot of - specialized and inflexible - code to add an affect to a graph. If I wanted to add this to my own graphs then I would have to do quite a bit of work and math and customization. I want this on any graph at any point without doing all this work. So, let us take it apart and make a function we can all use. We&#39;re not going to use their data since we want to apply it to any random set of data so we&#39;re using random data. . # our pretend dataset data = np.random.randn(50) data = pd.DataFrame(data, columns=[&#39;Random&#39;]) . Much of that is boilerplate and repetition so the summary of a single application would be: . # Initialize the lineplot # Add the left dotted line # Add the right dotted line. # Add the notated text # Add a stackplot to fill the space between the lines . We&#39;ll cover the plot first since it&#39;s the easiest and can go outside of the function we&#39;re trying to build. We&#39;ll use the seaborn lineplot the same. We&#39;ll need use an index similar to the example post while we build this - so a simple range: . p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) . We&#39;ll want to save the graph reference as we&#39;ll need it for later. . Looking at the docs for the horizintal line function, we need: . The x tick value. | The line display type. | The color | We&#39;re going to also modify the line width in our example and fix it; you are welcome to modify this function to accept a thicker value but I like it at 1 and have no current reason to change it. We will need the left and right line points passed to the function as a starting point; we&#39;ll hold the other values constant for now. . def building(p = None, xCoord = None): # Add the lines: plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.figure(figsize=(12,5)) p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) plt.grid() building(p, (1,5)) plt.title(&quot;Working Through Example&quot;) . Text(0.5, 1.0, &#39;Working Through Example&#39;) . Good start. The next problem will be adding the text inside the box and how to position the text automatically. We could brute force - and in some instances we might have no other choice - but that is not convenient while we&#39;re automating the function. How I solved this is to think about what we&#39;re doing in mathematical terms. . Find the center of the x values range. | Translate the text to the left based off the figure size and the length of the text. | So, first we find the middle point by taking the difference of both sides, dividing it by two and then adding that to the left side. . def building(p = None, xCoord = None, text=None, xText=None): # Add the lines: plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) # Ignore this for now; we simply need it y_min, y_max = p.get_ylim() yPos = y_max*.8 xText = xCoord[0] + (xCoord[1] - xCoord[0])/2 plt.text(x=xText, y=yPos, s=text, fontsize=12, color=&#39;red&#39;) plt.figure(figsize=(12,5)) p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) plt.grid() building(p, (1,5), &quot;Solid&quot;) plt.title(&quot;Working Through Example&quot;) . Text(0.5, 1.0, &#39;Working Through Example&#39;) . Next, we&#39;ll need to get the figure size as well as the length of the text. Thankfully, matplotlib allows you to get the dimensions of the figure using .figure.get_size_inches(). . def building(p = None, xCoord = None, text=None, xText=None): # Add the lines: plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) # Ignore this for now; we simply need it y_min, y_max = p.get_ylim() yPos = y_max*.8 width, _ = p.figure.get_size_inches() lenText = len(text) xText = xCoord[0] + (xCoord[1] - xCoord[0])/2 plt.text(x=xText, y=yPos, s=text, fontsize=12, color=&#39;red&#39;) plt.figure(figsize=(12,5)) p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) plt.grid() building(p, (1,5), &quot;Solid&quot;) plt.title(&quot;Working Through Example&quot;) . Text(0.5, 1.0, &#39;Working Through Example&#39;) . Now that we have these values, we can think about how to move them. The answer is this formula: lenText * 3/width. . def building(p = None, xCoord = None, text=None, xText=None): # Add the lines: plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) # Ignore this for now; we simply need it y_min, y_max = p.get_ylim() yPos = y_max*.8 width, _ = p.figure.get_size_inches() lenText = len(text) xText = xCoord[0] + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width plt.text(x=xText, y=yPos, s=text, fontsize=12, color=&#39;red&#39;) plt.figure(figsize=(10,5)) p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) plt.grid() building(p, (1,5), &quot;Solid&quot;) # Showing you can simply run this more than once # to add another box highlight building(p, (20,40), &quot;Something Else&quot;) plt.title(&quot;Working Through Example&quot;) . Text(0.5, 1.0, &#39;Working Through Example&#39;) . So, why does this work? Frankily, I&#39;m not sure quite sure where the three comes from here. Normalizing the number of characters by the figures width make sense but that alone is not enough; you can see this in the below example. . def building(p = None, xCoord = None, text=None, xText=None): # Add the lines: plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) # Ignore this for now; we simply need it y_min, y_max = p.get_ylim() yPos = y_max*.8 width, _ = p.figure.get_size_inches() lenText = len(text) xText = xCoord[0] + (xCoord[1] - xCoord[0])/2 - lenText/width plt.text(x=xText, y=yPos, s=text, fontsize=12, color=&#39;red&#39;) plt.figure(figsize=(10,5)) p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) plt.grid() building(p, (1,5), &quot;Solid&quot;) # Showing you can simply run this more than once # to add another box highlight building(p, (20,40), &quot;Something Else&quot;) plt.title(&quot;Working Through Example&quot;) . Text(0.5, 1.0, &#39;Working Through Example&#39;) . Anyways, we&#39;ll leave what works for now and move to the last problem which is much easier to solve: the stackplot. If you&#39;re not familiar with these graphs then it&#39;s more than likely you&#39;re unfamiliar with the name and not the plot itself. These are used to show percentage change of different categories - usually over time. A good example would be the follow image from the website Geeksforgeeks: . Except what we want is one box full of a single color: grey. Maybe there is a better way to do this but we&#39;re going to base our work off the post and it will work anyways. For a stackplot we need: . The coordinates to fill. | The values. | In the example, the height is filled with a known value: 4800. But this value is the distance from the x-axis and we cannot use this in our example - nor will this generalize. Since their data does not have negative values this works fine but sometimes we will have negative values. What we will need to do is find a fill value. We could use the figure height but there is a case where the total distance can be greater than the figure height strangely enough. What we&#39;ll do is take the sum of the absolute values of the y_min and y_max values + 1. Then, if that values is larger than the height we&#39;ll use that instead. . def building(p = None, xCoord = None, text=None, xText=None): # Ignore this for now; we simply need it y_min, y_max = p.get_ylim() p.set_ylim(y_min, y_max) yPos = y_max*.8 # Collecting height here: width, height = p.figure.get_size_inches() totalDist = abs(y_min) + abs(y_max) + 1 stackFill = height if totalDist &gt; height: stackFill = totalDist lenText = len(text) xText = xCoord[0] + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width # Add the lines: plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.text(x=xText, y=yPos, s=text, fontsize=12, color=&#39;red&#39;) plt.stackplot(np.arange(xCoord[0],xCoord[1]+1,1), [[stackFill]], baseline = &quot;sym&quot;, color=&#39;grey&#39;, alpha=0.3) . plt.figure(figsize=(12,6)) p = sns.lineplot(x=range(0, len(data)), y=&quot;Random&quot;, data=data) plt.grid() building(p, (1,5), &quot;Solid&quot;) # Showing you can simply run this more than once # to add another box highlight # building(p, (20,40), &quot;Something Else&quot;) plt.title(&quot;Working Through Example&quot;) . Text(0.5, 1.0, &#39;Working Through Example&#39;) . You may be asking what the baseline=&quot;sym&quot; argument is since it&#39;s not very descriptive. All that does is changes the relationship from 0 start to Symmetric around the x-axis which we needed for negative values. And, speaking let us test that we can filter for positive values and this all still works. . tmp = data.query(&quot;Random &gt;= 0&quot;) plt.figure(figsize=(15,5)) p = sns.lineplot(x=range(0, len(tmp)), y=&quot;Random&quot;, data=tmp) plt.grid() building(p, (5, 10), &quot;Turtles&quot;) building(p, (21, 25), &quot;Doves&quot;) . Excellent. The Doves to my eyes looks a little off center but that should be good enough. Just add a little bit of error handling and we&#39;ve got a generalized highlight box function! . def addDotBox(p = None, xCoord = None, text=None, xText=None, yText=None, fSize = 12): if not p: raise ValueError(&quot;Plot cannot be None Type.&quot;) if not text: raise ValueError(&quot;text cannot be left blank.&quot;) # Pull/reset the limits: y_min, y_max = p.get_ylim() p.set_ylim(y_min, y_max) # get width, height = p.figure.get_size_inches() totalDist = abs(y_min) + abs(y_max) + 1 stackFill = height if totalDist &gt; height: stackFill = totalDist lenText = len(text) if not xCoord: raise ValueError(&quot;Missing xCoord argument.&quot;) if len(xCoord) != 2: raise ValueError(&quot;There must be two values for xCoord&quot;) if not xText: xText = xCoord[0] + (xCoord[1] - xCoord[0])/2 - 3 * lenText/width if not yText: yText = y_max * .8 plt.axvline(xCoord[0], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.axvline(xCoord[1], ls=&#39;--&#39;, lw=1, color=&#39;red&#39;) plt.text(x=xText, y=y_max*.8 , s=text, fontsize=fSize, color=&#39;red&#39;) plt.stackplot(np.arange(xCoord[0],xCoord[1]+1,1), [[stackFill]], baseline = &quot;sym&quot;, color=&#39;grey&#39;, alpha=0.3) .",
            "url": "https://orgulo.us/python/data/science/exploration/plot/plotting/2022/11/02/simple-hightlight-over-exiting-plot.html",
            "relUrl": "/python/data/science/exploration/plot/plotting/2022/11/02/simple-hightlight-over-exiting-plot.html",
            "date": " • Nov 2, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Game Review: Powerwash Simulator",
            "content": ". Wash away your worries with the soothing sounds of high-pressure water. Fire up your power washer and blast away every speck of dirt and grime you can find, all with the simple satisfaction of power-washing to a sparkling finish. . My friends have been playing this game and I finally picked it up. After doing some cooperative levels with them, I started into the Career Mode of the game. This is the Story Mode and does what you’d expect from a Single Player experience: slowly increasing difficulty through different tools, different surfaces and different buildings with different shapes. There is a story here which is silly; reach the end or spoil it with a Let’s Play if you’d like but I laughed. . The beginning is mostly slow to work through since the early tools are acceptable for the tasks you’re given. If you invest the stars and money into betters tools and nozzles then you can get ahead of the curve - at least in my experience. Discussing which tools are best is somewhat moot since you’ll beat the levels faster - but do you really want even want to? . This brings me to my final thoughs: I’m not sure if this game is fun. After reaching the end of everything, I can say it was relaxing and I definitely accomplished something of value. If nothing else, the game lends itself well to listening to podcasts while you mindlessly clean bathrooms and boats. The core loop works but I found myself somewhat resistant to completing levels around the middle of the game. It was putting on the podcasts to distract me that ended up making the game playable. .",
            "url": "https://orgulo.us/game%20review/game%20design/simulation/2022/11/01/game-review-powerwash-simulator.html",
            "relUrl": "/game%20review/game%20design/simulation/2022/11/01/game-review-powerwash-simulator.html",
            "date": " • Nov 1, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Collecting External Data for Python.",
            "content": "While preparing for some upcoming blog posts taking material from Design and Analysis of Experiments With R by John Lawson, I wanted to convert the problems and solutions from R code to Python code. Diong this will require using the real data and - luckily - the data from the book is online on Github. Due to how these packages are, the data is uploaded and kept as binary data which we can use. Unfortunately, the data is in the .rda format which doesn&#39;t convert easily into python. . There is a package for this to convert the data: pyreadr. Which we&#39;re doing to use to convert the data into a dataframe Python understands. Sadly, this package doesn&#39;t handle urls so we&#39;ll need to download the data first. We could clone out the whole repository to collect the data but then we&#39;d have to start manually managing the data - which I don&#39;t want to do. . After a bit of working around, we can use the tempfile builtin package from Python to create a temporary file to dump the data into. This is useful since these will be deleted after it&#39;s .close() is called on the file. But, we&#39;ll want a Named version since we want this accessible to the file system: . This function operates exactly as TemporaryFile() does, except that the file is guaranteed to have a visible name in the file system (on Unix, the directory entry is not unlinked). That name can be retrieved from the name attribute of the returned file-like object. Whether the name can be used to open the file a second time, while the named temporary file is still open, varies across platforms (it can be so used on Unix; it cannot on Windows). . Source . We&#39;ll use the requests library to pull the data from the internet since it&#39;s builtin and easy to use. . # !pip install pyreadr import pyreadr as pyr import tempfile as tmp import requests as r . One caveat here is that you&#39;ll need to rewind the read location in the file to read the temporary file otherwise you&#39;ll get an LibrdataError: Unable to read from file. . with tmp.NamedTemporaryFile() as f: something = r.get(&quot;https://github.com/cran/daewr/raw/master/data/Apo.rda&quot;).content f.write(something) f.seek(0) data = pyr.read_r(f.name)[&#39;Apo&#39;] data.head(15).T . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 . lab A | A | A | A | A | A | A | B | B | B | B | B | B | B | B | . conc 1.195 | 1.144 | 1.167 | 1.249 | 1.177 | 1.217 | 1.187 | 1.155 | 1.173 | 1.171 | 1.175 | 1.153 | 1.139 | 1.185 | 1.144 | . There we go! You can use this as a simple way to collect data from the internet and feed it into a package which doesn&#39;t support urls to read in data. You can expect its usage in the near future while I work through the textbook. .",
            "url": "https://orgulo.us/python/data/r/rda/download/2022/10/26/how-to-quickly-use-other-formats-in-python.html",
            "relUrl": "/python/data/r/rda/download/2022/10/26/how-to-quickly-use-other-formats-in-python.html",
            "date": " • Oct 26, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Something Cute Or Something Dangerous",
            "content": "Today we&#39;re going to do something a tad simpier since the last attempt to build an image Classification model was too ambitious. Today we&#39;re going to build an image classifier model to tell the difference between Ferrets and Dragons. These are categories I&#39;m far more familiar with and much more comfortable telling apart. After all, one is a floofer and the other is a scaley boi. . We&#39;re going to be using the same code and the same process as before. As a guideline, if you&#39;re writing the same code more than twice then you should turn it into a function; if we&#39;re writing the same notebook code more than twice then it&#39;s time to build a template. By Template I mean a notebook that contains the boilerplate code which will be constaintly used over and over again. There may be a better name for this concept but looking over other words they don&#39;t fit. And, the template concept is already something used in Web Design - such as Jinja. Although, . The Actual Work . We&#39;ll start with our normal imports for working on these problems. Note that we didn&#39;t need to import pandas and such since fastai is doing this for us. . import os from pathlib import Path from time import sleep from duckduckgo_search import ddg_images from fastdownload import download_url from fastcore.all import * from fastai.vision.all import * # This is for the CNN learner. . As before, we&#39;re going to reuse the image searching and download function used by Jeremy since it&#39;s so useful. . # This is a function from the notebook: def search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot(&#39;image&#39;) . dest = Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-ferret.png&#39;) download_url(urls[0], dest, show_progress=False) im = Image.open(dest) im.to_thumb(420) . What a cute litte floofer! Now we&#39;ll collect our data for training - and remove the failed images. . searches = &#39;ferret&#39;,&#39;dragon&#39; path = Path(&#39;..&#39;, &#39;__data&#39;, &#39;ferret_or_dragon&#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) sleep(10) # Pause between searches to avoid over-loading server and blocking responses resize_images(path/o, max_size=400, dest=path/o) . /usr/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . 5 . I still need to write up a post about these Data Block objects since they&#39;re a good idea and might be able to generalize them beyond just Machine Learning. . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=71), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch(max_n=12) . And, now the training! . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(7) . /home/ranuse/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#39;pretrained&#39; is deprecated since 0.13 and will be removed in 0.15, please use &#39;weights&#39; instead. warnings.warn( /home/ranuse/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#39;weights&#39; are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights. warnings.warn(msg) . epoch train_loss valid_loss error_rate time . 0 | 0.740871 | 0.012448 | 0.000000 | 00:04 | . epoch train_loss valid_loss error_rate time . 0 | 0.084491 | 0.001599 | 0.000000 | 00:02 | . 1 | 0.049812 | 0.000780 | 0.000000 | 00:02 | . 2 | 0.032287 | 0.000196 | 0.000000 | 00:02 | . 3 | 0.023381 | 0.000115 | 0.000000 | 00:02 | . 4 | 0.018074 | 0.000076 | 0.000000 | 00:02 | . 5 | 0.014552 | 0.000082 | 0.000000 | 00:02 | . 6 | 0.012025 | 0.000100 | 0.000000 | 00:02 | . The model doing this well isn&#39;t surprising since the categories we&#39;ve picked are intentionally distinct. After all, the point of this post is to show how simple this can be. . So, now we check to see if it is a Floofer. . is_ferret,_,probs = learn.predict(PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-ferret.png&#39;))) print(f&quot;This is a: {is_ferret}.&quot;) print(f&quot;Probability it&#39;s a Floofer: {probs[1]:.4f}&quot;) . This is a: ferret. Probability it&#39;s a Floofer: 1.0000 . You may have caught there is a single difference in the code here. I changed the probs[0] to probs[1] to pull the probability. I&#39;m not sure how or why it&#39;s returning like this but the probability tensor is reversed even though the category is correct. If I do the Dragon category as a example: . learn.predict(PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-dragon.png&#39;))) . (&#39;dragon&#39;, TensorBase(0), TensorBase([1.0000e+00, 1.3599e-08])) . ... then we see that the category is correct and the probability tensor for Dragon is now in index 0 instead of ferret. We can check the this under learn.dls.vocab and the Dragon category is in fact in index 0: . learn.dls.vocab . [&#39;dragon&#39;, &#39;ferret&#39;] . I would not of expected to need to check this for such a simple model but from now on I&#39;ll need to keep in mind the order. So, let&#39;s wright some code quick so we don&#39;t have to think about this again. We&#39;ll attach these together using a dictionary. . _,_,probs = learn.predict(PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-dragon.png&#39;))) dict(zip(learn.dls.vocab, probs)) . {&#39;dragon&#39;: TensorBase(1.), &#39;ferret&#39;: TensorBase(1.3599e-08)} . Using this now, we can simply ask for the probability based on the category we care about. Moving this a step forward, we can turn this into a function now for future use: . def paired_categories(learner, impath): _,_,probs = learner.predict(PILImage.create(impath)) return dict(zip(learn.dls.vocab, probs)) . pCats = paired_categories(learn, Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-dragon.png&#39;)) pCats . {&#39;dragon&#39;: TensorBase(1.), &#39;ferret&#39;: TensorBase(1.3599e-08)} . pCats[&#39;dragon&#39;] . TensorBase(1.) . Conclusions . There we go! Now we have a model that can tell a floofy noodle from a scaley noodle! .",
            "url": "https://orgulo.us/machine%20learning/deep%20learning/fastai/lectures/lecture/2022/10/19/ferrets-or-dragons.html",
            "relUrl": "/machine%20learning/deep%20learning/fastai/lectures/lecture/2022/10/19/ferrets-or-dragons.html",
            "date": " • Oct 19, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Three.",
            "content": "This is the third post in a series about collecting, cleaning and analyzing which Jobs are worth doing in The Cycle: Frontier game. If you&#39;ve found this post before the others I would recommend the other posts first to catch up: Post One and Post Two. We&#39;ll pick up where we left off having separate datasets for the faction rewards as well as all the loot. Let&#39;s get our imports! . import pandas as pd # for the data. import numpy as np # for a NaN type import matplotlib.pyplot as plt # For plotting, and some customization of plots. import seaborn as sns # For pretty plots. import requests as r # For downloading from websites # Fix the size of the graphs sns.set(rc={&quot;figure.figsize&quot;:(11, 8)}) . There are two tasks we&#39;ll need to adjust before we continue. The first is that our Tasks work was only for one faction so we&#39;ll need to loop through each faction&#39;s tasks and then append them all together into a single dataset; the code is included simply for observation. . def breakLoot(taskString, index=0): parts = taskString.split(&#39; &#39;, maxsplit=1) if index == 0: return int(parts[index]) elif index == 1: return parts[index] else: # This shouldn&#39;t be called. return None tasks = [] for index in range(0,3): tasksSubset = siteJobs[index][[&quot;Name&quot;, &quot;Description&quot;, &quot;Tasks&quot;]].copy() tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()] tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(&quot;Kill&quot;)] regex = r&quot;( d+ s[ w]+ s[ w]+)&quot; tmp = tasksSubset.Tasks.str.extractall(regex) count = tmp.reset_index()[0].apply(breakLoot).values aLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values tmp = tmp.assign( count = count, loot = aLoot ) nameDescriptSlice = tasksSubset.loc[tmp.reset_index()[&quot;level_0&quot;], [&#39;Name&#39;, &#39;Description&#39;]] tmp = tmp.assign( name = nameDescriptSlice.Name.values, description = nameDescriptSlice.Description.values ) taskSlice = tmp.reset_index().drop([ &#39;level_0&#39;, &#39;match&#39;, 0 ], axis =1 ) taskSlice = taskSlice[[&#39;name&#39;, &#39;count&#39;, &#39;loot&#39;, &#39;description&#39;]] tasks.append(taskSlice) tasks = pd.concat([*tasks]) . The second task is that we need a single dataset for all the job rewards. We&#39;ll definitely want to tag each so we don&#39;t lose track of which task belongs to which Faction. . # This is new; stop inserting this trash: # want the faction when we join them all: korolevRewards[&#39;Faction&#39;] = &quot;Korolev&quot; icaRewards[&#39;Faction&#39;] = &quot;ICA&quot; osirisRewards[&#39;Faction&#39;] = &quot;Osiris&quot; allJobs = pd.concat([korolevRewards, icaRewards, osirisRewards]) # Proof they&#39;re all in there: allJobs.groupby(&#39;Faction&#39;).take([0]) . Units Rewards Job . Faction . ICA 1 4400 | K-Marks | Water Filtration System | . Korolev 1 3800 | K-Marks | New Mining Tools | . Osiris 1 2200 | K-Marks | Lab equipment | . For this analysis, we&#39;re really only interested in the Kmarks so we&#39;ll need to only pull those rows; we&#39;ll do some work with the others later but for now just the money. . lootKMarks = loot.query(&#39;Name == &quot;K-Marks&quot;&#39;) lootKMarks.head(8) . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 1 150.0 | K-Marks | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 6 570.0 | K-Marks | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . 11 854.0 | K-Marks | Rare | Yes x11 | Yes x25 | Yes | No | Clear Veltecite | . 16 1922.0 | K-Marks | Epic | Yes x7 | Yes x9 | Yes | No | Pure Veltecite | . 21 6487.0 | K-Marks | Legendary | Yes x4 | Yes x2 | Yes | No | Veltecite Heart | . 26 570.0 | K-Marks | Uncommon | Yes x36 | Yes x7 | Yes | No | Brittle Titan Ore | . 31 854.0 | K-Marks | Rare | Yes x6 | Yes x8 | Yes | No | Titan Ore | . 36 150.0 | K-Marks | Common | Yes x46 | Yes x1 | Yes | No | Nickel | . Joins are a complicated topic which I&#39;m not going to flesh out here. In this instance, what we want is the loot table with the Kmark value connected to the jobs table with respect to the name of the loot. And, we&#39;ll tell Pandas to join those together below making sure that as long as it exists in the left table that it gets connected to something in the right table. . taskLoot = tasks.merge(lootKMarks[[&#39;Loot&#39;, &#39;Unit&#39;]], left_on=&#39;loot&#39;, right_on=&#39;Loot&#39;, how=&#39;outer&#39;) taskLoot.head() . name count loot description Loot Unit . 0 New Mining Tools | 2.0 | Hydraulic Piston | We are producing new Mining Tools for new Pros... | Hydraulic Piston | 338.0 | . 1 Excavator Improvements | 3.0 | Hydraulic Piston | The suspension on our mining excavators need i... | Hydraulic Piston | 338.0 | . 2 New Mining Tools | 10.0 | Hardened Metals | We are producing new Mining Tools for new Pros... | Hardened Metals | 150.0 | . 3 Automated Security | 16.0 | Hardened Metals | We will have to build new turrets to help prot... | Hardened Metals | 150.0 | . 4 Air Lock Upgrades | 12.0 | Hardened Metals | Our engineers designed a safer airlock system ... | Hardened Metals | 150.0 | . We&#39;ll now multiply the count of the loot times their values to get the cost per resource in the task. . taskLoot[&#39;Cost&#39;] = taskLoot[&#39;count&#39;] * taskLoot[&#39;Unit&#39;] taskLoot.head() . name count loot description Loot Unit Cost . 0 New Mining Tools | 2.0 | Hydraulic Piston | We are producing new Mining Tools for new Pros... | Hydraulic Piston | 338.0 | 676.0 | . 1 Excavator Improvements | 3.0 | Hydraulic Piston | The suspension on our mining excavators need i... | Hydraulic Piston | 338.0 | 1014.0 | . 2 New Mining Tools | 10.0 | Hardened Metals | We are producing new Mining Tools for new Pros... | Hardened Metals | 150.0 | 1500.0 | . 3 Automated Security | 16.0 | Hardened Metals | We will have to build new turrets to help prot... | Hardened Metals | 150.0 | 2400.0 | . 4 Air Lock Upgrades | 12.0 | Hardened Metals | Our engineers designed a safer airlock system ... | Hardened Metals | 150.0 | 1800.0 | . Now we&#39;ll just group by the name of the task to and take the sume of each to get the total cost per task. . taskLoot[[&#39;name&#39;, &#39;Cost&#39;]].groupby(&#39;name&#39;).sum().head() . Cost . name . A Craving 4048.0 | . A Solution 10125.0 | . A new Energy Source 0.0 | . A new type of Alloy 1800.0 | . Air Lock Upgrades 3900.0 | . This is the first piece we&#39;ll need to get the results we&#39;re after; the other part is all the jobs we collected in the previous post - and brought here. . results = allJobs.query(&#39;Rewards == &quot;K-Marks&quot;&#39;).merge( taskLoot[[&#39;name&#39;, &#39;Cost&#39;]].groupby(&#39;name&#39;).sum(), left_on=&quot;Job&quot;, right_on=&quot;name&quot;, how=&#39;left&#39;) results.head() . Units Rewards Job Faction Cost . 0 3800 | K-Marks | New Mining Tools | Korolev | 2176.0 | . 1 11000 | K-Marks | Explosive Excavation | Korolev | 6836.0 | . 2 6900 | K-Marks | Mining Bot | Korolev | 1014.0 | . 3 7600 | K-Marks | None of your Business | Korolev | 1800.0 | . 4 10000 | K-Marks | Insufficient Processing Power | Korolev | 3845.0 | . Now we have the rewards and the cost per task we can finally calculate the Balance of each job! . results[&#39;Balance&#39;] = results[&#39;Units&#39;] - results[&#39;Cost&#39;] results.groupby(&#39;Faction&#39;).take([0,3]) . Units Rewards Job Cost Balance . Faction . ICA 46 4400 | K-Marks | Water Filtration System | 2700.0 | 1700.0 | . 49 12000 | K-Marks | Air Lock Upgrades | 3900.0 | 8100.0 | . Korolev 0 3800 | K-Marks | New Mining Tools | 2176.0 | 1624.0 | . 3 7600 | K-Marks | None of your Business | 1800.0 | 5800.0 | . Osiris 94 2200 | K-Marks | Lab equipment | 676.0 | 1524.0 | . 97 26000 | K-Marks | Cretins | 6300.0 | 19700.0 | . Conclusions - and Questions! . So, now that we have the data let&#39;s work on some questions! . Given any particular faction, which task has the highest Balance? . balanceMax = results.Balance.max() results.loc[results.Balance == balanceMax] . Units Rewards Job Faction Cost Balance . 91 227000 | K-Marks | Striking Big | ICA | 0.0 | 227000.0 | . Looks like it is the Job Striking Big. What&#39;s this job about?: . Damn it! We ran out of Fuel for our Radiation Shields here on the Station. And there&#39;s no replacement for Nanite Infused Crude Oil. You know what to do. . It&#39;s collecting Oil Cannisters! Those are worth a lot of money so it&#39;s not a surprise that a task which wants you to collect them would also pay out so highly. . What about the job that pays the least? . balanceMin = results.Balance.min() results.loc[results.Balance == balanceMin] . Units Rewards Job Faction Cost Balance . 94 2200 | K-Marks | Lab equipment | Osiris | 676.0 | 1524.0 | . Well, it our good friends Osiris failing the station again. That looks like a low level job from a cost and reward that low. Not really much of a surprise here. . So, what about the mean balance - and how many tasks are there which reward more than that average value? . balanceMean = results.Balance.mean() f&quot;${round(balanceMean, 2)}&quot; . &#39;$20600.89&#39; . len( results.loc[results.Balance &gt;= balanceMean]) . 40 . # How many are there per faction? results.loc[ results.Balance &gt;= balanceMean ] .groupby(&#39;Faction&#39;) .count() .reset_index() .rename({&#39;Units&#39;:&#39;Count&#39;}, axis=1)[[ &#39;Faction&#39;, &#39;Count&#39; ]] . Faction Count . 0 ICA | 14 | . 1 Korolev | 14 | . 2 Osiris | 12 | . And, unsurpisingly Osiris is slightly behind but they&#39;re mostly the same. . Ok, so what&#39;s the cost of the Job that inspired all this work: And Two Smoking Barrels? . results.loc[results.Job.str.contains(&quot;Barrel&quot;)] . Units Rewards Job Faction Cost Balance . 39 19000 | K-Marks | And two smoking Barrels | Korolev | 0.0 | 19000.0 | . Wait a minute! There should be a cost here and it is missing! Well, that&#39;s because there are no guns in the loot table. So, there is no cost when we connect the data together. And, after some further checking there are some missing values here. That quest we checked above with the Oil? They&#39;re not named the same betwee the tables: NiC Oil vs NiC Oil Cannister. It&#39;s not the only one like this either. . Looks like we&#39;ve got more work to do. .",
            "url": "https://orgulo.us/game/python/data/science/exploration/cycle/frontier/2022/10/14/cycle-calculate-job-values.html",
            "relUrl": "/game/python/data/science/exploration/cycle/frontier/2022/10/14/cycle-calculate-job-values.html",
            "date": " • Oct 14, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part Two.",
            "content": "This is Part Two of a Series towards scraping, cleaning and analyzing the Jobs for The Cycle: Frontier. If you haven&#39;t read Part One then I&#39;d suggest start there. We&#39;re picking up now with cleaning the tasks to complete for the job instead. So, let&#39;s get started! We&#39;ll pull our normal libraries for working on projects like this. . import pandas as pd # for the data. import numpy as np # for a NaN type import matplotlib.pyplot as plt # For plotting, and some customization of plots. import seaborn as sns # For pretty plots. # Fix the size of the graphs sns.set(rc={&quot;figure.figsize&quot;:(11, 8)}) . We&#39;re actually going to be using the same data table as before from the Official Wiki and the Jobs Page . Like before, wer&#39;e going to use the same read_html() call targetting the name class. . url = &quot;https://thecyclefrontier.wiki/wiki/Jobs&quot; site = pd.read_html(url, match=&quot;Name&quot;, converters = { &quot;Name&quot;: str, &quot;Description&quot;: str, &quot;Unlocked&quot;: int, &quot;Tasks&quot;: str, &quot;Rewards&quot;: str}) . And here is the data in the weird rows and columns like before: . # Weird Problem: Data Looks funny, can still use this: site[0].head(8) . Name Description Unlock Level Difficulty Tasks Rewards . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | 4.0 | Easy | Collect: 2 Hydraulic Piston 10 Hardened Metals | 3800 K-Marks 1 Korolev Scrip 15 Korolev R... | . 1 3800 | K-Marks | NaN | NaN | NaN | NaN | . 2 1 | Korolev Scrip | NaN | NaN | NaN | NaN | . 3 15 | Korolev Reputation | NaN | NaN | NaN | NaN | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | 7.0 | Medium | Collect: 4 Derelict Explosives | 11000 K-Marks 8 Korolev Scrip 52 Korolev ... | . 5 11000 | K-Marks | NaN | NaN | NaN | NaN | . 6 8 | Korolev Scrip | NaN | NaN | NaN | NaN | . 7 52 | Korolev Reputation | NaN | NaN | NaN | NaN | . We&#39;re going to make a copy of the specific columns we want to avoid any strange insert/update issues. . tasksSubset = site[0][[&quot;Name&quot;, &quot;Description&quot;, &quot;Tasks&quot;]].copy() tasksSubset . Name Description Tasks . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | Collect: 2 Hydraulic Piston 10 Hardened Metals | . 1 3800 | K-Marks | NaN | . 2 1 | Korolev Scrip | NaN | . 3 15 | Korolev Reputation | NaN | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | Collect: 4 Derelict Explosives | . ... ... | ... | ... | . 183 470 | Korolev Reputation | NaN | . 184 No Expiry Date | There you are, finally! There&#39;s been an accide... | Collect: 10 Old Medicine | . 185 6300 | K-Marks | NaN | . 186 9 | Korolev Scrip | NaN | . 187 62 | Korolev Reputation | NaN | . 188 rows × 3 columns . # get rid of the middle stuff we don&#39;t need. tasksSubset = tasksSubset[ ~tasksSubset.Tasks.isna()] tasksSubset.head(15) . Name Description Tasks . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | Collect: 2 Hydraulic Piston 10 Hardened Metals | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | Collect: 4 Derelict Explosives | . 8 Mining Bot | Our engineers have designed an autonomous mini... | Collect: 2 Zero Systems CPU 3 Ball Bearings | . 12 None of your Business | Prospector. We need Toxic Glands. Don&#39;t ask qu... | Collect: 2 Toxic Glands | . 16 Insufficient Processing Power | Prospector! The Zero Systems CPU you brought u... | Collect: 1 Master Unit CPU | . 20 Excavator Improvements | The suspension on our mining excavators need i... | Collect: 2 Co-TEC MultiTool 3 Ball Bearings 3 ... | . 24 A new type of Alloy | Our scientists are confident they can create a... | Collect: 4 Hardened Bone Plates 12 Compound Sh... | . 28 Automated Security | We will have to build new turrets to help prot... | Collect: 5 Zero Systems CPU 16 Hardened Metals | . 32 Energy Crisis | Veltecite supplies are low, but we need energy... | Collect: 4 Miniature Reactor | . 36 Classified I | Prospector! We need Derelict Explosives, Maste... | Collect: 10 Derelict Explosives 2 Master Unit ... | . 40 Clear Veltecite | The Veltecite you brought us the other day is ... | Collect: 2 Clear Veltecite | . 44 Time to Focus | One of our miners searching the Jungle for Foc... | Kill 6 Creatures at Jungle Collect: 4 Focus Cr... | . 48 Pure Veltecite | The Clear Veltecite was an improvement, we gai... | Collect: 2 Pure Veltecite | . 52 Titans of Industry | Scouts have found Titan Ore deposits on Fortun... | Collect: 2 Titan Ore 6 Altered Nickel | . 56 Crystal Frenzy | We&#39;re working on a new type of laser for our l... | Collect: 2 Clear Veltecite 8 Focus Crystal | . As I discussed in the previous post, there are three kinds of jobs: Collect, Deposit and Kill. The Collect and Deposit jobs are fine since they involved something easily quantifiable: loot. However, the Kill quests present a very real problem since there is no simple way to address quanitfying them. For killing creatures, maybe we could take the sum of their expected drops and their rate of drop and include that as part of the rewards? Of course, the player may simply choose not to pick any of that up. . Another problem is killing players; how much is a player kill actually worth? And, the difficulty of killing players is connected to the skill level of each player - which we also cannot know. Therefore, I&#39;ve elected to remove the Kill Jobs from the analysis. . # anything with kill just remove until I can think of a better way to deal with this. tasksSubset = tasksSubset[ ~tasksSubset.Tasks.str.contains(&quot;Kill&quot;)] tasksSubset.head(15) . Name Description Tasks . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | Collect: 2 Hydraulic Piston 10 Hardened Metals | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | Collect: 4 Derelict Explosives | . 8 Mining Bot | Our engineers have designed an autonomous mini... | Collect: 2 Zero Systems CPU 3 Ball Bearings | . 12 None of your Business | Prospector. We need Toxic Glands. Don&#39;t ask qu... | Collect: 2 Toxic Glands | . 16 Insufficient Processing Power | Prospector! The Zero Systems CPU you brought u... | Collect: 1 Master Unit CPU | . 20 Excavator Improvements | The suspension on our mining excavators need i... | Collect: 2 Co-TEC MultiTool 3 Ball Bearings 3 ... | . 24 A new type of Alloy | Our scientists are confident they can create a... | Collect: 4 Hardened Bone Plates 12 Compound Sh... | . 28 Automated Security | We will have to build new turrets to help prot... | Collect: 5 Zero Systems CPU 16 Hardened Metals | . 32 Energy Crisis | Veltecite supplies are low, but we need energy... | Collect: 4 Miniature Reactor | . 36 Classified I | Prospector! We need Derelict Explosives, Maste... | Collect: 10 Derelict Explosives 2 Master Unit ... | . 40 Clear Veltecite | The Veltecite you brought us the other day is ... | Collect: 2 Clear Veltecite | . 48 Pure Veltecite | The Clear Veltecite was an improvement, we gai... | Collect: 2 Pure Veltecite | . 52 Titans of Industry | Scouts have found Titan Ore deposits on Fortun... | Collect: 2 Titan Ore 6 Altered Nickel | . 56 Crystal Frenzy | We&#39;re working on a new type of laser for our l... | Collect: 2 Clear Veltecite 8 Focus Crystal | . 60 Geologist | You got time for a job, Prospector? The sample... | Collect: 2 Pure Veltecite 1 Pure Focus Crystal | . So, each Job can request you collect multiple loots as well as more than one type of loot. Here is a good example of what I mean by this: . tasksSubset.loc[28].Tasks . &#39;Collect: 5 Zero Systems CPU 16 Hardened Metals&#39; . As you can see, this task requires you to collect both Zero System CPUs as well as Hardened Metals - and a good number of them. What we want is to not only extract each type of loot independently of each other but also to keep the count paired with the loot type. . I would like to take this moment to thank the developers of Pandas. I spent a bit of time thinking about how I would solve this and they had already included a solution to this problem: extractall(). What this does is allows you to pass Regular Expressions and it will then pull out anything in the string which matches. It even puts them into their own separate rows! Again, thank you! . For Regular Expressions, this is something you will have to learn on your own. I used a website to test and build mine from an example row; there is plenty of documentaiton about how to use these. . regex = r&quot;( d+ s[ w]+ s[ w]+)&quot; tmp = tasksSubset.Tasks.str.extractall(regex) tmp.head(15) . 0 . match . 0 0 2 Hydraulic Piston | . 1 10 Hardened Metals | . 4 0 4 Derelict Explosives | . 8 0 2 Zero Systems | . 1 3 Ball Bearings | . 12 0 2 Toxic Glands | . 16 0 1 Master Unit | . 20 0 3 Ball Bearings | . 1 3 Hydraulic Piston | . 24 0 4 Hardened Bone | . 1 12 Compound Sheets | . 28 0 5 Zero Systems | . 1 16 Hardened Metals | . 32 0 4 Miniature Reactor | . 36 0 10 Derelict Explosives | . Perfect! Now we have all the different loot and we got to keep the row&#39;s index for later when we&#39;ll attach the job name and description. Before that though, we&#39;ll need to do some work to separate the count and the loot type into their own columns. While I&#39;m sure there is a better way to do this, I could not think of one so we&#39;re going to write a function to break the loot and count apart and then return them. . There is a solid function for this already called .split() and we&#39;re going to use it to split on spaces but we since some of the loot is multiple words we need to force it to only split once. . example = tmp.reset_index()[0][1] parts = example.split(&#39; &#39;, maxsplit=1) number, loot = int(parts[0]), parts[1] number, loot . (10, &#39;Hardened Metals&#39;) . Now we&#39;ll create the function. What we can do here though is return either the count value or the loot value depending on an index passed: 0 for count and 1 for loot. . def breakLoot(taskString, index=0): parts = taskString.split(&#39; &#39;, maxsplit=1) if index == 0: return int(parts[index]) elif index == 1: return parts[index] else: # This shouldn&#39;t be called. return None . Now we just run two .apply() calls to get the values out: . count = tmp.reset_index()[0].apply(breakLoot).values aLoot = tmp.reset_index()[0].apply(breakLoot, index=1).values . And, then assign them to our brand new columns for them. . tmp = tmp.assign( count = count, loot = aLoot ) tmp.head(15) . 0 count loot . match . 0 0 2 Hydraulic Piston | 2 | Hydraulic Piston | . 1 10 Hardened Metals | 10 | Hardened Metals | . 4 0 4 Derelict Explosives | 4 | Derelict Explosives | . 8 0 2 Zero Systems | 2 | Zero Systems | . 1 3 Ball Bearings | 3 | Ball Bearings | . 12 0 2 Toxic Glands | 2 | Toxic Glands | . 16 0 1 Master Unit | 1 | Master Unit | . 20 0 3 Ball Bearings | 3 | Ball Bearings | . 1 3 Hydraulic Piston | 3 | Hydraulic Piston | . 24 0 4 Hardened Bone | 4 | Hardened Bone | . 1 12 Compound Sheets | 12 | Compound Sheets | . 28 0 5 Zero Systems | 5 | Zero Systems | . 1 16 Hardened Metals | 16 | Hardened Metals | . 32 0 4 Miniature Reactor | 4 | Miniature Reactor | . 36 0 10 Derelict Explosives | 10 | Derelict Explosives | . And, there we go! We have our columns how we want them. Now we just need to work on getting the Name and Description values attached to our new data frame. One way to do this would be to do some sort of merge or join based on the index we&#39;ve saved. Or, we can do something even easier! . If we look at the rows when we do an index reset: . tmp.reset_index().head(8) . level_0 match 0 count loot . 0 0 | 0 | 2 Hydraulic Piston | 2 | Hydraulic Piston | . 1 0 | 1 | 10 Hardened Metals | 10 | Hardened Metals | . 2 4 | 0 | 4 Derelict Explosives | 4 | Derelict Explosives | . 3 8 | 0 | 2 Zero Systems | 2 | Zero Systems | . 4 8 | 1 | 3 Ball Bearings | 3 | Ball Bearings | . 5 12 | 0 | 2 Toxic Glands | 2 | Toxic Glands | . 6 16 | 0 | 1 Master Unit | 1 | Master Unit | . 7 20 | 0 | 3 Ball Bearings | 3 | Ball Bearings | . ... we can see that the column level_0 actaully contains duplicate indexes from our matches. So, the values Hydraulic Piston and Hardened Metals both are associated with Task with index 0. As long as we can use that index to get duplicate values then we can just pull all the Name and Descriptions in their matching order. . tasksSubset.loc[tmp.reset_index().loc[:5, &quot;level_0&quot;], [&#39;Name&#39;, &#39;Description&#39;]] . Name Description . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | . 8 Mining Bot | Our engineers have designed an autonomous mini... | . 8 Mining Bot | Our engineers have designed an autonomous mini... | . 12 None of your Business | Prospector. We need Toxic Glands. Don&#39;t ask qu... | . ... which is exactly what we get! Duplicates! Time to slice it out and then assign the values. . nameDescriptSlice = tasksSubset.loc[tmp.reset_index()[&quot;level_0&quot;], [&#39;Name&#39;, &#39;Description&#39;]] tmp = tmp.assign( name = nameDescriptSlice.Name.values, description = nameDescriptSlice.Description.values ) tmp.head(15) . 0 count loot name description . match . 0 0 2 Hydraulic Piston | 2 | Hydraulic Piston | New Mining Tools | We are producing new Mining Tools for new Pros... | . 1 10 Hardened Metals | 10 | Hardened Metals | New Mining Tools | We are producing new Mining Tools for new Pros... | . 4 0 4 Derelict Explosives | 4 | Derelict Explosives | Explosive Excavation | One of our mines collapsed with valuable equip... | . 8 0 2 Zero Systems | 2 | Zero Systems | Mining Bot | Our engineers have designed an autonomous mini... | . 1 3 Ball Bearings | 3 | Ball Bearings | Mining Bot | Our engineers have designed an autonomous mini... | . 12 0 2 Toxic Glands | 2 | Toxic Glands | None of your Business | Prospector. We need Toxic Glands. Don&#39;t ask qu... | . 16 0 1 Master Unit | 1 | Master Unit | Insufficient Processing Power | Prospector! The Zero Systems CPU you brought u... | . 20 0 3 Ball Bearings | 3 | Ball Bearings | Excavator Improvements | The suspension on our mining excavators need i... | . 1 3 Hydraulic Piston | 3 | Hydraulic Piston | Excavator Improvements | The suspension on our mining excavators need i... | . 24 0 4 Hardened Bone | 4 | Hardened Bone | A new type of Alloy | Our scientists are confident they can create a... | . 1 12 Compound Sheets | 12 | Compound Sheets | A new type of Alloy | Our scientists are confident they can create a... | . 28 0 5 Zero Systems | 5 | Zero Systems | Automated Security | We will have to build new turrets to help prot... | . 1 16 Hardened Metals | 16 | Hardened Metals | Automated Security | We will have to build new turrets to help prot... | . 32 0 4 Miniature Reactor | 4 | Miniature Reactor | Energy Crisis | Veltecite supplies are low, but we need energy... | . 36 0 10 Derelict Explosives | 10 | Derelict Explosives | Classified I | Prospector! We need Derelict Explosives, Maste... | . Finally, we&#39;ll drop all those extra columns we don&#39;t need. . tasks = tmp.reset_index().drop([ &#39;level_0&#39;, &#39;match&#39;, 0 ], axis =1 ) tasks = tasks[[&#39;name&#39;, &#39;count&#39;, &#39;loot&#39;, &#39;description&#39;]] tasks.head(15) . name count loot description . 0 New Mining Tools | 2 | Hydraulic Piston | We are producing new Mining Tools for new Pros... | . 1 New Mining Tools | 10 | Hardened Metals | We are producing new Mining Tools for new Pros... | . 2 Explosive Excavation | 4 | Derelict Explosives | One of our mines collapsed with valuable equip... | . 3 Mining Bot | 2 | Zero Systems | Our engineers have designed an autonomous mini... | . 4 Mining Bot | 3 | Ball Bearings | Our engineers have designed an autonomous mini... | . 5 None of your Business | 2 | Toxic Glands | Prospector. We need Toxic Glands. Don&#39;t ask qu... | . 6 Insufficient Processing Power | 1 | Master Unit | Prospector! The Zero Systems CPU you brought u... | . 7 Excavator Improvements | 3 | Ball Bearings | The suspension on our mining excavators need i... | . 8 Excavator Improvements | 3 | Hydraulic Piston | The suspension on our mining excavators need i... | . 9 A new type of Alloy | 4 | Hardened Bone | Our scientists are confident they can create a... | . 10 A new type of Alloy | 12 | Compound Sheets | Our scientists are confident they can create a... | . 11 Automated Security | 5 | Zero Systems | We will have to build new turrets to help prot... | . 12 Automated Security | 16 | Hardened Metals | We will have to build new turrets to help prot... | . 13 Energy Crisis | 4 | Miniature Reactor | Veltecite supplies are low, but we need energy... | . 14 Classified I | 10 | Derelict Explosives | Prospector! We need Derelict Explosives, Maste... | . Conclusions . And, there we have it! Another piece to the puzzle solved. Next we&#39;re going to work to combine all the faction rewards, the job requirements and the loot tables together to finally calculate which jobs you should definitly avoid doing. .",
            "url": "https://orgulo.us/game/python/data/science/exploration/cycle/frontier/2022/10/07/cycle-jobs-part-two.html",
            "relUrl": "/game/python/data/science/exploration/cycle/frontier/2022/10/07/cycle-jobs-part-two.html",
            "date": " • Oct 7, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Downloading and Exploring Loot in Cycle Frontier",
            "content": "First, we&#39;ll pull our normal libraries for working on projects like this. . import pandas as pd # for the data. import numpy as np # for a NaN type import matplotlib.pyplot as plt # For plotting, and some customization of plots. import seaborn as sns # For pretty plots. # Fix the size of the graphs sns.set(rc={&quot;figure.figsize&quot;:(11, 8)}) . Our data source is still going to be the Official Wiki and we&#39;ll be pulling from the Loot Page - which has already started being updated for Season Two. From my previous post, the class zebra is still the best attribute to target to pull the data into a usable Data Frame. . url = &quot;https://thecyclefrontier.wiki/wiki/Loot&quot; site = pd.read_html(url, attrs={&quot;class&quot;:&quot;zebra&quot;})[0] site.head(15) . Image Name Rarity Weight K-Marks K-Marks / Weight Personal Quarters Campaigns Jobs Printing . 0 NaN | Flawed Veltecite | Common | 3.0 | 150 K-Marks 2 Reputation | 50 K-Marks / Weight 0.67 Reputation / Weight | Yes x11 | Yes x10 | Yes | NaN | . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 5 NaN | Cloudy Veltecite | Uncommon | 3.0 | 570 K-Marks 6 Reputation | 190 K-Marks / Weight 2 Reputation / Weight | Yes x8 | Yes x14 | Yes | Yes | . 6 570.00 | K-Marks | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 7 6.00 | Reputation | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 8 190.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 9 2.00 | Reputation / Weight | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 10 NaN | Clear Veltecite | Rare | 3.0 | 854 K-Marks 9 Reputation | 285 K-Marks / Weight 3 Reputation / Weight | Yes x11 | Yes x25 | Yes | Yes | . 11 854.00 | K-Marks | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 12 9.00 | Reputation | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 13 285.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 14 3.00 | Reputation / Weight | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Again, from previous experience, it is best to create a copy of columns that we actually need. . lootSubset = site[[&#39;Image&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Personal Quarters&#39;, &#39;Campaigns&#39;, &#39;Jobs&#39;, &#39;Printing&#39;]].copy() lootSubset.head() . Image Name Rarity Personal Quarters Campaigns Jobs Printing . 0 NaN | Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | NaN | . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | NaN | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | NaN | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | NaN | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | NaN | . Looking at our data, we already have a problem: the Printing column has columns NaN values insted of no. The blank values got converted to NaN when it was imported. While I&#39;m sure there is a way to correct this on import, this isn&#39;t how I&#39;m going to do it. What I&#39;m going to do is find the indexes which contain Yes and then update the other columns. You might be thinking - correctly - that we could simply find the indexes of the other values and then update those. Unfortunately, this is harder and I&#39;ll show you what I mean. . If we check the values, we&#39;ll see there is this nan value which doens&#39;t look like the normal np.NaN value. . lootSubset.Printing.unique(), lootSubset.Printing[0] . (array([nan, &#39;Yes&#39;], dtype=object), nan) . And, if we try to do a logical comparison with the literal value from the data it fails. . lootSubset.Printing.iloc[:5] == lootSubset.Printing[0] . 0 False 1 False 2 False 3 False 4 False Name: Printing, dtype: bool . Here we so the problem with objects and probably the pointers underneath. We could fix this but it&#39;s much easier to reverse what we&#39;re detecting so I&#39;m doing that instead. . # That nan value is weird; will do the opposite filterIndex = lootSubset.Printing == &quot;Yes&quot; lootSubset[~filterIndex].head() . Image Name Rarity Personal Quarters Campaigns Jobs Printing . 0 NaN | Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | NaN | . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | NaN | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | NaN | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | NaN | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | NaN | . I&#39;m not going to spend much time explaining this part; please see the previous posts about how this works. . # Correct it lootSubset.loc[~filterIndex, &quot;Printing&quot;] = &quot;No&quot; lootSubset.head(9) . Image Name Rarity Personal Quarters Campaigns Jobs Printing . 0 NaN | Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | No | . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | No | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | No | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | No | . 5 NaN | Cloudy Veltecite | Uncommon | Yes x8 | Yes x14 | Yes | Yes | . 6 570.00 | K-Marks | NaN | NaN | NaN | NaN | No | . 7 6.00 | Reputation | NaN | NaN | NaN | NaN | No | . 8 190.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | . Looking at the rows, we can see the data we&#39;re after - the Name of the Loot - is in the Name column. Luckily, like the previous post about data extraction, these are in multiples of 5 insted of 4 like before. So, we can simply borrow the same code from before and update the range. . # Change range to 5 instead of 4 index = range( 0, len(lootSubset) - 4, 5) offset = np.array([1, 2, 3, 4]) . tmp = index[0] + offset lootSubset.iloc[tmp] . Image Name Rarity Personal Quarters Campaigns Jobs Printing . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | No | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | No | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | No | . lootSubset.iloc[index].tail() . Image Name Rarity Personal Quarters Campaigns Jobs Printing . 445 NaN | Hardened Metals | Common | Yes x17 | Yes x2 | Yes | Yes | . 450 NaN | Compound Sheets | Common | Yes x18 | Yes x36 | Yes | No | . 455 NaN | Print Resin | Uncommon | Yes x44 | Yes x2 | NaN | Yes | . 460 NaN | Salvaged Insulation | Common | Yes x38 | Yes x30 | NaN | No | . 465 NaN | Aluminum Scrap | Common | Yes x28 | NaN | Yes | Yes | . Now we&#39;ll build the loop for the iteration like before. . # this is how we&#39;ll iterate; proof it works. # refer the .head() call to see this is true. for i in index[:3]: # we&#39;ll need to shift this to index 1 since 0 contains information we want. aLoot = lootSubset.iloc[i, 1] print(f&#39;{aLoot} is at index {i}&#39;) . Flawed Veltecite is at index 0 Cloudy Veltecite is at index 5 Clear Veltecite is at index 10 . Now we need a column to dump the correct values into which is going to be called Loot. We&#39;ll fill these with np.NaN so we can repeat the same trick as the previous post. . lootSubset = lootSubset.assign( Loot = np.NaN ) lootSubset.head() . Image Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 0 NaN | Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | No | NaN | . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | No | NaN | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | No | NaN | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | NaN | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | No | NaN | . So, pull the name of of the loot per subset and then update it like before. . for i in index: # Correct Loot column aLoot = lootSubset.iloc[i, 1] indexes = i + offset lootSubset.iloc[ indexes, 7 ] = aLoot . lootSubset.head(15) . Image Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 0 NaN | Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | No | NaN | . 1 150.00 | K-Marks | NaN | NaN | NaN | NaN | No | Flawed Veltecite | . 2 2.00 | Reputation | NaN | NaN | NaN | NaN | No | Flawed Veltecite | . 3 50.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | Flawed Veltecite | . 4 0.67 | Reputation / Weight | NaN | NaN | NaN | NaN | No | Flawed Veltecite | . 5 NaN | Cloudy Veltecite | Uncommon | Yes x8 | Yes x14 | Yes | Yes | NaN | . 6 570.00 | K-Marks | NaN | NaN | NaN | NaN | No | Cloudy Veltecite | . 7 6.00 | Reputation | NaN | NaN | NaN | NaN | No | Cloudy Veltecite | . 8 190.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | Cloudy Veltecite | . 9 2.00 | Reputation / Weight | NaN | NaN | NaN | NaN | No | Cloudy Veltecite | . 10 NaN | Clear Veltecite | Rare | Yes x11 | Yes x25 | Yes | Yes | NaN | . 11 854.00 | K-Marks | NaN | NaN | NaN | NaN | No | Clear Veltecite | . 12 9.00 | Reputation | NaN | NaN | NaN | NaN | No | Clear Veltecite | . 13 285.00 | K-Marks / Weight | NaN | NaN | NaN | NaN | No | Clear Veltecite | . 14 3.00 | Reputation / Weight | NaN | NaN | NaN | NaN | No | Clear Veltecite | . If we play through the old trick of dropping the Loot columns now we&#39;ll lose valuable data. So, we&#39;ll need a new trick to keep that data. Thankfully, this is a problem I&#39;d already solved at a previous time: we&#39;ll simply fill the values. As a word of caution, filling values can be dangerous to an analysis so if you&#39;re doing this then make sure it will not have a negative impact. In this instance, I&#39;m duplicating the data so that it&#39;s not lost at all. Even then, we&#39;ll need to be cautious in the future about how the data is used. . There are a few techniques for filling in missing values and one such technique is called Fill Forward. What this does is take the values in a column, take that value and then simply inserts it down the rows where it finds NAs. This is exactly what we&#39;re after - but only for those specific columns. . # fix the middle by filling down tmp = lootSubset.iloc[:, 1:7] tmp = tmp.fillna(method=&quot;ffill&quot;) tmp.head(15) . Name Rarity Personal Quarters Campaigns Jobs Printing . 0 Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | No | . 1 K-Marks | Common | Yes x11 | Yes x10 | Yes | No | . 2 Reputation | Common | Yes x11 | Yes x10 | Yes | No | . 3 K-Marks / Weight | Common | Yes x11 | Yes x10 | Yes | No | . 4 Reputation / Weight | Common | Yes x11 | Yes x10 | Yes | No | . 5 Cloudy Veltecite | Uncommon | Yes x8 | Yes x14 | Yes | Yes | . 6 K-Marks | Uncommon | Yes x8 | Yes x14 | Yes | No | . 7 Reputation | Uncommon | Yes x8 | Yes x14 | Yes | No | . 8 K-Marks / Weight | Uncommon | Yes x8 | Yes x14 | Yes | No | . 9 Reputation / Weight | Uncommon | Yes x8 | Yes x14 | Yes | No | . 10 Clear Veltecite | Rare | Yes x11 | Yes x25 | Yes | Yes | . 11 K-Marks | Rare | Yes x11 | Yes x25 | Yes | No | . 12 Reputation | Rare | Yes x11 | Yes x25 | Yes | No | . 13 K-Marks / Weight | Rare | Yes x11 | Yes x25 | Yes | No | . 14 Reputation / Weight | Rare | Yes x11 | Yes x25 | Yes | No | . Then, we&#39;ll take these values and simply insert them into the real data frame where we want them. . lootSubset.iloc[:, 1:7] = tmp lootSubset.head(15) . Image Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 0 NaN | Flawed Veltecite | Common | Yes x11 | Yes x10 | Yes | No | NaN | . 1 150.00 | K-Marks | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 2 2.00 | Reputation | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 3 50.00 | K-Marks / Weight | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 4 0.67 | Reputation / Weight | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 5 NaN | Cloudy Veltecite | Uncommon | Yes x8 | Yes x14 | Yes | Yes | NaN | . 6 570.00 | K-Marks | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . 7 6.00 | Reputation | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . 8 190.00 | K-Marks / Weight | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . 9 2.00 | Reputation / Weight | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . 10 NaN | Clear Veltecite | Rare | Yes x11 | Yes x25 | Yes | Yes | NaN | . 11 854.00 | K-Marks | Rare | Yes x11 | Yes x25 | Yes | No | Clear Veltecite | . 12 9.00 | Reputation | Rare | Yes x11 | Yes x25 | Yes | No | Clear Veltecite | . 13 285.00 | K-Marks / Weight | Rare | Yes x11 | Yes x25 | Yes | No | Clear Veltecite | . 14 3.00 | Reputation / Weight | Rare | Yes x11 | Yes x25 | Yes | No | Clear Veltecite | . And, now we&#39;ll pull the same trick and delete the rows with np.NaN. . cutNA = lootSubset.Loot.isna() lootData = lootSubset[ ~cutNA ] lootData . Image Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 1 150.00 | K-Marks | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 2 2.00 | Reputation | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 3 50.00 | K-Marks / Weight | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 4 0.67 | Reputation / Weight | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 6 570.00 | K-Marks | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 464 0.60 | Reputation / Weight | Common | Yes x38 | Yes x30 | Yes | No | Salvaged Insulation | . 466 506.00 | K-Marks | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 467 5.00 | Reputation | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 468 51.00 | K-Marks / Weight | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 469 0.50 | Reputation / Weight | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 376 rows × 8 columns . Almost there! The column name Image is not what that should be called so we&#39;ll update that to Unit like we had in the prevous post. . lootData = lootData.rename(columns={&#39;Image&#39;:&#39;Unit&#39;}) lootData . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 1 150.00 | K-Marks | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 2 2.00 | Reputation | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 3 50.00 | K-Marks / Weight | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 4 0.67 | Reputation / Weight | Common | Yes x11 | Yes x10 | Yes | No | Flawed Veltecite | . 6 570.00 | K-Marks | Uncommon | Yes x8 | Yes x14 | Yes | No | Cloudy Veltecite | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 464 0.60 | Reputation / Weight | Common | Yes x38 | Yes x30 | Yes | No | Salvaged Insulation | . 466 506.00 | K-Marks | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 467 5.00 | Reputation | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 468 51.00 | K-Marks / Weight | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 469 0.50 | Reputation / Weight | Common | Yes x28 | Yes x30 | Yes | No | Aluminum Scrap | . 376 rows × 8 columns . Just one more problem I&#39;d like to correct before we do some fun questions at the end. Pandas has also copied the idea of Categoreies from R. And, it allows us to set the order of the values as well. We&#39;re going to use this in a minute but it&#39;s also a nice to have. . Changing the type is just as easy as you&#39;d think: . lootData.Rarity.astype(&#39;category&#39;) . 1 Common 2 Common 3 Common 4 Common 6 Uncommon ... 464 Common 466 Common 467 Common 468 Common 469 Common Name: Rarity, Length: 376, dtype: category Categories (6, object): [&#39;Common&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;, &#39;Rare&#39;, &#39;Uncommon&#39;] . You can see that now there is a list of the possible Categories below the values: Categories (6, object): [&#39;Common&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;, &#39;Rare&#39;, &#39;Uncommon&#39;]. However, the order matters and so we cannot leave this how it is; Uncommon is clealy not larger in type then Epic. The proper way to do this would be to declare the order - which is what is next. We&#39;ll use pd.Categorical() to convert them and set the order of the Categories. . lootData[&#39;Rarity&#39;] = pd.Categorical(lootData.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;]) pd.Categorical(lootData.Rarity, categories = [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;]) . [&#39;Common&#39;, &#39;Common&#39;, &#39;Common&#39;, &#39;Common&#39;, &#39;Uncommon&#39;, ..., &#39;Common&#39;, &#39;Common&#39;, &#39;Common&#39;, &#39;Common&#39;, &#39;Common&#39;] Length: 376 Categories (6, object): [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;] . There we go! See that the order has now been fixed: Categories (6, object): [&#39;Common&#39;, &#39;Uncommon&#39;, &#39;Rare&#39;, &#39;Epic&#39;, &#39;Exotic&#39;, &#39;Legendary&#39;]. Here I would like to note that I did take liberty to set Legendary above Exotic since the wiki as them mixed and the actual games sorting puts Legendary above Exotic and so I did the same. . Conclusion and Some Questions . And, that&#39;s how you pull data from a website! Now for some interesting questions. Looking at the data we have here, there are two questions that popped out at me: . If you have to pick up only two items, what would they be per Rarity? | Given the sell values per category, is the relationship linear as the the category rises? | What Are The Best Two Items To Sell, Per Rarity? . This is a pretty simple ask. We just have to remember that we&#39;re working with Tidy Data and we&#39;ll want to filter the Name column for K-Marks before doing anything else. Next, we&#39;ll do a groupby() for the Rarity and then pull out the Units column since that has the values we&#39;re after. Luckily, pandas already has a function to get the largest values .largest() which also works as an aggregation function for groupby(). . tmp = lootData.query(&quot;Name == &#39;K-Marks&#39;&quot;).groupby(&quot;Rarity&quot;)[&#39;Unit&#39;].nlargest(2) tmp . Rarity Common 336 900.0 111 760.0 Uncommon 356 3417.0 181 1709.0 Rare 371 11533.0 366 5126.0 Epic 121 20183.0 396 17300.0 Exotic 401 129746.0 106 77848.0 Legendary 441 518985.0 431 116772.0 Name: Unit, dtype: float64 . So, we have our values but we don&#39;t have any way to identify them since we cannot include the Name column since then it becomes a Data Frame and .nlargest() doesn&#39;t work. If we look closely, the old indexes have been carried over and are included in our results so we&#39;ll need to give ourselves access to them. We&#39;re going to do this with .reset_index() since that will push those indexes into the Data Frame. . tmp.reset_index() . Rarity level_1 Unit . 0 Common | 336 | 900.0 | . 1 Common | 111 | 760.0 | . 2 Uncommon | 356 | 3417.0 | . 3 Uncommon | 181 | 1709.0 | . 4 Rare | 371 | 11533.0 | . 5 Rare | 366 | 5126.0 | . 6 Epic | 121 | 20183.0 | . 7 Epic | 396 | 17300.0 | . 8 Exotic | 401 | 129746.0 | . 9 Exotic | 106 | 77848.0 | . 10 Legendary | 441 | 518985.0 | . 11 Legendary | 431 | 116772.0 | . # Keep those index numbers tmp = tmp.reset_index() . The index column is called level_1 which we&#39;re going to simply insert into the original data. . # Best items to sell per rarity: lootData.loc[ tmp[&#39;level_1&#39;].tolist() ].sort_values(&quot;Rarity&quot;, ascending=False)[[&#39;Unit&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]] . Unit Name Rarity Loot . 441 518985.0 | K-Marks | Legendary | Alpha Crusher Head | . 431 116772.0 | K-Marks | Legendary | Savage Marauder Head | . 401 129746.0 | K-Marks | Exotic | Alpha Crusher Heart | . 106 77848.0 | K-Marks | Exotic | Progenitor Slag | . 121 20183.0 | K-Marks | Epic | NiC Oil Cannister | . 396 17300.0 | K-Marks | Epic | Crusher Flesh | . 371 11533.0 | K-Marks | Rare | Crusher Hide | . 366 5126.0 | K-Marks | Rare | Mature Rattler Eyes | . 356 3417.0 | K-Marks | Uncommon | Hardened Bone Plates | . 181 1709.0 | K-Marks | Uncommon | Derelict Explosives | . 336 900.0 | K-Marks | Common | Toxic Glands | . 111 760.0 | K-Marks | Common | Progenitor Composite | . There we go! Well, except that this doesn&#39;t account for weight and what we really want is to carry the most value per weight since we&#39;re limited in the game by the backpack size. Let&#39;s do the same but for the per weight value instead. . tmp = lootData.query(&quot;Name == &#39;K-Marks / Weight&#39;&quot;).groupby(&quot;Rarity&quot;)[&#39;Unit&#39;].nlargest(2).reset_index() lootData.loc[ tmp[&#39;level_1&#39;].tolist() ].sort_values(&quot;Rarity&quot;, ascending=False)[[&#39;Unit&#39;, &#39;Name&#39;, &#39;Rarity&#39;, &#39;Loot&#39;]] . Unit Name Rarity Loot . 443 17300.0 | K-Marks / Weight | Legendary | Alpha Crusher Head | . 433 5839.0 | K-Marks / Weight | Legendary | Savage Marauder Head | . 108 6487.0 | K-Marks / Weight | Exotic | Progenitor Slag | . 403 6487.0 | K-Marks / Weight | Exotic | Alpha Crusher Heart | . 123 4037.0 | K-Marks / Weight | Epic | NiC Oil Cannister | . 118 3844.0 | K-Marks / Weight | Epic | Letium Clot | . 373 1153.0 | K-Marks / Weight | Rare | Crusher Hide | . 368 1025.0 | K-Marks / Weight | Rare | Mature Rattler Eyes | . 458 570.0 | K-Marks / Weight | Uncommon | Print Resin | . 358 380.0 | K-Marks / Weight | Uncommon | Hardened Bone Plates | . 148 1000.0 | K-Marks / Weight | Common | Old Currency | . 283 506.0 | K-Marks / Weight | Common | Nutritional Bar | . And, now we see there are some important changes. The values in the Common Rarity are totally different! . Are the Rarity Values Linear? . We&#39;ll aggregate over all the values in a category and then simply plot them. . lootData.query(&quot;Name == &#39;K-Marks&#39;&quot;).groupby(&quot;Rarity&quot;)[&#39;Unit&#39;].mean().plot() plt.title(&quot;Mean K-Marks Per Rarity&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Mean K-Marks Per Rarity&#39;) . Wait, what? This looks terrible! Everything from Common to Epic is worth nothing compared to Exotic and Legendary. Maybe we&#39;re dealing with some outlier problems? Let&#39;s check the median just in case. . lootData.query(&quot;Name == &#39;K-Marks&#39;&quot;).groupby(&quot;Rarity&quot;)[&#39;Unit&#39;].median().plot() plt.title(&quot;Median K-Marks Per Rarity&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Median K-Marks Per Rarity&#39;) . This is even worse! Legendary loot is definitely being affected by outliers but Exotic is so far and above better than everything else. Maybe we&#39;re looking at the wrong values? Maybe we&#39;re making the same mistake and we need to account for the per weight? . lootData.query(&quot;Name == &#39;K-Marks / Weight&#39;&quot;).groupby(&quot;Rarity&quot;)[&#39;Unit&#39;].median().plot() plt.title(&quot;Median K-Marks/Weight Per Rarity&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Median K-Marks/Weight Per Rarity&#39;) . I mean it&#39;s better but not really. What is in this category? . lootData.query(&quot;Rarity == &#39;Exotic&#39;&quot;).query(&quot;Name == &#39;K-Marks&#39;&quot;) . Unit Name Rarity Personal Quarters Campaigns Jobs Printing Loot . 91 8650.0 | K-Marks | Exotic | Yes x4 | Yes x9 | Yes | No | Charged Tharis Iron Ingot | . 106 77848.0 | K-Marks | Exotic | Yes x6 | Yes x9 | Yes | No | Progenitor Slag | . 401 129746.0 | K-Marks | Exotic | Yes x6 | Yes x1 | Yes | No | Alpha Crusher Heart | . This category makes no sense and I don&#39;t know why it exists. I&#39;m not sure they understand what this category is for either looking at what is in here. But, that will be it for now. .",
            "url": "https://orgulo.us/game/python/data/science/exploration/cycle/frontier/2022/10/05/cycle-loot-values-download.html",
            "relUrl": "/game/python/data/science/exploration/cycle/frontier/2022/10/05/cycle-loot-values-download.html",
            "date": " • Oct 5, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Quick Test of Ipywidget Interact Function",
            "content": "While watching the Lectures from Fastai this time around, Jeremy used a Python Decorator called @interact which creates a function with interactable variables. This is really useful feature when you want to experiment with specific values - like Jeremy did in the lecture. However, there is a warning in the Lecture notes: . Reminder:If the sliders above aren&#39;t working for you, that&#39;s because the interactive features of this notebook don&#39;t work in Kaggle&#39;s Reader mode. They only work in Edit mode. Please click &quot;Copy &amp; Edit&quot; in the top right of this window, then in the menu click Run and then Run all. Then you&#39;ll be able to use all the interactive sliders in this notebook. . Let&#39;s step that back a little bit for an explanation since some of this might not be familiar. Python Decorators are function annotations which modify the behavior of their function. There are excellent articles and descriptions about them but we&#39;ll use this one from the Python Docs as an example: @cache from the functools library. . from functools import cache @cache def factorial(n): return n * factorial(n-1) if n else 1 . factorial(10) # no previously cached result, makes 11 recursive calls . 3628800 . factorial(5) # just looks up cached value result . 120 . factorial(12) # makes two new recursive calls, the other 10 are cached . 479001600 . The Decorator @cache will modify the behavior of the function to record previous computations. In this way, you can save time using the modified function we&#39;ve written. In the instance with Jeremy, the @interact call modifies the function written to allow us to modify the values in real time without ending execution nor updating the values in the code block. What we&#39;re really after here though is Does this work with Fastpages? . Considering the warning given by Jeremy, I wouldn&#39;t expect this to work but I&#39;ve gotten R working in these Jupyter Notebooks and uploaded them so let&#39;s try it. First, we&#39;ll need the imports to do this and then we&#39;ll simply use the example from the Fastai Lecture Notebook and then push it to the site. . from ipywidgets import interact import torch import matplotlib.pyplot as plt import numpy as np from functools import partial plt.rc(&#39;figure&#39;, dpi=90) # This modifies the size of the graphs . # This is just from the notebook def plot_function(f, title=None, min=-2.1, max=2.1, color=&#39;r&#39;, ylim=None): x = torch.linspace(min,max, 100)[:,None] if ylim: plt.ylim(ylim) plt.plot(x, f(x), color) if title is not None: plt.title(title) # Define a quadratic function: def f(x): return 3*x**2 + 2*x + 1 # define generic quadratic def quad(a, b, c, x): return a*x**2 + b*x + c def mk_quad(a,b,c): return partial(quad, a,b,c) def noise(x, scale): return np.random.normal(scale=scale, size=x.shape) def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add) . plot_function(f, &quot;$3x^2 + 2x + 1$&quot;) . f2 = mk_quad(3,2,1) plot_function(f2) . np.random.seed(42) x = torch.linspace(-2, 2, steps=20)[:,None] y = add_noise(f(x), 0.15, 1.5) . @interact(a=1.1, b=1.1, c=1.1) def plot_quad(a, b, c): plt.scatter(x,y) plot_function(mk_quad(a,b,c), ylim=(-3,13)) . Conclusion . After pushing the page, it doesn&#39;t work. I assumed as much but was hopeful it would work. .",
            "url": "https://orgulo.us/machine%20learning/fastai/lectures/trial/notebook/jupyter/interact/2022/09/30/testing-if-interact-works-online.html",
            "relUrl": "/machine%20learning/fastai/lectures/trial/notebook/jupyter/interact/2022/09/30/testing-if-interact-works-online.html",
            "date": " • Sep 30, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Which Cycle Frontier Jobs Are Worth Doing? - Part One.",
            "content": "This post series was inspired by the Korolev Job And Two Smoking Barrels where upon accepting the quest I realized how terrible the rewards were in comparison. For those that don&#39;t play the game a little background will be necessary. The Cycle: Frontier is an Evacuation Shooter game and there are three main Corporations - or Organizations, if you prefer - who act as quest and reward givers in the game. There are two kinds of these: Campaign and Jobs. Whereas the campaign quests will push you along to different areas of the world, instead the Jobs function as a way to collect scrips and excuse to send players to the planet. . There are three kinds of quests: . Collect Stuff. | Deposit Stuff. | Kill Stuff: including players. | For Deposit Jobs, you carry the requested items to a Dead Drop and then deposit the items in question. For this one, it requires you deposit a gun you purchase from the shop. Now, you could find this weapon or loot it from other people but those are not garunteed at all. The Kmark - which is cash, basically - reward is $19,000 and the gun it wants you to deposit is $22,000 so there is no reason to take this job unless you already have this gun. Anyways, lets get to the fun part. . Scraping and Cleaning Cycle Data . So, we&#39;ll start with the normal imports for doing data in Python. . import pandas as pd import numpy as np import seaborn as sns import requests as r . Thankfully, there is an official wiki for the game which is maintained by both the Developers and the Community together. We&#39;re going to pull the data from there as that should be the most up to date data. Like most online data scraping, there is some try-fail loops to getting what you&#39;re after from the webpage. Since there are three organizations, there are three tables with jobs we&#39;d like to pull from the website. After some trial and error I found that using match=&quot;Name&quot; was perfect for pulling the tables out of the webpage. . url = &quot;https://thecyclefrontier.wiki/wiki/Jobs&quot; site = pd.read_html(url, match=&quot;Name&quot;, converters = { &quot;Name&quot;: str, &quot;Description&quot;: str, &quot;Unlocked&quot;: int, &quot;Tasks&quot;: str, &quot;Rewards&quot;: str}) . You may notice the addition of converters argument above which is a really useful feature I didn&#39;t know previously; basically, if you know the column names coming in then you can tell Pandas what data type you want so you don&#39;t have to convert later. So, what does the data look like? . site[0].head(8) . Name Description Unlock Level Difficulty Tasks Rewards . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | 4.0 | Easy | Collect: 2 Hydraulic Piston 10 Hardened Metals | 3800 K-Marks 1 Korolev Scrip 15 Korolev R... | . 1 3800 | K-Marks | NaN | NaN | NaN | NaN | . 2 1 | Korolev Scrip | NaN | NaN | NaN | NaN | . 3 15 | Korolev Reputation | NaN | NaN | NaN | NaN | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | 7.0 | Medium | Collect: 4 Derelict Explosives | 11000 K-Marks 8 Korolev Scrip 52 Korolev ... | . 5 11000 | K-Marks | NaN | NaN | NaN | NaN | . 6 8 | Korolev Scrip | NaN | NaN | NaN | NaN | . 7 52 | Korolev Reputation | NaN | NaN | NaN | NaN | . That is not really what we were hoping would come in. Checking the actual site, this is caused by a table which exists inside one of the row cells. But - like in the previous post - this can still be used after some work. So, lets get to work! . Fixing the Job Rewards . We dont need most of the columns for what we&#39;re going to be accomplishing so we&#39;re going to pull them out. I&#39;m going to do a copy as pandas sometimes doesn&#39;t play so nicely with updates. What I&#39;ve found is that since Pandas uses pointers underneath, sometimes when doing updates to slices I don&#39;t always get what I expect. So, we&#39;ll splice and copy to only get the data we care about in its own independent dataframe. . rewardsSubset = site[0][[&quot;Name&quot;, &quot;Description&quot;, &quot;Difficulty&quot;]].copy() rewardsSubset . Name Description Difficulty . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | Easy | . 1 3800 | K-Marks | NaN | . 2 1 | Korolev Scrip | NaN | . 3 15 | Korolev Reputation | NaN | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | Medium | . ... ... | ... | ... | . 183 470 | Korolev Reputation | NaN | . 184 No Expiry Date | There you are, finally! There&#39;s been an accide... | Medium | . 185 6300 | K-Marks | NaN | . 186 9 | Korolev Scrip | NaN | . 187 62 | Korolev Reputation | NaN | . 188 rows × 3 columns . These column names are not useful so we&#39;re going to correct those so they make sense with the project. We&#39;re going to call the final column Job which will make more sense as the work gets done. . rewardsSubset.columns = [&quot;Units&quot;, &quot;Rewards&quot;, &quot;Job&quot;] rewardsSubset.head() . Units Rewards Job . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | Easy | . 1 3800 | K-Marks | NaN | . 2 1 | Korolev Scrip | NaN | . 3 15 | Korolev Reputation | NaN | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | Medium | . So, looking at above we can see that we have extra data in the Job Column which is no longer appropriate. We&#39;re going to simply fill that column with a null value: np.NaN . rewardsSubset.Job = np.NaN rewardsSubset.head(12) . Units Rewards Job . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | NaN | . 1 3800 | K-Marks | NaN | . 2 1 | Korolev Scrip | NaN | . 3 15 | Korolev Reputation | NaN | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | NaN | . 5 11000 | K-Marks | NaN | . 6 8 | Korolev Scrip | NaN | . 7 52 | Korolev Reputation | NaN | . 8 Mining Bot | Our engineers have designed an autonomous mini... | NaN | . 9 6900 | K-Marks | NaN | . 10 9 | Korolev Scrip | NaN | . 11 62 | Korolev Reputation | NaN | . So, now for the hard part: getting the Job Title into the Job Column. Looking at the data above, we can see that the Job Title is always stored in a multiple of four. We can confirm this by simply dividing the total number of columns by 4 just to be sure. . # This should be divisible by 4 since they rewards for jobs are always in this format now. len(rewardsSubset) / 4 . 47.0 . And, we have a perfect divide! Good! This is since each Job always hands out Kmarks, a matching Corp Scrip and Corp Reputation. So, what we need to do now is pull the Job Title from the Units column and insert it into the next three columns under Job. To do this, we&#39;re going to build a range of values which are multiples of 4 starting at 0 and up to the total amount of jobs. We don&#39;t want to hard code this since the count of jobs should be expected to change over time. . topIndex = len(rewardsSubset) / 4 - 3 index = range( 0, 44, 4) . Next we&#39;ll want a numpy array of the offsets. We don&#39;t want to use a list because then it will add the values to a python list instead of creating a set of indexes. In effect, we&#39;re trying to take advantage of Broadcasting in numpy. We&#39;ll do an illustration of this quick. . listMistake = [1,2,3] broadcastCorrect = np.array(listMistake) [index[1]] + listMistake, index[1] + broadcastCorrect . ([4, 1, 2, 3], array([5, 6, 7])) . Above you can see [4, 1, 2, 3] is definitely not what we&#39;re after. So, after setting up the proper offset lets make sure we&#39;re getting what we want. I often sanity check my initial design since experience as taught me you can still trip even after the initial testing works. So, lets do that now. . offset = np.array([1, 2, 3]) . # this is how we&#39;ll iterate; proof it works. for i in index[:3]: aJob = rewardsSubset.iloc[i, 0] print(f&#39;{aJob} is at index {i}&#39;) . New Mining Tools is at index 0 Explosive Excavation is at index 4 Mining Bot is at index 8 . And, there we go! We&#39;re getting exactly what we wanted and expected. This is also a good initial test for the loop which we&#39;re going to tuck into a function at the end of all this. So, now to test the logic of swapping the values from the Unit Column to the Job Column. . # Do the thing: aJob = rewardsSubset.iloc[index[0], 0] indexes = index[0] + offset rewardsSubset.iloc[ indexes, 2 ] = aJob rewardsSubset.head(9) . Units Rewards Job . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | NaN | . 1 3800 | K-Marks | New Mining Tools | . 2 1 | Korolev Scrip | New Mining Tools | . 3 15 | Korolev Reputation | New Mining Tools | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | NaN | . 5 11000 | K-Marks | NaN | . 6 8 | Korolev Scrip | NaN | . 7 52 | Korolev Reputation | NaN | . 8 Mining Bot | Our engineers have designed an autonomous mini... | NaN | . for i in index: aJob = rewardsSubset.iloc[i, 0] indexes = i + offset rewardsSubset.iloc[ indexes, 2 ] = aJob rewardsSubset.head(12) . Units Rewards Job . 0 New Mining Tools | We are producing new Mining Tools for new Pros... | NaN | . 1 3800 | K-Marks | New Mining Tools | . 2 1 | Korolev Scrip | New Mining Tools | . 3 15 | Korolev Reputation | New Mining Tools | . 4 Explosive Excavation | One of our mines collapsed with valuable equip... | NaN | . 5 11000 | K-Marks | Explosive Excavation | . 6 8 | Korolev Scrip | Explosive Excavation | . 7 52 | Korolev Reputation | Explosive Excavation | . 8 Mining Bot | Our engineers have designed an autonomous mini... | NaN | . 9 6900 | K-Marks | Mining Bot | . 10 9 | Korolev Scrip | Mining Bot | . 11 62 | Korolev Reputation | Mining Bot | . Perfect! Now all we have to do is cut the Units Columns where the Job Title still remains. Luckily, the np.NaN has remained so we can collect the indexes for Job where that values exists. And, then simply get rid of them. . # Kill the NA&#39;s cutNA = rewardsSubset.Job.isna() rewardsSubset[ ~cutNA ].head(15) . Units Rewards Job . 1 3800 | K-Marks | New Mining Tools | . 2 1 | Korolev Scrip | New Mining Tools | . 3 15 | Korolev Reputation | New Mining Tools | . 5 11000 | K-Marks | Explosive Excavation | . 6 8 | Korolev Scrip | Explosive Excavation | . 7 52 | Korolev Reputation | Explosive Excavation | . 9 6900 | K-Marks | Mining Bot | . 10 9 | Korolev Scrip | Mining Bot | . 11 62 | Korolev Reputation | Mining Bot | . 13 7600 | K-Marks | None of your Business | . 14 10 | Korolev Scrip | None of your Business | . 15 90 | Korolev Reputation | None of your Business | . 17 10000 | K-Marks | Insufficient Processing Power | . 18 11 | Korolev Scrip | Insufficient Processing Power | . 19 110 | Korolev Reputation | Insufficient Processing Power | . Function to build Job Rewards . Now that we have all this we can push it into a function and run it against all the different Corporation tables. . def buildJobsRewards(data): # Function to take job rewards data and return a cleaned version rewardsSubset = data[[&quot;Name&quot;, &quot;Description&quot;, &quot;Difficulty&quot;]].copy() rewardsSubset.columns = [&quot;Units&quot;, &quot;Rewards&quot;, &quot;Job&quot;] index = range( 0, len(rewardsSubset) - 4, 4) offset = np.array([1, 2, 3]) rewardsSubset.Job = np.NaN for i in index: aJob = rewardsSubset.iloc[i, 0] indexes = i + offset rewardsSubset.iloc[ indexes, 2 ] = aJob cutNA = rewardsSubset.Job.isna() rewardsSubset = rewardsSubset[ ~cutNA ] rewardsSubset = rewardsSubset.assign( Units = rewardsSubset[&#39;Units&#39;].astype(int) ) return rewardsSubset . And, the final test! . KorolevRewards = buildJobsRewards( site[0] ) icaRewards = buildJobsRewards( site[1] ) osirisRewards = buildJobsRewards( site[2] ) . KorolevRewards.head(9) . Units Rewards Job . 1 3800 | K-Marks | New Mining Tools | . 2 1 | Korolev Scrip | New Mining Tools | . 3 15 | Korolev Reputation | New Mining Tools | . 5 11000 | K-Marks | Explosive Excavation | . 6 8 | Korolev Scrip | Explosive Excavation | . 7 52 | Korolev Reputation | Explosive Excavation | . 9 6900 | K-Marks | Mining Bot | . 10 9 | Korolev Scrip | Mining Bot | . 11 62 | Korolev Reputation | Mining Bot | . icaRewards.head(9) . Units Rewards Job . 1 4400 | K-Marks | Water Filtration System | . 2 1 | ICA Scrip | Water Filtration System | . 3 15 | ICA Reputation | Water Filtration System | . 5 7500 | K-Marks | New Beds | . 6 9 | ICA Scrip | New Beds | . 7 62 | ICA Reputation | New Beds | . 9 13000 | K-Marks | Station Defense | . 10 12 | ICA Scrip | Station Defense | . 11 130 | ICA Reputation | Station Defense | . osirisRewards.head(9) . Units Rewards Job . 1 2200 | K-Marks | Lab equipment | . 2 1 | Osiris Scrip | Lab equipment | . 3 13 | Osiris Reputation | Lab equipment | . 5 8100 | K-Marks | Surveillance Center | . 6 8 | Osiris Scrip | Surveillance Center | . 7 43 | Osiris Reputation | Surveillance Center | . 9 8100 | K-Marks | Gun Manufacturing | . 10 8 | Osiris Scrip | Gun Manufacturing | . 11 52 | Osiris Reputation | Gun Manufacturing | . Conclusion . Now we&#39;ve got tidy data for all the jobs from all the Corporations and their matching rewards. Next we&#39;ll need to clean the actual tasks which is going to be much harder since there is no consistent formatting. But, we&#39;ll end this post with a simple question using the data we have: Which Corporation gives out the best average Kmarks? . KorolevRewards.query(&quot;Rewards == &#39;K-Marks&#39;&quot;).Units.mean() . 22936.956521739132 . ( round(KorolevRewards.query(&quot;Rewards == &#39;K-Marks&#39;&quot;).Units.mean(),2), round(icaRewards.query(&quot;Rewards == &#39;K-Marks&#39;&quot;).Units.mean(),2), round(osirisRewards.query(&quot;Rewards == &#39;K-Marks&#39;&quot;).Units.mean(), 2) ) . (22936.96, 23158.33, 21136.17) . And, the winner is ICA barely over Korolev! Suck Less Osiris! .",
            "url": "https://orgulo.us/game/python/data/science/exploration/cycle/frontier/2022/09/28/cycle-jobs-part-one.html",
            "relUrl": "/game/python/data/science/exploration/cycle/frontier/2022/09/28/cycle-jobs-part-one.html",
            "date": " • Sep 28, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Some Data Exploration of The Cycle - Frontier Weapons",
            "content": "Let&#39;s Start At the Beginning . Recently, I&#39;ve been playing as much The Cycle: Frontier as I can reasonably fit into my days along with getting work and projects done. If you&#39;re not familiar, it&#39;s a First Person Shooter game which focuses around dropping you to a planet in a semi-persistent world with loot and other players. The Station has some Corporations which hand out jobs as a pretty thin attempt to get you down to the planet. While down there, other players - who are not on your team - can decide how they want to deal with you: talk to you, lie to you, kill you, help you. I&#39;ve heard these games be called both Looter Shooters as well as Evac Shooters and I&#39;m admittidly not the biggest fan of these names. . As you build reputation with the different Corps on the Station you can unlock the ability to purchase weapons that each specializes in. Some of them are pretty fun and others are kind of terrible. Today we&#39;re going to do part of the process which was inspired by this article by Robert Ritz. In it, he goes over how to setup an automated Data Pipeline using Kaggle and Deepnote together. This part is going to be simply getting the data downloaded, cleaned and some observations about the guns in this game. I still need to do some more investigation about Deepnote - namely the price, utility and such before actually commiting to that part; I should be able to simply cut that part out and do the download/upload to Kaggle from one of my own servers but if it works then I&#39;m going to use it. . Scraping and Cleaning Cycle Data . To start with, if you&#39;re following along, scraping data from the Cycle&#39;s Wiki page is annoying. There are tables inserted inside the tables which caused quite a problem while trying to simply pull the data from the website. So, if you&#39;re going to us this as the basis for your own tools then beware that you&#39;ll be certain to need to do some custom work. We&#39;ll start with the normal imports for a project like this. . import pandas as pd # for the data. import numpy as np # for a NaN type import matplotlib.pyplot as plt # For plotting, and some customization of plots. import seaborn as sns # For pretty plots. # Fix the size of the graphs sns.set(rc={&quot;figure.figsize&quot;:(11, 8)}) . The website we&#39;re going to be using for the data is their official wiki page - which can be found here. We&#39;ll be pulling from the weapons page which luckily contains a table of all the guns without having to join them. Pandas allows you to read html off a website and will attempt to pull any tables it finds on the webpage. Sadly, due to the nested tables and the way the tables are tagged this simply doesn&#39;t work here. But, you can ask pd.read_html() to look for an attribute and then pull the data from the page using that; it will still need to end up as a table though otherwise pandas will reject it. After doing quite a bit of exploration, I found that you can pull the total table with the attribute zebra as that is the only table which uses it. . url = &quot;https://thecyclefrontier.wiki/wiki/Weapons&quot; site = pd.read_html(url, attrs={&quot;class&quot;:&quot;zebra&quot;})[0] . site.head() . Image Name Type Ammo Faction Buy Price Sell Value Rarity Weight Crit Multi Damage Pen Mag Size Refire Rate RPM Reload Time Move Speed Proj. Speed . 0 NaN | Advocate | AR | Medium | ICA | 76000 K-Marks | 22781 K-Marks | Epic | 35.0 | 1.7 | 11.0 | 26.0 | 24.0 | 0.105 | 571.43 | 3.2 | 0.9 | 29000 | . 1 76000.0 | K-Marks | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 22781.0 | K-Marks | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 NaN | AR-55 Autorifle | AR | Medium | Station | 1700 K-Marks | 524 K-Marks | Common | 35.0 | 1.7 | 12.0 | 10.0 | 22.0 | 0.110 | 545.45 | 2.7 | 0.9 | 28000 | . 4 1700.0 | K-Marks | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . You can see above that this works but the formatting is still a little messed up. Looking at the tables though, this will be an easy fix since rows which we don&#39;t need contain lots NaN. If you&#39;ve never seen this before it just means Not A Number and is a special value used by numpy for this. So, let&#39;s clean the data for use. You can check a Series for these values using .isna() and then we&#39;ll pass the opposite indexes to pull those out: . data = site[~site.Type.isna()] data.head() . Image Name Type Ammo Faction Buy Price Sell Value Rarity Weight Crit Multi Damage Pen Mag Size Refire Rate RPM Reload Time Move Speed Proj. Speed . 0 NaN | Advocate | AR | Medium | ICA | 76000 K-Marks | 22781 K-Marks | Epic | 35.0 | 1.7 | 11.0 | 26.0 | 24.0 | 0.105 | 571.430 | 3.20 | 0.9 | 29000 | . 3 NaN | AR-55 Autorifle | AR | Medium | Station | 1700 K-Marks | 524 K-Marks | Common | 35.0 | 1.7 | 12.0 | 10.0 | 22.0 | 0.110 | 545.450 | 2.70 | 0.9 | 28000 | . 6 NaN | Asp Flechette Gun | SMG | Light | Osiris | 54000 K-Marks | 16131 K-Marks | Epic | 30.0 | 1.5 | 9.0 | 26.0 | 20.0 | 0.095 | 631.580 | 2.50 | 1.0 | 24000 | . 9 NaN | B9 Trenchgun | Shotgun | Shotgun | Station | 1200 K-Marks | 371 K-Marks | Common | 25.0 | 1.2 | 10.0 | 10.0 | 5.0 | 0.950 | 63.158 | 2.40 | 1.0 | 26000 | . 12 NaN | Basilisk | DMR | Heavy | Osiris | 275000 K-Marks | 82448 K-Marks | Exotic | 50.0 | 1.6 | 34.0 | 28.0 | 8.0 | 0.500 | 120.000 | 3.85 | 0.8 | 45000 | . Looking much better now. Let&#39;s quickly check the data&#39;s types and make sure they make sense still. . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 26 entries, 0 to 75 Data columns (total 18 columns): # Column Non-Null Count Dtype -- -- 0 Image 0 non-null float64 1 Name 26 non-null object 2 Type 26 non-null object 3 Ammo 26 non-null object 4 Faction 26 non-null object 5 Buy Price 26 non-null object 6 Sell Value 26 non-null object 7 Rarity 26 non-null object 8 Weight 26 non-null float64 9 Crit Multi 26 non-null float64 10 Damage 26 non-null float64 11 Pen 26 non-null float64 12 Mag Size 26 non-null float64 13 Refire Rate 26 non-null float64 14 RPM 26 non-null float64 15 Reload Time 26 non-null float64 16 Move Speed 26 non-null float64 17 Proj. Speed 26 non-null object dtypes: float64(10), object(8) memory usage: 3.9+ KB . Something is wrong with the Proj. Speed at this point since it shouldn&#39;t be an object but instead should be a number. Checking the values we find that there are string values in here. . # There is a hitscan in there; how should we deal with that? data[&#39;Proj. Speed&#39;].unique() . array([&#39;29000&#39;, &#39;28000&#39;, &#39;24000&#39;, &#39;26000&#39;, &#39;45000&#39;, &#39;50000&#39;, &#39;Hitscan&#39;, &#39;30000&#39;, &#39;60000&#39;, &#39;40000&#39;, &#39;70000&#39;, &#39;4000&#39;, &#39;35000&#39;, &#39;34000&#39;], dtype=object) . The value of Hitscan is preventing the conversion to numbers. I didn&#39;t realize any of the guns in this game were hitscan at all. Which weapons are these? . data[ data[&#39;Proj. Speed&#39;] == &#39;Hitscan&#39; ][[&#39;Name&#39;, &#39;Proj. Speed&#39;]] . Name Proj. Speed . 21 Gorgon | Hitscan | . 75 Zeus Beam | Hitscan | . Ok, so we&#39;ll need to replace this with something that wont hurt our analysis so we&#39;re also going to change these to np.NaN. . # Fix hitscan info: indx = data[&#39;Proj. Speed&#39;] == &#39;Hitscan&#39; data.loc[indx, &#39;Proj. Speed&#39;] = np.NaN . There are some other columns - Sell Value, Buy Price as examples - which have come in as object so they&#39;re being treated as strings. We need these to be numbers if we end up using them. And, after fixing the Hitscan problem we&#39;ll need to convert that column to numbers. . data = data.assign( Sell = data[&#39;Sell Value&#39;].str.replace(&#39; K-Marks&#39;, &#39;&#39;).astype(&#39;float&#39;), Buy = data[&#39;Buy Price&#39;].str.replace(&#39; K-Marks&#39;, &#39;&#39;).astype(&#39;float&#39;), DPS = data[&#39;Refire Rate&#39;] * data[&#39;Damage&#39;], Faction = data[&#39;Faction&#39;].astype(&#39;category&#39;), Velocity = data[&#39;Proj. Speed&#39;].astype(&#39;float&#39;) ) data = data.assign( perWeight = data[&#39;Sell&#39;] / data[&#39;Weight&#39;] ) # # This removes the legendary weapons # data = data.query(&#39;Faction != &quot;Printing&quot;&#39;) data = data.drop(labels = [&#39;Sell Value&#39;, &#39;Buy Price&#39;, &#39;Image&#39;, &#39;Proj. Speed&#39;],axis = 1) . Now we&#39;ve got data to work with! . data.head() . Name Type Ammo Faction Rarity Weight Crit Multi Damage Pen Mag Size Refire Rate RPM Reload Time Move Speed Sell Buy DPS Velocity perWeight . 0 Advocate | AR | Medium | ICA | Epic | 35.0 | 1.7 | 11.0 | 26.0 | 24.0 | 0.105 | 571.430 | 3.20 | 0.9 | 22781.0 | 76000.0 | 1.155 | 29000.0 | 650.885714 | . 3 AR-55 Autorifle | AR | Medium | Station | Common | 35.0 | 1.7 | 12.0 | 10.0 | 22.0 | 0.110 | 545.450 | 2.70 | 0.9 | 524.0 | 1700.0 | 1.320 | 28000.0 | 14.971429 | . 6 Asp Flechette Gun | SMG | Light | Osiris | Epic | 30.0 | 1.5 | 9.0 | 26.0 | 20.0 | 0.095 | 631.580 | 2.50 | 1.0 | 16131.0 | 54000.0 | 0.855 | 24000.0 | 537.700000 | . 9 B9 Trenchgun | Shotgun | Shotgun | Station | Common | 25.0 | 1.2 | 10.0 | 10.0 | 5.0 | 0.950 | 63.158 | 2.40 | 1.0 | 371.0 | 1200.0 | 9.500 | 26000.0 | 14.840000 | . 12 Basilisk | DMR | Heavy | Osiris | Exotic | 50.0 | 1.6 | 34.0 | 28.0 | 8.0 | 0.500 | 120.000 | 3.85 | 0.8 | 82448.0 | 275000.0 | 17.000 | 45000.0 | 1648.960000 | . What Do the Weapons Look Like? . A word here about some extra cleaning which I&#39;ve elected to do. Looking at the data, there are two more problems that should be brought up here. The first is that I&#39;m taking Snipers out of the analysis. The reason for this is that there really are only two of them and everyone understands why they&#39;re as powerful as they are. . sns.scatterplot(x = data.Damage, y = data[&#39;Pen&#39;], hue = data.Type) plt.title(&quot;The Reason Snipers Rule&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;The Reason Snipers Rule&#39;) . The second adjustment is that I&#39;m pulling the ICA Garuntee out of the analysis because it&#39;s the only one of its kind. . data.query(&#39;Type == &quot;LMG&quot;&#39;)[[&#39;Name&#39;, &#39;Type&#39;]] . Name Type . 27 ICA Guarantee | LMG | . And, the same for the Komrad for the same reason. . data.query(&#39;Type == &quot;Launcher&quot;&#39;)[[&#39;Name&#39;, &#39;Type&#39;]] . Name Type . 45 KOMRAD | Launcher | . So, let&#39;s start with what we all care about the most: Damage. . data[[&#39;Type&#39;, &#39;Damage&#39;]].groupby(&#39;Type&#39;).mean().sort_values(&#39;Damage&#39;, ascending=False).T . Type DMR Pistol AR SMG Shotgun . Damage 32.0 | 17.666667 | 11.0 | 9.25 | 9.25 | . So, DMR&#39;s do about twice as much damage as the next category down. From this, we&#39;d expect DMRs to be used quite a bit; what weapons are in this category? . data.query(&#39;Type == &quot;DMR&quot;&#39;) . Name Type Ammo Faction Rarity Weight Crit Multi Damage Pen Mag Size Refire Rate RPM Reload Time Move Speed Sell Buy DPS Velocity perWeight . 12 Basilisk | DMR | Heavy | Osiris | Exotic | 50.0 | 1.6 | 34.0 | 28.0 | 8.0 | 0.5 | 120.0 | 3.85 | 0.8 | 82448.0 | 275000.0 | 17.0 | 45000.0 | 1648.96 | . 36 KBR Longshot | DMR | Heavy | Korolev | Epic | 50.0 | 1.5 | 35.0 | 26.0 | 12.0 | 0.6 | 100.0 | 3.55 | 0.9 | 29776.0 | 99000.0 | 21.0 | 40000.0 | 595.52 | . 51 Lacerator | DMR | Heavy | ICA | Rare | 50.0 | 1.5 | 27.0 | 23.0 | 16.0 | 0.4 | 150.0 | 2.55 | 0.9 | 12203.0 | 41000.0 | 10.8 | 35000.0 | 244.06 | . All these weapons get used in my experience - and from watching others play the game. Although, the lowest tier gun in here is Rare so that probably helps a lot. Note that the Rarity of a gun informs the Pen for the Gun and therefore adds more damage when being fired. So, the higher the tier of Rarity therefore the more damage the gun can do per hit - and they have higher damage counts as well. . Considering this, these guns have a limit placed on their Rate of Fire. . data[[&#39;Type&#39;, &#39;Refire Rate&#39;]].groupby(&#39;Type&#39;).mean().sort_values(&#39;Refire Rate&#39;, ascending=False).T . Type Shotgun DMR Pistol AR SMG . Refire Rate 0.77625 | 0.5 | 0.288333 | 0.1475 | 0.08125 | . This is a match in the order of the columns - ignoring the Shotgun Category. Shotguns have a low damage (per pellet), and a high re-fire rate. That&#39;s obviously because the damage per pellet masks how lethal Shotguns are: See any Shattergun Montage basically. . So, it looks like either this was coincidence or they&#39;re intentionally using this to offset damage. . sns.scatterplot(x = data.Damage, y = data[&#39;Refire Rate&#39;], hue = data.Type) plt.title(&quot;Damage With Refire Rate&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Damage With Refire Rate&#39;) . If we check the relationship, the Damage also quite high against the constraint on the Refire Rate. These guns appear really strong in comparison to everything else - setting aside Snipers of course. But is it really? Let&#39;s Normalize the Damage column and see if it really is that far out. If you&#39;re not familiar with Normalization then this is a common process in Machine Learning where the values of a column are scaled based on the minimum and maximum values. . def normalize(column): return ( column - column.min()) / (column.max() - column.min()) round(normalize(data[&#39;Refire Rate&#39;]).mean(), 2) # Lets save this and re-plot: data[&#39;Norm Refire&#39;] = round(normalize(data[&#39;Refire Rate&#39;]), 2) data[&#39;Norm Damage&#39;] = round(normalize(data[&#39;Damage&#39;]), 2) # Re-plot: sns.scatterplot(x = data[&#39;Norm Damage&#39;], y = data[&#39;Norm Refire&#39;], hue = data.Type) plt.title(&quot;Normalized Damage With Normalized Refire Rate&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Normalized Damage With Normalized Refire Rate&#39;) . Normalizing doesn&#39;t look to have changed anyting aside from the scale so that still looks really solid. And, If we check the Critical Multiplier - which is a stand in for HeadShots since I believe the Groin also counts - the reward is about average for accuracy. . sns.scatterplot(x = data.Damage, y = data[&#39;Crit Multi&#39;], hue = data.Type) plt.title(&quot;Damage With Critical Multiplier&quot;, size=15, fontweight=&#39;bold&#39;) . Text(0.5, 1.0, &#39;Damage With Critical Multiplier&#39;) . Conclusions . All in all, the DMR seems like a solid contender for best all around weapon type in the game. If you&#39;re going in budget with a single gun then I&#39;d recommend a DMR. We&#39;ll see if this is till true in Season 2. .",
            "url": "https://orgulo.us/game/python/data/science/exploration/cycle/frontier/2022/09/27/cycle-weapons-best-general.html",
            "relUrl": "/game/python/data/science/exploration/cycle/frontier/2022/09/27/cycle-weapons-best-general.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "The Lastest Lectures For Fastai are Online.",
            "content": "What is Fastai? . If you&#39;re not familiar with the list of frameworks and libraries that exist for Deep Learning - or even if you are - then you might not be aware of Fastai. This is both a Deep Learning Framework built on top of Pytorch as well as an online curriculum to quickly become productive using Deep Learning for your own interests. Jeremy is a very solid teacher and I recommend taking the course if you&#39;re interested in learning how this all works. And, I don&#39;t mean that simply because I like the lectures but also because I&#39;ve taken them before and I&#39;ve also built my own simple projects out of the lectures done by him - and Rachel since she&#39;s also assisting with building the course. One really big plus for this course is that you&#39;ll start by actually building a model and using it: in this version it will be Birds vs Forests instead of Cats vs Dogs like used to be. As Jeremy points out in the first lecture, most people don&#39;t learn in an academic way. By that we mean they start with theory and then learn how to interact with the system. Instead, we learn a few basic ideas and then toss ourselves in to apply and learn as we go. . So, What Am I Going to Do? . Lately, I&#39;ve been playing a game called The Cycle: Frontier which is a kind of Extraction Shooter game. What this means is that players are dropped into a semi-persistent server to collect items, kill creatures and even kill other players. Really, it is up to the player to decide how they interact with and play the game. But being a shooter means that there are guns - and since there are guns there are categories of guns. Since fantasy is inspired by the real world in some sense, the question I would like to ask is Knowing Models are Derived from the Real World Weapons, can we build a classifier based on real life weapons that can correctly predict Fantasy Weapons? . This post, like the lectures, is not going to be about how this all works but instead is going to be a simple application and retrospective. That being said, let&#39;s start! We&#39;ll start with the important imports for getting this working. . import os from pathlib import Path from time import sleep from duckduckgo_search import ddg_images # Will write more about this later. from fastdownload import download_url # Will need to explore this more. from fastcore.all import * from fastai.vision.all import * # This is for the CNN learner. from fastai.vision.widgets import * # This is required for the cleaner later. . The first three imports you should hopefully be familiar with. Everything else you should simply grant for now. . # This is a function from the notebook: def search_images(term, max_images=200): return L(ddg_images(term, max_results=max_images)).itemgot(&#39;image&#39;) . urls = search_images(&#39;assult rifle photos&#39;, max_images=1) urls[0] . &#39;http://allhdwallpapers.com/wp-content/uploads/2016/07/Assault-Rifle-5.jpg&#39; . So, we&#39;re going to download an example image using the helper functions so far and make sure its working: . dest = Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-ar-gun.png&#39;) download_url(urls[0], dest, show_progress=False) im = Image.open(dest) im.to_thumb(256,256) . Now let&#39;s check for a DMR and make sure that is sane: . dmrUrls = search_images(&#39;dmr photos&#39;, max_images=1) dest = Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-dmr-gun.png&#39;) download_url(dmrUrls[0], dest, show_progress=False) im = Image.open(dest).to_thumb(256,256) im . So, now we need data from the Internet: . searches = &#39;assault rifle&#39;,&#39;dmr&#39; path = Path(&#39;..&#39;, &#39;__data&#39;, &#39;ar_or_dmr&#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) sleep(10) # Pause between searches to avoid over-loading server and blocking responses resize_images(path/o, max_size=400, dest=path/o) . /usr/lib/python3.10/site-packages/PIL/Image.py:959: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images warnings.warn( . The below just checks to make sure that the downloaded images are in fact images. And, then we&#39;re iterating though the results to remove what are failed images: . failed = verify_images(get_image_files(path)) failed.map(Path.unlink) len(failed) . 4 . We can even check the source code using ?? and see for ourselves that is what it is doing: . ??verify_images . Signature: verify_images(fns) Docstring: Find images in `fns` that can&#39;t be opened Source: def verify_image(fn): &#34;Confirm that `fn` can be opened&#34; try: im = Image.open(fn) im.draft(im.mode, (32,32)) im.load() return True except: return False File: ~/.local/lib/python3.10/site-packages/fastai/vision/utils.py Type: function . DataBlocks are are really good idea which is a lazy wrapper that defines how and what to do with the downloaded images in preparation for use in a model. There is more information at the docs page here where you can check out other ways to use this. For now, we&#39;re following Jeremy&#39;s example and just mostly running the code as is. . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch(max_n=12) . Looking over some of htese images, we can already tell that we&#39;re getting images that are not what we&#39;re after. The second image in the top row is some kind radio which is not what we want. Regardless, we&#39;re again going to simply train the model even with that image in there since we&#39;re following Jeremy&#39;s advice and just going to train the model and we&#39;ll deal with the strange Radio later. . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.135293 | 1.895362 | 0.333333 | 00:01 | . epoch train_loss valid_loss error_rate time . 0 | 0.783494 | 1.115906 | 0.250000 | 00:01 | . 1 | 0.578771 | 0.935797 | 0.222222 | 00:01 | . 2 | 0.455172 | 0.774320 | 0.263889 | 00:01 | . 3 | 0.365537 | 0.762151 | 0.263889 | 00:01 | . 4 | 0.311685 | 0.737330 | 0.277778 | 00:01 | . 5 | 0.263628 | 0.734078 | 0.277778 | 00:01 | . 6 | 0.224883 | 0.718644 | 0.291667 | 00:01 | . So, this is what training will look like. The highlights are that training was very fast but - compared to many other models trained, including Jeremy&#39;s - this one is doing pretty terrible for a Deep Learning model. The validation loss is worse than random chance which means there are very serious problems with either out Deep Learning Architecture or the data. And, the architecture is definitely not the problem so it&#39;s the data. . is_ar,_,probs = learn.predict(PILImage.create(Path(&#39;__data&#39;, &#39;example-dmr-gun.png&#39;))) print(f&quot;This is a(n): {is_ar}.&quot;) print(f&quot;Probability it&#39;s an Assult Rifle: {probs[0]:.4f}&quot;) . This is a(n): assault rifle. Probability it&#39;s an Assult Rifle: 0.9858 . im = Image.open(Path(&#39;__data&#39;, &#39;example-dmr-gun.png&#39;)).to_thumb(512,512) im . Big Oof. That&#39;s not an Assault Rifle. . Time to do some data cleaning. Thankfully, the Fastai library comes with a very useful function which takes the images from the data, checks what their loss is and then presents them to us so that we can remove or re-label them. Make sure you don&#39;t make the same mistake as I did and correct import the widgets: from fastai.vision.widgets import * . cleaner = ImageClassifierCleaner(learn) cleaner . Don&#39;t forget once you&#39;ve actually ran and collected what changes you want made, you&#39;ll need actually run the below code to actually make those corrections: . for idx in cleaner.delete(): cleaner.fns[idx].unlink() for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . While trying to train this, I found that I was having issues unless I re-built the Data Loader so that&#39;s what we&#39;re doing here: . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch(max_n=12) . Got some weird stuff again. Looks like some bikes as well as some other equipment that I don&#39;t know. That would imply that using dmr is a bad search term for what we&#39;re after. We&#39;ll do one more training attempt to see how much harm those are doing and then we&#39;ll consider correcting the search terms. . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.281920 | 1.533394 | 0.413333 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.869596 | 0.949424 | 0.306667 | 00:02 | . 1 | 0.667905 | 0.852354 | 0.266667 | 00:02 | . 2 | 0.529514 | 0.767100 | 0.213333 | 00:02 | . 3 | 0.418889 | 0.783195 | 0.213333 | 00:01 | . 4 | 0.338373 | 0.754332 | 0.226667 | 00:02 | . 5 | 0.277043 | 0.730826 | 0.226667 | 00:02 | . 6 | 0.237752 | 0.718148 | 0.226667 | 00:02 | . Looks like our validation loss is still struggling so it&#39;s time to update our search term from dmr to designated marksmen rifle. Speedrun time! . searches = &#39;assault rifle&#39;,&#39;designated marksmen rifle&#39; path = Path(&#39;..&#39;, &#39;__data&#39;, &#39;ar_or_dmr_v2&#39;) for o in searches: dest = (path/o) dest.mkdir(exist_ok=True, parents=True) download_images(dest, urls=search_images(f&#39;{o} photo&#39;)) sleep(10) # Pause between searches to avoid over-loading server resize_images(path/o, max_size=600, dest=path/o) failed = verify_images(get_image_files(path)) failed.map(Path.unlink) . (#7) [None,None,None,None,None,None,None] . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch(max_n=12) . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.279173 | 0.758920 | 0.247312 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.725494 | 0.552330 | 0.215054 | 00:02 | . 1 | 0.572957 | 0.452633 | 0.161290 | 00:02 | . 2 | 0.429993 | 0.460270 | 0.172043 | 00:02 | . 3 | 0.329015 | 0.517723 | 0.215054 | 00:02 | . 4 | 0.260919 | 0.542659 | 0.215054 | 00:02 | . 5 | 0.212036 | 0.543762 | 0.204301 | 00:02 | . 6 | 0.176182 | 0.541302 | 0.215054 | 00:02 | . cleaner = ImageClassifierCleaner(learn) cleaner . for idx in cleaner.delete(): cleaner.fns[idx].unlink() . for idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat) . dls = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=[Resize(192, method=&#39;squish&#39;)] ).dataloaders(path) dls.show_batch(max_n=12) . learn = vision_learner(dls, resnet18, metrics=error_rate) learn.fine_tune(7) . epoch train_loss valid_loss error_rate time . 0 | 1.303078 | 1.055721 | 0.304348 | 00:02 | . epoch train_loss valid_loss error_rate time . 0 | 0.759311 | 0.487650 | 0.250000 | 00:02 | . 1 | 0.568219 | 0.461627 | 0.184783 | 00:02 | . 2 | 0.444043 | 0.539364 | 0.163043 | 00:02 | . 3 | 0.340317 | 0.588352 | 0.141304 | 00:02 | . 4 | 0.269084 | 0.642941 | 0.152174 | 00:02 | . 5 | 0.217371 | 0.619099 | 0.152174 | 00:02 | . 6 | 0.183307 | 0.610297 | 0.163043 | 00:02 | . is_ar,_,probs = learn.predict(PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;example-dmr-gun.png&#39;))) print(f&quot;This is a(n): {is_ar}.&quot;) print(f&quot;Probability it&#39;s an Assult Rifle: {probs[0]:.4f}&quot;) . This is a(n): designated marksmen rifle. Probability it&#39;s an Assult Rifle: 0.0029 . That is much better than what we were getting. Now for the real test: Can the model tell the difference between an Assult Rifle and a DMR from the game&#39;s wiki. . for image in [PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;cycle-dmr-gun.png&#39;)), PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;cycle-ar-gun.png&#39;))]: plt.figure() plt.imshow(image) . is_ar,_,probs = learn.predict(PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;cycle-dmr-gun.png&#39;))) print(f&quot;This is a(n): {is_ar}.&quot;) print(f&quot;Probability it&#39;s an Assult Rifle: {probs[0]:.4f}&quot;) . This is a(n): assault rifle. Probability it&#39;s an Assult Rifle: 0.9968 . is_ar,_,probs = learn.predict(PILImage.create(Path(&#39;..&#39;, &#39;__data&#39;, &#39;cycle-ar-gun.png&#39;))) print(f&quot;This is a(n): {is_ar}.&quot;) print(f&quot;Probability it&#39;s an Assult Rifle: {probs[0]:.4f}&quot;) . This is a(n): assault rifle. Probability it&#39;s an Assult Rifle: 0.9805 . Looking like either I don&#39;t have enough data or I don&#39;t know enough about the domain because it is struggling to figure it out. . Addendum . DMR also stands for Digital Mobile Radio which is why we were getting all those weird results earlier in the post. .",
            "url": "https://orgulo.us/machine%20learning/deep%20learning/fastai/lectures/lecture/lecture%201/2022/09/08/Reviewing-Fastai-2022-Coursework-TheBeginning.html",
            "relUrl": "/machine%20learning/deep%20learning/fastai/lectures/lecture/lecture%201/2022/09/08/Reviewing-Fastai-2022-Coursework-TheBeginning.html",
            "date": " • Sep 8, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Game Review: Pawnbarian",
            "content": "What is Pawnbarian? . A friend of mine recommended that I check this game out as he’d stumbled across it somewhere, somehow. Pawnbarian is what looks to be the first published game by j4nw and it’s a good start. There was an earlier iteration of the game on their itch.io page if you go lookikng for it to try out. Checking out the store page, the game presents itself well and is instantly recognizable since it’s a chess board with unique avatars of somekind for your piece and the enemies. The aesthetic is clean and simple which doesn’t distract while playing the game: . Gameplay? . The game is simple to undertand too - even if you’ve never been good at chess. You play as an Avatar - the starter is, of course, the Pawnbarian - which comes with usually a single gimmick and a different deck of moves. The Deck of Moves is how you move around the board to kill the enemy tokens - and they’re simply cards with different chess pieces which limit how you move. For example, the pawn moves like the pawn does in chess; the rook moves like the rook and so on. Your moves for the turn are drawn from this deck and you can see both the remaining moves as well as what was discarded so you have some idea of what could happen next round as the cards get pulled. You’re not limited by time since once the all the cards are drawn then they are simply put back and you get access to all of them again. . As mentioned, your avatar has a gimmick and the Pawnbarians is that when he plays a card with the Cantrip - the lightning symbol - then he gets to move again. And, you can chain these the more you play them to take as many moves as possible. When the cantrip is played then it also draws another card for you to play so you’re never limited by the cards in the deck. At the end of each round, you can use the gold earned by winning the round to add more abilities to your cards. The cantrip is not the only gimmick to add to cards; you can also add diagonal attack or a horizontal attack which will not only harm the square you land on but allow you to hit multiple enemy tokens. There more as well but we’ll limit the discussion to just those as they get the point across. . Enemies also have their own gimmicks, such as the Nimble attribute in the screenshot above, which constrains you simply stomping all over the enemy tokens. There are three dungeons with different enemies and different gimmicks to hinder you in your progress in the dungeons. I do wish there were more than the three dungeons since I stomped most of them much faster than I hoped. That isn’t to say some levels are not a challenge; the Void Grasp afflicted boss of Dungeons three was a frustrating challenge to beat but that’s why we play the game. What is not lacking is the list of different avatars you can play; there are 6 of them with their own ways to play the boards. . If you like Rouge-lights and Chess then you’ll end up enjoying turning this game on when you’re looking for a break from either Chess or Rouge-lights. .",
            "url": "https://orgulo.us/games/review/indie/chess/2022/09/08/Game-Review-Pawnbarion.html",
            "relUrl": "/games/review/indie/chess/2022/09/08/Game-Review-Pawnbarion.html",
            "date": " • Sep 8, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "How to Setup V Rising on Linux",
            "content": "The Quick Version: . If you’re here becuase you just want the answer then here are the exact steps that always work for me. . Download the V Rising Dedicted Server from steam to your account. | Download Proton Experimental and change to the Bleeding Edge Beta branch. | Run this command: STEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp &lt;Path/to/proton/experimental/install/location/proton&gt; run VRisingServer.exe &lt;args-from-offical-repo&gt; . If this doesn’t work then proceed to the below: | . | Download Lutris and install the dependecies it asks for. | Find and install the Epic Games laucher from Lutris. | Do the steps it asks you to run through: including those dependencies. | Reboot. | Run the command: STEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp &lt;Path/to/proton/experimental/install/location/proton&gt; run VRisingServer.exe &lt;args-from-offical-repo&gt; . | Story Time . I have been looking forward to playing this game since I accidently found it a while back. As someone who’s favorite genre is definitely Survival Games and Vampires are one of my favorite supernatural creatures, this game was going to be at the top of the list. Adding to that the skills along with skillshots for combat and this is about as perfect as it gets. Sadly, the game does not support linux and - per the Protondb page - most people were simply not able to get the game running. End of Story. . But in actuality, after a few days of work and some investigation by the wonderful Proton Community they found that it was missing some configuration files and they were taken from another game the developer had released. Once you copied those into the proper place then the game could launch. Luckily, Proton is very active and soon this change was pushed into the the Bleeding Edge Beta of Proton and once swapped over the game would run without issues. I also confirmed this myself and happily booted the game into my first test world! End of Story. . So, What About the Dedicated? . So, the problem is that this cannot work for the Dedicated server. Since launching the dedicated server only shows a link to the Instructions to configure the server and it assumes that it is running on a Windows Computer. All we have locally is the simple example .bat file for windows which is of little use to us. What we need to do is run the Dedicated server somehow with Proton and we’ll have to find a way to put this into a script. . How Do We Run Something Manaully With Proton? . Luckily, just like the wonder people over at the Github Page, we can find that answer online! According to this old thread from Reddit: . STEAM_COMPAT_DATA_PATH=~/.proton/ ~/.steam/steam/steamapps/common/Proton 3.7/proton run whatever.exe You need to create ~/.proton (it can be any directory and can be empty) . Excellent! Now we have something to work with. Looking at the command I built, I simply put those in /tmp since I didn’t want to think about what compatdata is; I still don’t really know what this is after a quick Google but it works like this. So, simply create a little script to be ran in bash: . #!/usr/bin/bash STEAM_COMPAT_CLIENT_INSTALL_PATH=/tmp STEAM_COMPAT_DATA_PATH=/tmp &lt;Path/to/proton/experimental/install/location/proton&gt; run VRisingServer.exe &lt;args-from-offical-repo&gt; . … and then chmod +x scriptName.sh and run it to start the server. . This Didn’t Work! . So, in my own troubleshooting I found that I ended up with an error Failed to create batch mode window: Success. when I ran this a second time on my server. However, after doing some testing on multiple different distributions - Ubuntu 20.10, EndevourOS, Manajaro - I found that it didn’t work on any of my systems even though it had ran once. Adding to my confusion, the dedicated server ran find on my own Desktop and without issues at all. Considering one of my servers was Manjaro just like my desktop then there must be something installed on locally which is not being installed along with the Steam Proton Dependencies. The only noticable difference in dependencies was that I have Lutris Installed on my own Destktop and it’s not on any of my other systems. So, I ran through exacty what is installed via Lutris and once I rebooted my other server it worked without issues. . So, What Is Missing? . The honest answer is that I don’t know. The list of dependences that gets installed along with the process is quite a list and I simply don’t have time to isolate which ones are the correct packages. If you have the time to figure it out then please let me know so I can update this post. Otherwise, Happy Hunting! .",
            "url": "https://orgulo.us/dedicated/gaming/v%20rising/linux/instructions/2022/06/01/how-to-setup-a-dedicated-v-rising-server-on-linux.html",
            "relUrl": "/dedicated/gaming/v%20rising/linux/instructions/2022/06/01/how-to-setup-a-dedicated-v-rising-server-on-linux.html",
            "date": " • Jun 1, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "How Do You Create a Normal Map?",
            "content": "How We Got Here . Looking around the Internet, it ended up being much harder to find the information about how to make a Custom Normal Map then I though it should be. After finally figuring this out, I thought I’d formalize/share it here so that it doesn’t get lost among everything else. . Where We’re Going . We’ll go over the how to do this with a simple example: dirt. First you’ll want to start blender up and clear the scene. Drop in a new plane and make it whatever size you like; I scaled it by 2x so if you’re trying to just copy my examples then simply do that. Since we’re just working on making a material to import into a game, we don’t really need anything else. So, we’re moving on to making the texture: . Again, this is just a basic example so we’ll create something like this: Not bad, not complicated but will show us the effect we’re after; feel free to copy the settings if you’re following along. Now add an Image Texture Node to the graph but don’t attach it to anything: . Create a new image and call it what your target material is about. In my case, I’m just making dirt so we’re calling it BasicDirt: Now we’re going to do something called Baking. So, what is Baking actually? What we’re doing is saving information to the texture so that it doesn’t have to be re-calculated. Eevee - the default engine in blender - doesn’t support baking so you’ll need to switch over to Cycles for this to work. Once that is done then go ahead and click Bake and it should work! You should now see the image show up in the Image Editor in the UI. You’ll want to save this to an acutal file for use in the Game Engine - or for other uses like making a normal map for it. . The Next Stop . Ok, now that you have the texture go ahead and pull this into your photo editing software. I’m going to do this with Gimp so to follow along you’re going to need it. Otherwise, you should just be able to look up “Making a Normal Map in Photoshop” and it should be a simple option. I spent quite a bit of time trying to make the Normal Map in Blender and I couldn’t get it working; if someone did figure this out then please let me know. But until then, you’ll want pull your image into Gimp first. Once that is done, you’ll select Filters &gt; Generic &gt; Normal Maps. and it will present you with the ability to set how harsh the effect is: … and that scale is not the default. Go ahead and play around with that number to get the effect you want. Depending on your Game Engine, save it with the proper nomenclature for when you pull it in. . Get your Blender project back open and hop into the Shading Pane once more. Add a new Image Texture Node and insert the new Normal map you created. Once added the image into the Image Texture Node, you’ll want to switch over to non-color: . And, that’s how you Make a Normal Map, and add it into blender. You can improve it some using other options like adding a Bump Node between it. .",
            "url": "https://orgulo.us/gimp/blender/baking/normal/map/game/2022/02/02/How-To-Make-Matching-Textures-And-Normals.html",
            "relUrl": "/gimp/blender/baking/normal/map/game/2022/02/02/How-To-Make-Matching-Textures-And-Normals.html",
            "date": " • Feb 2, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Game Review: Eraser",
            "content": ". Throw your stick-figure-self at cannonballs and airplanes in an upward pursuit of the eraser! Play cooperatively online to share the journey! Who’s to say whether the company will help or hinder, though… . Recently, I was looking for some simple and free games to enjoy and ran across this little gem. Eraser by Ringating feels like playing a Platformer from the 90s but with the Visuals and Game Design philosophy of now. Let me explain. . First off, there is no such thing as death in this game. You simply cannot die and there is no real punishment for making a mistake. The worst that happens is that you have to start all over or fall a little ways down and have to regain ground again. You might be thinking they’re the same because if you died in platformers from before then you’d also have to start all over; They all had a checkpoint system where if you had lives and you died then you could start from the checkpoint again. Games like Spyro have the faeries which record your position after all. But, this game has no hard Start Over Because You Died like those games do. Sure, you could incidentilly fall all the way down - I certainly did more times than I’d like to admit - but there is no hard lose condition. You keep playing as long as you’re willing to and sometimes you fall and have to start part of it again. Somewhere along the way, the levels make it harder to fall to to the beginning so there isn’t even really a checkpoint system as much as the design makes it harder to return to the beginning. . The game itself is just smooth to play. The musical theme of simple classical fits nicely with the theme of the game of being a simple stick figure in a simple world full of simple enemies and obstacles. I’d definitely recommend picking it up and playing it for a few minutes when you don’t have a lot of time to commit to something else - and it’s free on Steam! .",
            "url": "https://orgulo.us/game%20review/game%20design/platformer/free/2022/01/06/Game-Review-How-I-Lost-My-Eraser.html",
            "relUrl": "/game%20review/game%20design/platformer/free/2022/01/06/Game-Review-How-I-Lost-My-Eraser.html",
            "date": " • Jan 6, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Review of 2021 - Challenges, Thoughts",
            "content": "How did the challenges go? . The honest truth is that I what I wanted to get done this year was pretty fluid and I did not get many of the commits done. Looking at what I have been working on though - and what I ended up spending my time on - I didn’t do so bad this year. . Firstly, I finally got a macro workflow that is working for pushing my projects forward along with learning new topics. One of the problems which I have been struggling with is how to both learn enough new concepts and frameworks along with actually using them. The normal process being used it to . Find something of interest. | See what’s been made in it. | Work on tutorials, docs for the framework/tool/information. | Try to apply it to pet projects. | Maybe use it in a real project. | This process has problems that I needed solved. Since I have my hands in multiple different places - Data Science, Linux, Programming, Web Design, Secret Project I can’t discuss yet, etc - then this simply doesn’t work. With my life growing in so many dimensions, I simply cannot sit down and follow this process for a single framework and then maybe add it to what I am working on. If it takes a month or two to apply and become comfortable with the framework then it’s too slow and the other aspects of my life will end up neglected. So, what do I do now? . I have split the months of the year up into four quarters - just like a a business calendar. Then, the first month of that quarter is a Learning Month which means I review what I need to learn, allocate out a set of classes for the month and then do power through them in that month. I take notes about what I’m learning and for some parts will actually practice with a very small pet project which is meant to implement a single thing. After that month is over, the next two months are Apply Months which means that I take everything that I learned in that month, review it, and then start adding it to my projects. . This allows me to schedule up to four of classes covering any material I might need coming up in the next few months. This is also better because it keeps learning and application separate. When my schedule was fluid I kept finding ways to not work on projects since I constantly fell back into learning about new frameworks and tools. Now, the clear separation is allowing me to be strict about when and what tasks are supposed to be done. It is already paying out dividends – even though the first attempt at it I was sick for most of the first month of Apply. . Read 100 Books . Of all the challenges that I wanted to complete, this is the one. And, I did worse than last year too: 81 books last year vs 31 books this year. Looking over how my life has changed since the Pandemic started, I think the biggest reason is that much of my reading was done in between. If I was waiting in line at a store then I was reading; while walking from the parking garage at work into the office I was reading. Now that we’re remote only and there are no lines when I am out running errands, I just don’t really read books as much as I used to. I do listen to them while walking around stores but often I don’t count those in the totals; I finished all of the Dragonlance Chronicles books via Audible for example and they’re not filled out on Goodreads. This means setting up time at home to read and I’m always working on projects or doing chores or watching lecture videos. That really only leaves downtime at work to read and we’ve been relatively busy lately. And, I happen to also have projects for Datto that I work on so that’s not really in the cards either. I’ll have a separate post to what the Challenges of 2022 are going to be so strategies will be discussed in that post. . Create Three Potential Streams of Income . For this one, I was successful. Currently, I have two streams of income: one active (Datto) and one semi-passive (Crypto) to work with. I am making money in both of those realms at this point which was more than I had set for the goal. The goal was to identify three potential streams of income that I could have for the year and there was no shortage of ideas for this one. I’ll discuss some of my favorites in the real Challenges of 2022 post but there are plenty more than three, . New Job Title . This is was a failed challenge. After exploring and talking with lots of people, this is going to be the hardest one. And, you’d consider that since I already have so many useful skills that it would be the easiest but it’s simply not. Realistically, I like where I work and I don’t really want to leave any time soon. All the positions that are within reach are jobs that I simply don’t want. And, I am not willing to trade in the job I have for those other jobs since they’d change something fundamental about what I want: working overnights, well limiting interacting with users, flexible time table for my own work. I’ll discuss more details in the Challenges of 2022 post when I write it. . A Blog Post a Month . This is another place that I did decent but did not hold myself too. The biggest factor for this was simply not setting aside the time for it. It’s lower on the list of priorities which need to get done so I simply didn’t schedule out the time. Along side that was anything I wanted to write about I didn’t feel either qualified to write about or didn’t feel what I was willing to write would be be long or interesting enough to justify the post. This is a mindset problem which I should be able to easily solve in the coming year. Especially after looking at what the average person on Medium is writing about; they’re not even trying to write quality content most of the time but instead are simply writing very short not-even-tutorial posts about how to use a framework. There is also a surprising amount of “X Ideas fro Passive Income” posts which are mostly all the same thing over and over. I can do better than that. . Draw an Original Comic . I dropped this one entirely – and I’m dropping it for next year too. I have a list of Daily Tasks which include things like exercise, typing practice along with drawing practice and by far the task I skip the most is drawing. I need to change this so I can actually start drawing at an ok level. Right now, I am struggling with curved lines and that’s quite basic for this level of challenge. I have a few ideas – when don’t I? - but this is a long road which I need to build basic habits out before even trying some kind of real challenge like a Comic. .",
            "url": "https://orgulo.us/projects/challenges/future/2021/12/08/Review-Of-2021-Challenges.html",
            "relUrl": "/projects/challenges/future/2021/12/08/Review-Of-2021-Challenges.html",
            "date": " • Dec 8, 2021"
        }
        
    
  
    
        ,"post18": {
            "title": "Challenges for The Year: 2022 Edition",
            "content": "Read 50 Books . I’m lowering the number back to 50 books even though 100 books is still the larger goal. The real reason is I need to find a way to fit reading back into my schedule. With so many projects, I will be shifting to audiobooks which is depressing since I read faster than I can process audiobooks. I do have a trick for this: listen to ebooks via the Kindle Store in a Web Browser. This is probably a good enough place to explain how this works. . Install an extension for your Web Browswer which does Text-To-Speech - such as Read Aloud. Honestly, finding a quaility Text-to-Speech reader and paying for the extension is a better idea than free; the lack of intonation from the extension I am using makes listening much harder. Concentrating is harder compared to a professionally done book from Audible. Go to Your Online Kindle Library and find a book to listen to. Open the book up and then hit the play button inside your Text-To-Speech extension. You will need to adjust the speed, voice, volume and location when you hit play per new book for tuning. But, this is a good way to fit more average fiction reading into your day. . Putting this in the background of the first half of my work shift sounds correct. Since I am not usually interacting with people then it should be an easy way to get 2-3 hours of “reading” done each day. Let the challenge begin. . Have Three Streams of Income . We’re moving from planning to application with this one. . Primary “9-5” Job. . Obviously, my job is the primary source of income - and I have no real interest in changing that. Working Technicaly Support is usually an awful experience since you’re dealing with End Users which have no real understanding of technology nor how their behaviors make little sense. However, where I work I am dealing with people that at least have some background in technology so having conversations with them is usually much less painful than End Users. Since my role is also on Overnights, this significantly limits the interactions with the kinds of people who constantly cause problems for themselves - and therefore me. . Streaming . I’m going to take a serious attempt at making money on Content Creation sites like Youtube, Twitch and TikTok. The competition here is enormous and this is not the kind of job I admit I’m naturally good at but it fits well into my life. I play games with my friends and coworkers consistenly enough that these can be put online without too much trouble. Getting people to watch or care about the content is a totally different problem. Doing the research that I have so far, we’re really going to have to lean on Personality quite a bit otherwise this simply wont work. Due to being on Linux, there really is nothing but Splitgate which I can play competitively and stream. And looking at the numbers, Splitgate is struggling both as a streamed game as well as keeping an audiance of players: . So, maybe I could try and niche with Splitgate but I would be concerned with getting locked into playing a game struggling to grow. To start though, I will need: . To become Affiliate on Twitch. | Get better at tags on Youtube. | To start posting shorts to Tik Tok. | Web Design . This is the least fun option but I’m capable of this. I have found a set of frameworks and a niche that I can work in without getting too far dragged into the larger frameworks: Single Page Websites. As I expand my ability to use tools such as Blender, Inkscape then I can push into more artistic and aesthetic styles instead of the more normal Single Page Sites. Not much else to say aside from if this works out then expect a Portfolio coming soon. . Logo Design . Between slowly expanding Blender skills and working on my drawing skills, this would be something fun that I could doodle designs and then try to sell to others. There is quite a bit about communicating concepts artistically along with a good amount of Color Theory to learn before I can do this. . New Job Title . This year is the real year that matters though for my job; I have a patent pending for the company and this should be year I get the nay/yay for whether it goes through. No matter how that goes, I’m pushing for an Analytics Job either inside or outside the company. I am familiar enough and competent enough to be doing this as a real job and I am well bored of doing Technical Support. It also is not a real future for me; there is no where to go that I want to be. So, if by six months in I don’t see any progress here then I’m going to start applying for part time Analytics jobs to build experience and push into a new career. . 36 Blog Posts For the Year. . I have been spending a good amount of time reading Medium posts and articles by others this year. And, my expectations are far too high and totally unnecessary. While in college, I learned about one of the books that significanly changed my expecations about writing: The Elements of Style by Strunk and White. The book emphasis is always on Omit Needless Words and I extended that to all of my writing since then: write only what needs to be said. While a good guide, this is now preventing me from writing at all. If someone else has said it then I am not writing about it; if someone hasn’t written about it then I am not confident in writing about it. I know enough and am learning enough to write content to clarify my thoughts for myself and others. So, three posts per month on anything I feel like: Game Design, Data Science, etc . Make the Secret Project Public . By the end of the year, there will be public posts about this project. Watch for a reveal in the future. .",
            "url": "https://orgulo.us/projects/challenges/future/2021/12/08/Challenges-For-2022.html",
            "relUrl": "/projects/challenges/future/2021/12/08/Challenges-For-2022.html",
            "date": " • Dec 8, 2021"
        }
        
    
  
    
        ,"post19": {
            "title": "Game Review: Ethereal Estate",
            "content": ". I had quite a bit of fun with this game. While working on a Data Science project to try and make predictions on “What Genre Is this Game Based on it’s Header Image?” I stumbled across this weird little game trying to solve an error. Looking through the sample set of games, Ethereal Estate pops out among all the other DLC headers and who knows what else these other images are supposed to inspire. The game is free and you play as a ghost - along with maybe your friend - to terrorize, ruin and throw everything within reach around: like a proper ghost should! . Anyways, the trailer is simple but hilarious so good work Emergence Studio for getting my attention and pulling me in. Your ghost experience is a shopping list of mayhem courtesy of the designer where you cross all kinds of lines: lighting things on fire, throwing people through windows, ramming things with swords, stuff people in closets, poisoning food, breaking records, bringing your victoms back to murder them again, etc. The game is surprising smooth and simple - and fun. My friend and I found the list a bit unclear while roaming around throwing people through windows; my favorite activity in the whole game. I just hope there are more levels coming since there is only the one and we cleared it without too much trouble. Maybe a more traditional graveyard? .",
            "url": "https://orgulo.us/games/review/sandbox/ethereal/estate/simulation/funny/2021/09/07/game-review-ethereal-estate.html",
            "relUrl": "/games/review/sandbox/ethereal/estate/simulation/funny/2021/09/07/game-review-ethereal-estate.html",
            "date": " • Sep 7, 2021"
        }
        
    
  
    
        ,"post20": {
            "title": "Game Review: The Forest",
            "content": "The Forest is another game in the Survival Genre which tries to use Environmental Storytelling to push a plot. You can play with your friends or you can play alone - which definitely gives you a different experience. Alone - so I’m told - feels a lot lot a horror game where you’ll fear leaving the walls of whatever you’ve built. With friends, it was good fun but definitely doesn’t feel like a horror game: FREE BONES! . We’ll start with the stuff I didn’t like first. Like most survival games there is crafting for shelters, walls, defenses and cooking. Unlike most other survival games that I’ve played, crafting buildings in this one feels pretty pointless. There are about four buildings that actually matter: Drying Rack, Bonfire, Water Collector, Shelter. Anything that produces light barely matters and you have to constantly feed fuel to the fires to get the initial “overburn” to really see in the darkness - or really far at all. . Walls are fun but we built on an island off the coast of the main island so we basically just started building random stuff for fun since our base never got attacked. Which is real unfortunate because this game has one of the best building systems I’ve ever played with. The buildings are cumulative so you can build what is basically a blueprint that your friends can collectively dump resources into. And, everyone can see them and since there is a visual blueprint you can actually lay out your base “Blueprint” in full for discussion. What a great idea! . Since it’s Environmental Storytelling, you can basically skip all of it until the end while murdering everything along the way. This is what happened to us - we ended up running into the caves to wipe out the natives and just kind of picked up whatever we found - on our way to kill more of the natives. This isn’t really the game’s fault but more the way we played so beware of missing stuff until the end when you have no choice but to address the story that you don’t really understand. . The combat is surprisingly fun and one the better Combat systems in a Survival Game. Ranged weapons are not broken and do not nullify enemies into a shooting range and melee feels really good with trading blows with the Cannibals. This isn’t really due to the system being complicated but much more about how well done the animations and the interactions with the Natives themselves: they flee, they try to kind of spook and intimidate you, they call their friends. Enemy and animal variety was pretty good and you get introduced to newer enemies at the right times - at least for us. Learn how to block; you’re going to need it. . I mentioned the Night above but it’s really a positive: when it’s dark it is fucking dark. Wandering around at night can get you lost really fast. Everything moving around you - leaves, trees, animals - will making you constantly on edge thinking about if you’re about to get attacked in the dark. Following enemies in the dark is just hard so you wont know how many there are nor where they ran of to. Running around in the Forest really does feel like moving around in the woods stumbling onto deer and opposing murderhobos while you try and find a stable water supply. . A few annoyances around multiplayer we ran into where one time we died and my friend had to afk to do something else. And, I got stuck in the plane behind him for a full hour - not game time, real life time - since it was impossible to actually get around him to play the game. . Another is that the server doesn’t actually save the state. You must save your own character at shelters otherwise you lose progress. This makes absolutely no sense at all when we’re playing on a server being hosted by someone else. You will lose progress if you don’t personally constantly save. . Another was that since we were all doing our own tasks and split up often, some of us didn’t end up having all the tools necessary to get to the end of the game. As a five-man group, one of us got stuck behind because of this while we basically tried to finish the game. If you do this then the bad news is that you all have to vote at the end otherwise you can’t finish the game. We ended up just watching the endings on Youtube after doing all the work to get to the end. . Also, game works in Linux completely in Proton 6.3-4. .",
            "url": "https://orgulo.us/games/review/survival/2021/05/18/Game-Review-The-Forest.html",
            "relUrl": "/games/review/survival/2021/05/18/Game-Review-The-Forest.html",
            "date": " • May 18, 2021"
        }
        
    
  
    
        ,"post21": {
            "title": "Credit Card Security",
            "content": "How Secure is a Credit Card? . Everyone is worried about having their credit card information stolen at some point. We all trust the banks to ensure nobody can just guess our number and submit orders in our name. But, did you know math is the secret behind your security? There are four main issues with stealing a credit card: . Getting the Credit Card Number | Getting the Security Code | Getting the Expiration Date | Getting your Personal Details | Getting the Credit Card Number . There are a total of 10000000000000000 total numbers that can exist for a credit card number. The odds, therefore, to get your number is 1:10000000000000000. But, it’s worse than that because the entire collection of numbers are not used to generate credit cards. In fact, credit cards can pass or fail passed on something called Luhn’s Algorithm This algorithm is used to create what is called a subgroup inside of the total number of cards. We don’t want criminals to just guess numbers and accidentally hit anyone. This is formula is a first line of defense to ensure fake numbers cannot be submitted. . ALGORITHM_CONSTANT = 9 def confirmLuhn(number): #==== Luhn&#39;s Algo ========================== count = 0 # Sum all numbers excluding the last number. sum = 0 # For every other number, square it. # Compare the last digit of the CC number for digit in number: # against the last digit of the sum. num = int(digit) if count == 15: # If final digit, return (sum *ALGORITHM_CONSTANT %10 == num) # compare final digits. elif count % 2 == 1: # If off number, sum += num*num # square number. count +=1 else: # If not off number, sum += num # just add the number to the sum count += 1 #=========================================== outfile = open(&quot;CCNumbers.txt&quot;, &#39;w&#39;) # File opening, obviously. # First we generate the number space that we&#39;re testing. for x in range(0, 0000000000000100): # The real CC number space is actually [0, 9999999999999999], but that&#39;s too much. xString = &quot;%.16d&quot; % x # This is probably lazy design, but I want to fill out the space of numbers and keep the 0&#39;s if confirmLuhn(xString): outfile.write(xString+&quot;n&quot;) # If it&#39;s one of them, then write the result to the file. outfile.close() # BILLIONS OF YEARS OF WORK AND YOU FORGOT TO CLOSE THE STORAGE FILE IN THE CODE?!? . I wrote a basic CC number finder that uses a very small space of numbers to check against as existing numbers - and, you can find it herefor those wishing to pull it and build on it. . Getting the Security Code . The probability of getting the 3 digit number on the back of that card is 1:1000. That might not seem like a lot, and it’s not, but without that number you can’t use the card online. And, most banks will lock your card after a few failed tries. If I can see and memorize the 16 digit Credit Card number from seeing once - this isn’t all that hard, it just takes a bit of practice - this 1:1000 probability is what keeps them from using it online without your consent. . Let’s assume that I did memorize your number and I’m trying to figure out what that code is. Most banks I know of will lock a hard after 5 failed attempts. That means I have a 5:1000 or 1:200 chance of guessing right before the card number becomes useless. Of course, that’s completely ignoring if I don’t have the number and am just trying to guess everything. Even if I beat the 1:10000000000000000 chance of picking the right number and it passes the Luhn Algorithm then I still need to guess with a 1:1000 chance of getting the right number within each potential number. But, it gets even worse for me. . Getting the Expiration Date . I still have to get past your expiration date. There really is no good way to know the actual probability since banks can use any number of years before the card expires. But, I’ll assume a larger case to overestimate since banks will make the task as hard as possible. Let’s guess that the range of expiration dates is from 2000 - 2030 since some cards are in the wild. We’re looking at a 1:360 chance - which is really tiny when you compare it to other probability spaces. But, we need to think about the entire probability space of 1:10000000000000000 * 1:200 * 1:360 just to find the right card. To select the right card data of just you is 1:720,000,000,000,000,000,000. For comparison, that’s about 6 multiples large than the estimated total of every human being ever alive. . But, what if I memorized all of this information? . Getting your Personal Details . Even after selecting the right credit card with the right security code and the right expiration date you still need to know the precise living address of the person you’re trying to attack to submit orders online. No wonder credit card scammers steal them with skimmers or fake online bank pages! Math wins. .",
            "url": "https://orgulo.us/cysa/security/math/2021/02/20/Luhns-Algorithm-Credit-Card.html",
            "relUrl": "/cysa/security/math/2021/02/20/Luhns-Algorithm-Credit-Card.html",
            "date": " • Feb 20, 2021"
        }
        
    
  
    
        ,"post22": {
            "title": "What is a Memorandum of Understanding?",
            "content": "This post is for the Review and understanding of concepts related to the Comptia Cybersecurity Analyst+ Certificate. . A Memorandum of Understanding primarily serves as a point of negotiation between two or more parties with respect to their intents. The document serves as a way to find common ground along with set expectations about what individual parties are both responsible for and expect from the other party. Special consideration should be paid to whether the document is Legally binding in the legal context it is being used; these are normally not legally binding. It is best thought of as similar to a Gentlemen’s Agreement: “an informal and legally non-binding agreement between two or more parties. It is typically oral, but it may be written or simply understood as part of an unspoken agreement by convention or through mutually-beneficial etiquette”[2]. . Examples . Sample from Violence Against Women Oganization | FormSwift | . Citations: . https://en.wikipedia.org/wiki/Memorandum_of_understanding | https://en.wikipedia.org/wiki/Gentlemen’s_agreement | https://www.investopedia.com/terms/m/mou.asp | https://www.investopedia.com/terms/l/letterofintent.asp |",
            "url": "https://orgulo.us/cysa+/review/2021/02/09/Memorandum-Of-Understanding-Blurb.html",
            "relUrl": "/cysa+/review/2021/02/09/Memorandum-Of-Understanding-Blurb.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post23": {
            "title": "Quick Introduction to Streamlit",
            "content": "What is Streamlit? . Streamlit is a Framework which is growing in popularity among Python Data Scientists. And the reason for this is rather obvious if you’ve every tried mixing building Web Applications with Data Exporation or Reporting for other people inside your company - or just in general for your projects. . We’re going to build a little Web App with the Iris dataset. And yes, please don’t groan; I am also sick of the Iris dataset but it’s clean and simple and accessible. . So, first we start with actually downloading and it’s just their namesake: . python -m pip install streamlit . Next we’ll import and scaffold off their introduction application: . import streamlit as st # To make things easier later, we&#39;re also importing numpy and pandas for # working with sample data. import numpy as np import pandas as pd st.text(&quot;Hello World&quot;) . … and save that to a file called irisExplore.py. Once that’s done, you’ll run it from the console using streamlit run irisExplore.py: . And there we have our first Web Application. Calling the HTML elements that you want and expect is just that simple. If you want a paragraph then you’ll use st.title(): . st.title(&quot;Turtles Turtles Turtles&quot;) st.subheader(&quot;But, which movie reference is it?&quot;) st.text(&quot;Hello World&quot;) . And, there is even support for Latex too!: . st.title(&quot;Turtles Turtles Turtles&quot;) st.subheader(&quot;But, which movie reference is it?&quot;) st.text(&quot;Hello World&quot;) st.latex(r&#39;&#39;&#39; a + ar + a r^2 + a r^3 + cdots + a r^{n-1} = sum_{k=0}^{n-1} ar^k = a left( frac{1-r^{n}}{1-r} right) &#39;&#39;&#39;) . . Now with Data! . So, now we’re going to take a quick look at how to add charts. First we’re going to collect the iris dataset. If you have R or something like Tensorflow installed then you can import it from there. I’m going to collect it from someone random on the internet. You should probably download this dataset and store it somewhere; you’ll never get away from it. . import pandas as pd iris = pd.read_csv(&quot;https://datahub.io/machine-learning/iris/r/iris.csv&quot;) . Adding the chart is just as easy and you would expect: . st.line_chart(iris) . Not exactly what we’re looking but it’s kind of neat. This should serve as a reminder that when it comes to tools, it’s Garbage In, Garbage Out for what you feed it. Be careful what you’re doing because our tools wont know any better. .",
            "url": "https://orgulo.us/python/data%20vis/data%20visualization/sreamlit/2021/02/04/Introduction-To-Streamlit.html",
            "relUrl": "/python/data%20vis/data%20visualization/sreamlit/2021/02/04/Introduction-To-Streamlit.html",
            "date": " • Feb 4, 2021"
        }
        
    
  
    
        ,"post24": {
            "title": "Minimal Spark Cluster",
            "content": "Introduction . If you&#39;d like to setup Apache Spark to experiment with but you don&#39;t want to use a premade ISO or setup your own then I&#39;m going to show you how. This configuration will be a minimal one using Linux Operating Sytems; I&#39;m going to use Ubuntu so change the install based on your package mananger. I&#39;m going to assume that you&#39;ve setup the hosts, their networking and have some way to configure and deploy them. There are options like Puppet or Salt but I&#39;ll be avoiding those and leave them up to you. . Installation . I have script that does all of this but we&#39;re going to go over it so you understand each part and then I&#39;ll attach the bash script at the end. To start with, we&#39;re going to need Java since Spark is dependent on it. Login to each host - Master and Slaves - and you&#39;ll want to run: . apt update &amp;&amp; apt install openjdk-8-jre openjdk-8-jdk -y . Remember to adjust this for own Linux Distribution. . Figure out which directory you want to install Apache Spark into; Normally, you&#39;d use /opt so that&#39;s what we&#39;re going to be using. Download the archive of the files from the website: . wget -O /opt/spark-2.4.6-bin-hadoop2.7.tgz https://downloads.apache.org/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz . If they obsolete the download link then you can find the versions online here. I&#39;m using the 3.0.1 version Pre-built for Apache Hadoop 2.7 for this but feel free to experiment. Once the file is downloaded, you&#39;ll want to unpack the archive: . tar -xzvf /opt/spark-2.4.6-bin-hadoop2.7.tgz -C /opt rm /opt/spark-2.4.6-bin-hadoop2.7.tgz . Now you&#39;ll have files ready for usage. Next you&#39;ll want to add the environmental variables so that linux will know where to look for the binary when you call it: . echo -e &quot; nexport SPARK_HOME=/opt/spark-2.4.6-bin-hadoop2.7 nexport PATH=/opt/spark-2.4.6-bin-hadoop2.7/bin:$PATH&quot; | tee -a ~/.bashrc export SPARK_HOME=/opt/spark-2.4.6-bin-hadoop2.7 export PATH=/opt/spark-2.4.6-bin-hadoop2.7/bin . Do this for all the hosts that you&#39;ll need to run Spark on. . Configuration Files . It is best to create a master copy of these next few configuratoin files to copy to each host in turn. This way you only need to edit each file ones and then copy them all the the appropriate hosts . For your master server, you&#39;ll need to update two files: spark-defaults.conf and spark-env.sh. Both of these should be found inside the conf directory of your spark home. Make sure you make a backup of them before you do anything else: . cp spark-defaults.conf spark-defaults.conf.bkp cp spark-env.sh spark-env.sh.bkp . Next you&#39;ll want to open them and add the master info information. For the defaults file simple uncomment and modify the line to look like: . spark.master spark://&lt;host or IP Addr&gt;:7077 . ... and the env file you&#39;ll want to look for the line: . SPARK_MASTER_HOST=&#39;&lt;host of IP Addr&gt;&#39; . You shouldn&#39;t need to change anything else in this file so long as you have full control of the systems you&#39;re using. In my case, I happen to not and changed where shuffle and worker logs are ran to avoid the operating system from filling up and crashing the host. If you need to worrry about those then update the lines SPARK_LOCAL_DIRS, SPARK_WORKER_DIR and SPARK_PID_DIR to point to somewhere else on the system which wont fill up the partition. . Next you&#39;ll want to collect the names or IP addresses of all the hosts in your cluster and add them to the slaves file in config directory just like were the others are. Make sure to test the connectivity of your hosts using ping or something else to confirm they can actually talk to one another! . Datastore . Now we&#39;re going to work around not having a Hadoop cluster. How this works, is that we&#39;re going to create a shared folder on all of the hosts which references the Master as the Source of Truth. First, create a folder in your spark home to hold the data: . mdkir $SPARK_HOME/Data . Go ahead and create a file in here for future usage: touch turtles . Next you&#39;ll go ahead and install a package called sshfs which is used to remotely mount a folder from one host and another: . sudo apt install sshfs . Repeat this for all the hosts in your cluster. Once that is done, you&#39;ll connect the slaves to the master using: . sshfs &lt;username&gt;@&lt;master-address&gt;:/opt/spark-2.4.6-bin-hadoop2.7/Data /opt/spark-2.4.6-bin-hadoop2.7/Data . Now you should be able to see the turtles file we created earlier if you list the files in the Data directory . ls Data . If you see the file then feel free to move on! If not, then double back and troubleshoot the connection between those two computers. Could also be permissions or something like that as well! . Connect the Dots, Start the Services . Now that we&#39;ve got it all connected together, go ahead and run the appropriate commands on the masters and servers to start them all up: . # master: $SPARK_HOME/sbin/start-master.sh # slaves: $SPARK_HOME/sbin/start-slave.sh spark://&lt;master-Addr&gt;:7077 . Success! . Now try and run it on the master: . username@HOST:~# $SPARK_HOME/bin/pyspark Python 2.7.12 (default, Apr 15 2020, 17:07:12) [GCC 5.4.0 20160609] on linux2 Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. 20/10/13 05:19:50 WARN Utils: Your hostname, HOST.localdomain resolves to a loopback address: 127.0.0.1; using &lt;address&gt; instead (on interface eth0) 20/10/13 05:19:50 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address 20/10/13 05:19:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Using Spark&#39;s default log4j profile: org/apache/spark/log4j-defaults.properties Setting default log level to &quot;WARN&quot;. To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel). Welcome to ____ __ / __/__ ___ _____/ /__ _ / _ / _ `/ __/ &#39;_/ /__ / .__/ _,_/_/ /_/ _ version 2.4.6 /_/ Using Python version 2.7.12 (default, Apr 15 2020 17:07:12) SparkSession available as &#39;spark&#39;. &gt;&gt;&gt; . That should give you the above. . Now you can transfer data into that directory and read from it using the spark.read.* function that you need. Note that copying Big Data into that directory is not a good idea. If you&#39;re looking at TeraBytes or Petabytes worth of data then you&#39;ll definitely need a real Cluster. But, I&#39;ve already made some interesting observations in this limited environment. .",
            "url": "https://orgulo.us/python/data%20science/apache/spark/2021/01/18/Minimal-Spark-Cluster-Without-Hadoop.html",
            "relUrl": "/python/data%20science/apache/spark/2021/01/18/Minimal-Spark-Cluster-Without-Hadoop.html",
            "date": " • Jan 18, 2021"
        }
        
    
  
    
        ,"post25": {
            "title": "Challenges for 2021",
            "content": "Read 100 Books. . This is a challenge because I haven’t done it yet and fitting that many books into my life forces me to think about my spent time. With 52 weeks in the year, this means I will need to read 1.92 books per week. If I’m going to have a break day from everything - on Sundays - then that usually means reading about 100-134 pages of the current novel per day. Doing this along with being productive and pursuing larger goals gets it onto my list. . Create Three Potential Streams of Income. . I wont be discussing what those are but if modern society and the current state of affairs has taught me anything it’s to be proactive. You don’t want to be responding to a loss after it’s happened; you want to have responded before it has happened. Always have a plan B - and maybe a C. If you need your computer to work, then you’ll want a fallback that can throw in as a substitute. With working from home, the first thing I did was check to see if I can use my phone as a substitute for if my ISP decides to do “maintenance.” It works by the way. I wont hold myself to actually generating an income yet but that’s a stretch goal for the year. . New Job Title . I’m bored of being a Senior at this point and either need some larger technical challenges or just a swap in the set of responsibilities I deal with on a daily basis. There are some possibilities which I will try either to create or follow up on but I’m not staying where I am. . A Blog Post a Month. . I have this place where I can drop interesting project ideas or discuss problems that I’ve solved for myself to share with others. Since I don’t have a lack of problems to solve, then I should be writing about them for others to read and refer back to. Solving a problem a month is not the challenge but remembering to write about it is. . Draw An Original Comic. . If you’re following me on any social media then you’ve probably seen some crude drawings. While I admit I’m doing them for my own enjoyment - and challenge - I should push towards something tangible. . [A book you’ve been meaning to read.]: The Grammer of Science by Karl Pearson | [A book about a topic that fascinates.]: Unrestricted Warfare by Qiao Liang, Wang Xiangsui | [A book in the backlist of a favorite author.]: Legionaire by Jason Anspath, Nick Cole | [A book recommended by someone with great taste.]: Awake in the Nightland by John C. Wright | [Three books by the same author.]: Forgotten Ruin by Jason Anspath, Nick Cole | Hit And Fade by Jason Anspath, Nick Cole | Violence of Action by Jason Anspath, Nick Cole | . | [A book you chose for the cover.]: JinJang by Iris Paustian | [A book by an author who is new to you.]: The Forge of Christendom: The End of Days and the Epic Rise of the West by Tom Holland | [A book in translation.]: The Nine Chapters on the Mathematical Art: Companion and Commentary by Shen Kangshen | [A book outside your comfort zone.]: The Social Construction of Reality: A Treatise In the Sociology of Knowledge | [A book published before you were born]: The Constitution of the United States of America by The Found Fathers | .",
            "url": "https://orgulo.us/projects/challenges/2020/12/30/Challenges-For-2021.html",
            "relUrl": "/projects/challenges/2020/12/30/Challenges-For-2021.html",
            "date": " • Dec 30, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Book Review: The Pulp Mindset; A Newpub Survival Mindset",
            "content": "This book is about something worth noticing as the access to writing and publishing has broadened. There are healthy and helpful tips for writing and about writing learned from reading the Pulps of the past - and as someone that hasn’t read much of them and slowly breaking that particular seal I cannot agree more. However, reading the book feels more like what my own essays in high school felt like with repetition spread around and re-wordings of the same ideas. . That isn’t to say don’t purchase or read it; I think you should just because there is some good history here. The author’s knowledge about such topics as the speech Mutation or Death was quite a surprise and the relationship between Old Pub and New Pub is a meaningful distinction that Cowan does well. Personally, I think he’s overselling it as there is enough poor writing on display via Amazon Unlimited or niches such as LitRPG where the divide between practiced writers or amateurs shows up from the first page. . All in all, spend the money but get it in digital. .",
            "url": "https://orgulo.us/books/review/writing/2020/10/28/The-Pulp-Mindset-Review.html",
            "relUrl": "/books/review/writing/2020/10/28/The-Pulp-Mindset-Review.html",
            "date": " • Oct 28, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Simple Introduction to Python Debugging",
            "content": "Inspiration . While working on a script that I’m writing for work, I was having trouble really tracking down where my logic was failing. After looking over the code, everything looked correct but the results were showing me otherwise. So, I opened up python’s debugging system and started to dig in. . What is Debugging? . In computer programming and software development, debugging is the process of finding and resolving bugs (defects or problems that prevent correct operation) within computer programs, software, or systems. cf: https://en.wikipedia.org/wiki/Debugging . There are plenty of tools to do stuff like this but Python comes with its own: pdb — The Python Debugger. To activate it, you’d simply go into your terminal and type: . user@station# python3 -m pdb &lt;script-name&gt; . This with start the script from the top and immediately stop before running anything: . user@station:/scripts# python3 -m pdb keyIntegrityCheck.py &gt; /scripts/keyIntegrityCheck.py(5)&lt;module&gt;() -&gt; from pathlib import Path (Pdb) . While this is useful, we need to be able to explore the file. You could keep an editor open on a different monitor and move around but sometimes that’s not going to be an option. To print all the lines in the file you’d use: . (Pdb) ll 1 #!/usr/bin/env python3 2 ## Author: Collin Mitchell 3 ## Purpose: To check the integrity of keys without user input. 4 5 -&gt; from pathlib import Path 6 from itertools import filterfalse 7 import subprocess as sp 8 import json 9 import sys # ... 370 integrityFunc = dd.get(str(filename).split(&#39;.&#39;)[-1]) 371 if integrityFunc: 372 consistent = integrityFunc( filename ) 373 if not consistent: print(&quot;{} is not consistent.&quot;.format(filename)) 374 else: 375 print(&quot;{} is missing a checker; please report {} so it can be added.&quot;.format(filename,filename)) (Pdb) . You can see the pointer on line five telling us where the current execution point is. We can move the pointer a single line using n: . (Pdb) n [336/1995] &gt; /scripts/keyIntegrityCheck.py(6)&lt;module&gt;() -&gt; from itertools import filterfalse (Pdb) ll 1 #!/usr/bin/env python3 2 ## Author: Collin Mitchell 3 ## Purpose: To check the integrity of keys without user input. 4 5 from pathlib import Path 6 -&gt; from itertools import filterfalse 7 import subprocess as sp 8 import json 9 import sys . You can see the next 11 lines in the console buffer using l: . (Pdb) l 1 #!/usr/bin/env python3 2 ## Author: Collin Mitchell 3 ## Purpose: To check the integrity of keys without user input. 4 5 from pathlib import Path 6 -&gt; from itertools import filterfalse 7 import subprocess as sp 8 import json 9 import sys 10 11 # stuff to do with phpserialize: (Pdb) . Not that if you do this and run it again that you wont get the same result: . (Pdb) l 12 import codecs 13 try: 14 codecs.lookup_error(&#39;surrogateescape&#39;) 15 default_errors = &#39;surrogateescape&#39; 16 except LookupError: 17 default_errors = &#39;strict&#39; 18 try: 19 xrange 20 except NameError: 21 xrange = range 22 try: (Pdb) . … but you will get the next 11 lines instead. You can tell it which lines to list centered on a line number using l 10: . (Pdb) l 10 5 from pathlib import Path 6 -&gt; from itertools import filterfalse 7 import subprocess as sp 8 import json 9 import sys 10 11 # stuff to do with phpserialize: 12 import codecs 13 try: 14 codecs.lookup_error(&#39;surrogateescape&#39;) 15 default_errors = &#39;surrogateescape&#39; (Pdb) . Now that we can move around, let’s discuss how to actually stop code execution using breakpoints. These are locations you set - sometimes with conditions - to stop the code execution and explore the current state. You can set these using ‘b’: . (Pdb) b (Pdb) . Since we don’t have any breakpoints set, then it makes sense we don’t see any listed. So, now lets set one: . (Pdb) b 9 Breakpoint 1 at /scripts/keyIntegrityCheck.py:9 (Pdb) . … and continue execution until the breakpoint using c: . (Pdb) c &gt; /scripts/keyIntegrityCheck.py(9)&lt;module&gt;() -&gt; import sys (Pdb) . … and list the active breakpoints again: . (Pdb) b Num Type Disp Enb Where 1 breakpoint keep yes at /scripts/keyIntegrityCheck.py:9 breakpoint already hit 1 time (Pdb) . You can clear that breakpoint using the number of the breakpoint as well: . (Pdb) b Num Type Disp Enb Where 1 breakpoint keep yes at /scripts/keyIntegrityCheck.py:9 breakpoint already hit 1 time (Pdb) cl 1 Deleted breakpoint 1 at /scripts/keyIntegrityCheck.py:9 (Pdb) b (Pdb) . Conditional breakpoints I found a bit tricky to get to work correctly because you need to place a comma after the statement: . (Pdb) b 371, integrityFunc.__name__ == &#39;integrityPhp&#39; Breakpoint 3 at /scripts/keyIntegrityCheck.py:371 (Pdb) . … and then you c until it triggers: . &gt; /scripts/keyIntegrityCheck.py(371)&lt;module&gt;() -&gt; if integrityFunc: (Pdb) integrityFunc &lt;function integrityPhp at 0x7f99291477b8&gt; (Pdb) . Conclusion . I couldn’t find much outside of the official documentation when I needed it so hopefully you find the highlights useful. There are also tools for whichever IDE you’re using so look out for those as well. .",
            "url": "https://orgulo.us/python/programming/debugging/2020/10/21/Simple-Python-Debugging-Example.html",
            "relUrl": "/python/programming/debugging/2020/10/21/Simple-Python-Debugging-Example.html",
            "date": " • Oct 21, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Game Review: Post Void",
            "content": "Post Void is a hypnotic scramble of early first-person shooter design that values speed above all else. Keep your head full and reach the end; Kill what you can to see it mend; Get the high score or try again. . About a month ago I Steam was kind enough to recommend a rough looking LSD-inspired shooter. And, this little gem is called Post Void created by a little Developer group called YCJY Games. Looking at the videos, screenshots and description I was immediately sold: . “Post Void is a hypnotic scramble of early first-person shooter design that values speed above all else. Keep your head full and reach the end; Kill what you can to see it mend; Get the high score or try again.” . . The game delivers on all of that with the classic Arcade Shooter style that I miss everytime I spin up any modern First Person Shooter and with a price tag of $2.99, there wasn’t any risk. . There is a tutorial level to get you acquainted with the basic ideas of the game - which is really just: You have a slide like many modern shooters. Your health is that glass tiki skull which drains away. Then you’re dropped right in and good luck. . The management of your health which forces you to press on creates a fun stresser which is simple but not overwhelming to manage: kill to fill. The enemy varieties are also limited for the 11 stages you race through - which isn’t all that many. It’s not a long game and wouldn’t actually mind seeing a Post Void 2 which pulls some elements from Rouge-likes and maybe a bit of coop shuffle. Also, the aim is a bit forgiving and there were moments where I was sure I would miss but still killed an enemy. . The art style is a bit rough on the eyes at first but actually is the right choice to make registering enemies and threats blend into the background just enough that you can miss or run right into them if you’re not careful. I’m also partial to colorful games so I admit this visually is my kind of game. . All in all, I’d recommend stuffing this game into downtime between lobbies and breaks between studying to stay alert. . Also, the Shotgun is best. Who knows why anyone wants to use the knife: weirdos. .",
            "url": "https://orgulo.us/game/review/lsd/arcade%20shooter/2020/09/30/Game-Review-Post-Void.html",
            "relUrl": "/game/review/lsd/arcade%20shooter/2020/09/30/Game-Review-Post-Void.html",
            "date": " • Sep 30, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Game Review: Empyrion - Galactic Survival",
            "content": "If you’re looking for a TL:DR: game is fun but has some challenges. . The first time I opened the map to see all the different systems we could visit I was basically sold on the game. The first time we made our system hop, we almost got trapped in hellscape and had a blast trying to get off that planet. . Empyrion - Galactic Survival is - well - a Survival Game that adds the idea of different planets and systems to explore instead of simply other towns or woods or whatever that splotch over that hill is. What’s nice about it was as you explore the menus, it gives you something to plan for while trying to build up some semblance of safety where you are. As you scroll out, there are hundreds of systems to explore with their own rings of planets and moons and asteroid fields - which really is the allure of this game. I want to float around the Galaxy and do stupid things. . Out of the gate, there are lots of geometries to play with when building bases or anything else which is a nice change from other Survival games where you’re kind of stuck with a few pre-defined structures. Sadly, the internal structure of the actual block shape wont impact where you can actually place anything since you’re still locked into a cubic construction system. This does make a good number of the more interesting shapes rather impractical to try and use on anything you build. . The plethora of pieces to care about makes navigating and management challenging. Finding out just how to build your first actual base was annoying until you realize that you need a core or a starter block to build off anything. Different blocks are tied to different kinds of structures which are denoted by the initials HV,CV,SV,BA which are just not explained. You’re going to be reading the descriptions of the stuff you build; you’re going to have to just about all the time because you’re going to miss important details. My mistake was not realizing that shields will require a Pentaxid to fuel them and there is no warning about this until the shields don’t work - except the last line in the text. A better idea maybe would have been that you don’t have any shields until you’ve actually fed your Vessel Pentaxid and therefore you’ll be forced to realize this. There is a good amount of finding out the hard way. . Likewise, there are lots of small stats and management that goes into a base which make sense but just don’t feel intuitive at all. I know that I need power and to do that I need something which makes power but I don’t know anything about Solar Panel rates from looking at the device before construction; there is no real way to optimize buildings or make decisions about if I want something until I’ve made it and realized that I either don’t need it yet or it’s missing necessary components. The Solar Panels need a Capacitor else they’re really not much use once the sun is down - and the base doesn’t store anything. Explore the Control Panel as soon as you can. . Contrary to some complaints, the combat isn’t bad for a Survival game and fights in space have been pretty fun. Having my friend float around in a tuned Small Vessel hecking out resources in Zirax controlled space to avoid pulling us into a space fight with our starter Capital Vessel only to be discovered by a wandering drone led to quite the episode of us coordinating his docking while everything in the system came to murder us escaping with a queue’d up lock to a different system for just such an emergency is why this game has been fun. The complaints about being outgunned are certainly true though. The Defense Station that sits just outside the atmosphere is a space graveyard of failed attempts to kick it out of the orbit. After building a prototype to try and kill the thing, I point-blanked it with 6 pulse Lasers 49 times before it finally blew apart my small vessel and I lost. The AI for the enemies in the game is certainly questionable too. I’ve watched as three factions of different types mingle with one another as if they’re not even aware of each other until I am taking shots at them and then they’re all after me like there is some pact to murder non-NPCs in the Galaxy on sight. . All in all, I’m not regretting buying it and it’s probably my favorite current Survival game. I think that the criticisms are pretty fair though; the game is very ambitious and they haven’t delivered yet but the platform is there so long as they’re continuing to diligently add the pieces patch by patch I’ll keep playing it. Complicated but promising. .",
            "url": "https://orgulo.us/games/review/scifi/survival/2020/09/18/Game-Review-Empyrion-Galactic-Survival.html",
            "relUrl": "/games/review/scifi/survival/2020/09/18/Game-Review-Empyrion-Galactic-Survival.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "Fastbooks Incidentally Supports R",
            "content": "Pleasant Surprise . While trying to test the boundaries of what fastpages actually supports, I figured I&#39;d try out installing and setting up an R Notebook as well. Luckily enough, it does indeed support compiling and building R kernels as well. . The first step will be to install an R kernel for the notebook which can be done using: . conda install -c r r-essentials . This can be ran either from inside a notebook by prepending a ! in a cell such as !conda install -c r r-essentials or simply run it at the console if you&#39;re in linux and in the project directory. . This is mostly an exibition post about how this can be done so we&#39;re just going to show off some R stuff. . # I miss this selecting over Python&#39;s Pandas: mtcars[order(mtcars$gear, mtcars$mpg), ] . mpgcyldisphpdratwtqsecvsamgearcarb . Cadillac Fleetwood10.4 | 8 | 472.0 | 205 | 2.93 | 5.250 | 17.98 | 0 | 0 | 3 | 4 | . Lincoln Continental10.4 | 8 | 460.0 | 215 | 3.00 | 5.424 | 17.82 | 0 | 0 | 3 | 4 | . Camaro Z2813.3 | 8 | 350.0 | 245 | 3.73 | 3.840 | 15.41 | 0 | 0 | 3 | 4 | . Duster 36014.3 | 8 | 360.0 | 245 | 3.21 | 3.570 | 15.84 | 0 | 0 | 3 | 4 | . Chrysler Imperial14.7 | 8 | 440.0 | 230 | 3.23 | 5.345 | 17.42 | 0 | 0 | 3 | 4 | . Merc 450SLC15.2 | 8 | 275.8 | 180 | 3.07 | 3.780 | 18.00 | 0 | 0 | 3 | 3 | . AMC Javelin15.2 | 8 | 304.0 | 150 | 3.15 | 3.435 | 17.30 | 0 | 0 | 3 | 2 | . Dodge Challenger15.5 | 8 | 318.0 | 150 | 2.76 | 3.520 | 16.87 | 0 | 0 | 3 | 2 | . Merc 450SE16.4 | 8 | 275.8 | 180 | 3.07 | 4.070 | 17.40 | 0 | 0 | 3 | 3 | . Merc 450SL17.3 | 8 | 275.8 | 180 | 3.07 | 3.730 | 17.60 | 0 | 0 | 3 | 3 | . Valiant18.1 | 6 | 225.0 | 105 | 2.76 | 3.460 | 20.22 | 1 | 0 | 3 | 1 | . Hornet Sportabout18.7 | 8 | 360.0 | 175 | 3.15 | 3.440 | 17.02 | 0 | 0 | 3 | 2 | . Pontiac Firebird19.2 | 8 | 400.0 | 175 | 3.08 | 3.845 | 17.05 | 0 | 0 | 3 | 2 | . Hornet 4 Drive21.4 | 6 | 258.0 | 110 | 3.08 | 3.215 | 19.44 | 1 | 0 | 3 | 1 | . Toyota Corona21.5 | 4 | 120.1 | 97 | 3.70 | 2.465 | 20.01 | 1 | 0 | 3 | 1 | . Merc 280C17.8 | 6 | 167.6 | 123 | 3.92 | 3.440 | 18.90 | 1 | 0 | 4 | 4 | . Merc 28019.2 | 6 | 167.6 | 123 | 3.92 | 3.440 | 18.30 | 1 | 0 | 4 | 4 | . Mazda RX421.0 | 6 | 160.0 | 110 | 3.90 | 2.620 | 16.46 | 0 | 1 | 4 | 4 | . Mazda RX4 Wag21.0 | 6 | 160.0 | 110 | 3.90 | 2.875 | 17.02 | 0 | 1 | 4 | 4 | . Volvo 142E21.4 | 4 | 121.0 | 109 | 4.11 | 2.780 | 18.60 | 1 | 1 | 4 | 2 | . Datsun 71022.8 | 4 | 108.0 | 93 | 3.85 | 2.320 | 18.61 | 1 | 1 | 4 | 1 | . Merc 23022.8 | 4 | 140.8 | 95 | 3.92 | 3.150 | 22.90 | 1 | 0 | 4 | 2 | . Merc 240D24.4 | 4 | 146.7 | 62 | 3.69 | 3.190 | 20.00 | 1 | 0 | 4 | 2 | . Fiat X1-927.3 | 4 | 79.0 | 66 | 4.08 | 1.935 | 18.90 | 1 | 1 | 4 | 1 | . Honda Civic30.4 | 4 | 75.7 | 52 | 4.93 | 1.615 | 18.52 | 1 | 1 | 4 | 2 | . Fiat 12832.4 | 4 | 78.7 | 66 | 4.08 | 2.200 | 19.47 | 1 | 1 | 4 | 1 | . Toyota Corolla33.9 | 4 | 71.1 | 65 | 4.22 | 1.835 | 19.90 | 1 | 1 | 4 | 1 | . Maserati Bora15.0 | 8 | 301.0 | 335 | 3.54 | 3.570 | 14.60 | 0 | 1 | 5 | 8 | . Ford Pantera L15.8 | 8 | 351.0 | 264 | 4.22 | 3.170 | 14.50 | 0 | 1 | 5 | 4 | . Ferrari Dino19.7 | 6 | 145.0 | 175 | 3.62 | 2.770 | 15.50 | 0 | 1 | 5 | 6 | . Porsche 914-226.0 | 4 | 120.3 | 91 | 4.43 | 2.140 | 16.70 | 0 | 1 | 5 | 2 | . Lotus Europa30.4 | 4 | 95.1 | 113 | 3.77 | 1.513 | 16.90 | 1 | 1 | 5 | 2 | . mtcars[order(mtcars$gear, mtcars$mpg), ] %&gt;% ggplot(aes(disp, hp, colour = cyl)) + geom_point() . crimes &lt;- data.frame(state = tolower(rownames(USArrests)), USArrests) # Equivalent to crimes %&gt;% tidyr::pivot_longer(Murder:Rape) vars &lt;- lapply(names(crimes)[-1], function(j) { data.frame(state = crimes$state, variable = j, value = crimes[[j]]) }) crimes_long &lt;- do.call(&quot;rbind&quot;, vars) states_map &lt;- map_data(&quot;state&quot;) ggplot(crimes_long, aes(map_id = state)) + geom_map(aes(fill = value), map = states_map) + expand_limits(x = states_map$long, y = states_map$lat) + facet_wrap( ~ variable) . I did also try to use ggvis as well but it just wont display properly so that&#39;s unfortunately out. .",
            "url": "https://orgulo.us/r/jupyter/ggplot/2020/09/09/Fastbooks-Incidentally-Supports-R.html",
            "relUrl": "/r/jupyter/ggplot/2020/09/09/Fastbooks-Incidentally-Supports-R.html",
            "date": " • Sep 9, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "A Post Exploring Altair",
            "content": "This Is The Tool We Have . I&#39;m trying out the fastpages in hopes that I wont have to spend the time building out my own website toolset. I&#39;ve been slowly building something out of Wagtail which is a really just Djnago with some bells. The real allure though is going to be the Notebook conversions - specifically the Data Visualizations. The library for interactive version is Altair and we&#39;re going to explore some data! . Lets Explore! . So, the tutorial for Scatter Plot uses the car data but I figured we mind as well do the classic Iris dataset. We&#39;ll start by importing the dataset from vega_datasets - which is the Javascript library that Altair is built on top of - using iris = data.iris(). . iris = data.iris() iris . sepalLength sepalWidth petalLength petalWidth species . 0 5.1 | 3.5 | 1.4 | 0.2 | setosa | . 1 4.9 | 3.0 | 1.4 | 0.2 | setosa | . 2 4.7 | 3.2 | 1.3 | 0.2 | setosa | . 3 4.6 | 3.1 | 1.5 | 0.2 | setosa | . 4 5.0 | 3.6 | 1.4 | 0.2 | setosa | . ... ... | ... | ... | ... | ... | . 145 6.7 | 3.0 | 5.2 | 2.3 | virginica | . 146 6.3 | 2.5 | 5.0 | 1.9 | virginica | . 147 6.5 | 3.0 | 5.2 | 2.0 | virginica | . 148 6.2 | 3.4 | 5.4 | 2.3 | virginica | . 149 5.9 | 3.0 | 5.1 | 1.8 | virginica | . 150 rows × 5 columns . First lets see the graph - and then we&#39;ll discuss the functions . alt .Chart(iris) .mark_point() .encode( x=&#39;sepalLength&#39;, y=&#39;sepalWidth&#39;, color=&#39;species&#39;, tooltip = [&#39;species&#39;, &#39;petalLength&#39;, &#39;petalWidth&#39;] ) .interactive() . It is interesting to note that - per the Docs - : . Create a basic Altair/Vega-Lite chart. . Although it is possible to set all Chart properties as constructor attributes, it is more idiomatic to use methods such as mark_point(), encode(), transform_filter(), properties(), etc. . .. which means that it&#39;s found a way to do something similar to the R Programming Languages pipe operator. For reference, it would looks something like this:mtcars %&gt;% ggplot(aes(wt, mpg)) + geom_point(aes(colour = factor(cyl))) . . First we tell altair to make a chart using the dataset we&#39;re using: . .Chart(iris) . ... which is then followed by the kind of graph that we&#39;re after - in this case we&#39;re after a scatter plot: . .mark_point() . ... and then we tell it where everything belongs. . .encode( x=&#39;sepalLength&#39;, y=&#39;sepalWidth&#39;, color=&#39;species&#39;, tooltip = [&#39;species&#39;, &#39;petalLength&#39;, &#39;petalWidth&#39;] ) . Of interest is that you can add data from the other columns easily using the tooltip without having to add anything extra. Layering information which is relevant but lacks a meaningful graphic representation was a nice touch. . Then of course, you allow users to interact with it via: . .interactive() . Lets see what this post looks like on the blog! .",
            "url": "https://orgulo.us/altair/jupyter/python/2020/09/06/exploring-altair-visualization.html",
            "relUrl": "/altair/jupyter/python/2020/09/06/exploring-altair-visualization.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Book Review: Voices of the Void",
            "content": "The Voices of the Void by David Stewart is a pleasant short story that mixes a Science Fiction backdrop with some supernatural horror. We join the introduced protagonist - Andrew Dalatent - entering a mining colony called New Gibralter where he appears tasked with figuring out what happened to the people there. And, I say it appears because as he starts his descent you find out he’s more aware of the whys and whats of the situation than is initially let on. . Coupled with his own supernatural ability to observe future and past versions of events around him, we get to join him as he tries to survive against humanity removed of its own free will. These sections that Stewart uses were my own personal favorite because they’re surprisingly easy to follow but also because they grow slowly out of being just a novel tactic Andrew uses to protect himself into an important aspect of the later story. . Short read but enjoyable. .",
            "url": "https://orgulo.us/books/review/scifi/2020/07/09/Voices-of-the-Void.html",
            "relUrl": "/books/review/scifi/2020/07/09/Voices-of-the-Void.html",
            "date": " • Jul 9, 2020"
        }
        
    
  

  
  

  

  
      ,"page2": {
          "title": "Reading Challenge",
          "content": "Book Recommendations: . These recommendations are not about the best prose nor about the most interesting story; they’re about books to build a foundation on which will help you stabalize your life to get where you would like to go in some form of comfort. . How to Read a Book: The Classic Guide to Intelligent Reading How to Read a Book, originally published in 1940, has become a rare phenomenon, a living classic. It is the best and most successful guide to reading comprehension for the general reader. And now it has been completely rewritten and updated. . | The Elements of Style by William Strunk Jr. and E.B. White This is The Elements of Style, the classic style manual, now in a fourth edition. A new Foreword by Roger Angell reminds readers that the advice of Strunk &amp; White is as valuable today as when it was first offered. This book’s unique tone, wit and charm have conveyed the principles of English style to millions of readers. Use the fourth edition of “the little book” to make a big impact with writing. . | The Keys to Prolific Creativity by David V. Stewart Designed for all artists, writers, and musicians, The Keys to Prolific Creativity is a no-fluff guide to organizing your priorities, managing your growth, and designing a creative process that will help you achieve the creative output you have always desired. . | A Random Walk Down Wall Street by Burton G. Malkiel Especially in the wake of the financial meltdown, readers will hunger for Burton G. Malkiel’s reassuring, authoritative, gimmick-free, and perennially best-selling guide to investing. With 1.5 million copies sold, A Random Walk Down Wall Street has long been established as the first book to purchase when starting a portfolio. . | Profit First: Transform Your Business from a Cash-Eating Monster to a Money-Making Machine by Mike Michalowicz Conventional accounting uses the logical (albeit, flawed) formula: Sales - Expenses = Profit. The problem is, businesses are run by humans, and humans aren’t always logical. Serial entrepreneur Mike Michalowicz has developed a behavioral approach to accounting to flip the formula: Sales - Profit = Expenses. Just as the most effective weight loss strategy is to limit portions by using smaller plates, Michalowicz shows that by taking profit first and apportioning only what remains for expenses, entrepreneurs will transform their businesses from cash-eating monsters to profitable cash cows. Using Michalowicz’s Profit First system . |",
          "url": "https://orgulo.us/Readings/",
          "relUrl": "/Readings/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://orgulo.us/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}